{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ae230d7-2ab1-469e-a6a0-238293c1eeb1",
   "metadata": {},
   "source": [
    "# 0 - Librerías y variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d99484c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Librerías\n",
    "# ------------------------------------------------------------------------------\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d680460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY\n",
      "PROXYCURL_API_KEY\n",
      "TAVILY_API_KEY\n",
      "LANGCHAIN_TRACING_V2\n",
      "LANGCHAIN_ENDPOINT\n",
      "LANGCHAIN_API_KEY\n",
      "LANGCHAIN_PROJECT\n"
     ]
    }
   ],
   "source": [
    "# Variables\n",
    "# ------------------------------------------------------------------------------\n",
    "env_vars = dotenv_values()\n",
    "for key in env_vars.keys():\n",
    "    print(key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b9e5aaaa",
   "metadata": {},
   "source": [
    "# 1 - Introducción a los RAG"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85b775e8",
   "metadata": {},
   "source": [
    "RAG significa Retrieval-Augmented Generation, o Generación Aumentada por Recuperación. Es una arquitectura híbrida que combina LLMs con motores de recuperación de información, generalmente basados en embeddings y búsqueda vectorial.\n",
    "\n",
    "La idea clave es no depender solo del conocimiento interno del LLM, sino complementarlo con información externa relevante que se recupera en tiempo real desde una base de datos documental."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd86aae8",
   "metadata": {},
   "source": [
    "**¿Por qué usar RAG?**\n",
    "\n",
    "Los modelos generativos, aunque son muy potentes, tienen tres limitaciones clave:\n",
    "- Alucinaciones: inventan respuestas cuando no saben algo.\n",
    "- Obsolescencia: su conocimiento está limitado al momento en que fueron entrenados.\n",
    "- Coste de fine-tuning: actualizar su conocimiento requiere reentrenar o usar técnicas más complejas.\n",
    "\n",
    "RAG soluciona esto al buscar primero información en una base de datos externa (documentos, PDFs, páginas web, etc.), y luego pasar esa información como contexto al modelo LLM, que produce una respuesta más precisa, basada en datos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03d85ab2",
   "metadata": {},
   "source": [
    "**¿Cómo funciona RAG? (flujo típico)**\n",
    "\n",
    "Un sistema RAG moderno, como el que puedes construir con LangChain, suele organizarse en las siguientes etapas:\n",
    "\n",
    "- Load (Carga de documentos): Se ingestan las fuentes de información que van a alimentar al sistema. El objetivo es leer los datos en bruto y convertirlos en texto plano.\n",
    "- Split (Segmentación de texto): Los textos se dividen en fragmentos más pequeños para facilitar su uso como contexto. Se usan técnicas de ventana deslizante con solapamiento para preservar la coherencia semántica.\n",
    "- Storage (Vectorización y almacenamiento): Cada fragmento se convierte en un embedding y se almacena en una base vectorial para búsquedas por similitud semántica.\n",
    "- Retrieval (Recuperación): La consulta del usuario se vectoriza y se buscan los fragmentos más similares en la base vectorial. Estos fragmentos forman el contexto que se pasará al modelo generativo.\n",
    "- Output (Generación de respuesta):Se combinan el contexto recuperado y la consulta original, y se pasan al modelo de lenguaje, que genera una respuesta informada.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0298aac8",
   "metadata": {},
   "source": [
    "![](../img/RAG.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f4e4db3",
   "metadata": {},
   "source": [
    "# 2 - Document Loading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34848d07",
   "metadata": {},
   "source": [
    "El primer paso en cualquier sistema RAG es **ingestar los documentos que contienen el conocimiento base**. En LangChain, esta tarea se gestiona mediante los **loaders**, que permiten leer datos desde múltiples formatos y fuentes (PDFs, Word, Markdown, HTML, páginas web, etc.).\n",
    "\n",
    "El objetivo es transformar estas fuentes heterogéneas en una representación uniforme: una lista de objetos `Document`, cada uno con su contenido de texto y metadatos asociados.\n",
    "\n",
    "LangChain ofrece una interfaz común para todos los loaders, lo que facilita trabajar con diferentes tipos de datos sin tener que preocuparte por los detalles de bajo nivel.\n",
    "\n",
    "> A partir de aquí, el texto ya puede ser segmentado y convertido en embeddings para la recuperación semántica.\n",
    "\n",
    "En esta sección exploraremos cómo utilizar algunos de los loaders más comunes y cómo inspeccionar el resultado de la carga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b3f5f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, WebBaseLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "507c26d3",
   "metadata": {},
   "source": [
    "## 2.1. - PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c166a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciar PDFLoader\n",
    "loader = PyPDFLoader(\"../data/docs/cs229_lectures/MachineLearning-Lecture01.pdf\")\n",
    "\n",
    "# Cargar PDF\n",
    "pages = loader.load()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b27eae2e",
   "metadata": {},
   "source": [
    "Cada página es un `Document` de LangChain, y cada `Document` contiene:\n",
    "- contenido: `page_content`\n",
    "- metadata: `metadata`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a058d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Páginas del PDF: 22\n"
     ]
    }
   ],
   "source": [
    "print(f\"Páginas del PDF: {len(pages)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96f0f068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creator': 'PScript5.dll Version 5.2.2', 'creationdate': '2008-07-11T11:25:23-07:00', 'author': '', 'moddate': '2008-07-11T11:25:23-07:00', 'title': '', 'source': '../data/docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'total_pages': 22, 'page': 0, 'page_label': '1'}, page_content='MachineLearning-Lecture01  \\nInstructor (Andrew Ng): Okay. Good morning. Welcome to CS229, the machine \\nlearning class. So what I wanna do today is just spend a little time going over the logistics \\nof the class, and then we\\'ll start to talk a bit about machine learning.  \\nBy way of introduction, my name\\'s Andrew Ng and I\\'ll be instructor for this class. And so \\nI personally work in machine learning, and I\\'ve worked on it for about 15 years now, and \\nI actually think that machine learning is the most exciting field of all the computer \\nsciences. So I\\'m actually always excited about teaching this class. Sometimes I actually \\nthink that machine learning is not only the most exciting thing in computer science, but \\nthe most exciting thing in all of human endeavor, so maybe a little bias there.  \\nI also want to introduce the TAs, who are all graduate students doing research in or \\nrelated to the machine learning and all aspects of machine learning. Paul Baumstarck \\nworks in machine learning and computer vision. Catie Chang is actually a neuroscientist \\nwho applies machine learning algorithms to try to understand the human brain. Tom Do \\nis another PhD student, works in computational biology and in sort of the basic \\nfundamentals of human learning. Zico Kolter is the head TA — he\\'s head TA two years \\nin a row now — works in machine learning a nd applies them to a bunch of robots. And \\nDaniel Ramage is — I guess he\\'s not here  — Daniel applies l earning algorithms to \\nproblems in natural language processing.  \\nSo you\\'ll get to know the TAs and me much better throughout this quarter, but just from \\nthe sorts of things the TA\\'s do, I hope you can already tell that machine learning is a \\nhighly interdisciplinary topic in which just the TAs find learning algorithms to problems \\nin computer vision and biology and robots and language. And machine learning is one of \\nthose things that has and is having a large impact on many applications.  \\nSo just in my own daily work, I actually frequently end up talking to people like \\nhelicopter pilots to biologists to people in computer systems or databases to economists \\nand sort of also an unending stream of people from industry coming to Stanford \\ninterested in applying machine learning methods to their own problems.  \\nSo yeah, this is fun. A couple of weeks ago, a student actually forwarded to me an article \\nin \"Computer World\" about the 12 IT skills that employers can\\'t say no to. So it\\'s about \\nsort of the 12 most desirable skills in all of IT and all of information technology, and \\ntopping the list was actually machine learning. So I think this is a good time to be \\nlearning this stuff and learning algorithms and having a large impact on many segments \\nof science and industry.  \\nI\\'m actually curious about something. Learning algorithms is one of the things that \\ntouches many areas of science and industries, and I\\'m just kind of curious. How many \\npeople here are computer science majors, are in the computer science department? Okay. \\nAbout half of you. How many people are from EE? Oh, okay, maybe about a fifth. How')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = pages[0]\n",
    "\n",
    "print(type(page))\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6d18bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'Acrobat Distiller 8.1.0 (Windows)',\n",
       " 'creator': 'PScript5.dll Version 5.2.2',\n",
       " 'creationdate': '2008-07-11T11:25:23-07:00',\n",
       " 'author': '',\n",
       " 'moddate': '2008-07-11T11:25:23-07:00',\n",
       " 'title': '',\n",
       " 'source': '../data/docs/cs229_lectures/MachineLearning-Lecture01.pdf',\n",
       " 'total_pages': 22,\n",
       " 'page': 0,\n",
       " 'page_label': '1'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f21b6c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MachineLearning-Lecture01  \\nInstructor (Andrew Ng): Okay. Good morning. Welcome to CS229, the machine \\nlearning class. So what I wanna do today is just spend a little time going over the logistics \\nof the class, and then we\\'ll start to talk a bit about machine learning.  \\nBy way of introduction, my name\\'s Andrew Ng and I\\'ll be instructor for this class. And so \\nI personally work in machine learning, and I\\'ve worked on it for about 15 years now, and \\nI actually think that machine learning is the most exciting field of all the computer \\nsciences. So I\\'m actually always excited about teaching this class. Sometimes I actually \\nthink that machine learning is not only the most exciting thing in computer science, but \\nthe most exciting thing in all of human endeavor, so maybe a little bias there.  \\nI also want to introduce the TAs, who are all graduate students doing research in or \\nrelated to the machine learning and all aspects of machine learning. Paul Baumstarck \\nworks in machine learning and computer vision. Catie Chang is actually a neuroscientist \\nwho applies machine learning algorithms to try to understand the human brain. Tom Do \\nis another PhD student, works in computational biology and in sort of the basic \\nfundamentals of human learning. Zico Kolter is the head TA — he\\'s head TA two years \\nin a row now — works in machine learning a nd applies them to a bunch of robots. And \\nDaniel Ramage is — I guess he\\'s not here  — Daniel applies l earning algorithms to \\nproblems in natural language processing.  \\nSo you\\'ll get to know the TAs and me much better throughout this quarter, but just from \\nthe sorts of things the TA\\'s do, I hope you can already tell that machine learning is a \\nhighly interdisciplinary topic in which just the TAs find learning algorithms to problems \\nin computer vision and biology and robots and language. And machine learning is one of \\nthose things that has and is having a large impact on many applications.  \\nSo just in my own daily work, I actually frequently end up talking to people like \\nhelicopter pilots to biologists to people in computer systems or databases to economists \\nand sort of also an unending stream of people from industry coming to Stanford \\ninterested in applying machine learning methods to their own problems.  \\nSo yeah, this is fun. A couple of weeks ago, a student actually forwarded to me an article \\nin \"Computer World\" about the 12 IT skills that employers can\\'t say no to. So it\\'s about \\nsort of the 12 most desirable skills in all of IT and all of information technology, and \\ntopping the list was actually machine learning. So I think this is a good time to be \\nlearning this stuff and learning algorithms and having a large impact on many segments \\nof science and industry.  \\nI\\'m actually curious about something. Learning algorithms is one of the things that \\ntouches many areas of science and industries, and I\\'m just kind of curious. How many \\npeople here are computer science majors, are in the computer science department? Okay. \\nAbout half of you. How many people are from EE? Oh, okay, maybe about a fifth. How'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abff89b7",
   "metadata": {},
   "source": [
    "## 2.2. URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20e006ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciar WebLoader\n",
    "loader = WebBaseLoader(\"https://raw.githubusercontent.com/gmachinromero/exploring-langchain/refs/heads/master/README.md\")\n",
    "\n",
    "# Cargar URL\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b25a5ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00216728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://raw.githubusercontent.com/gmachinromero/exploring-langchain/refs/heads/master/README.md'}, page_content='# Exploring LangChain! 🦜️🔗\\n\\nEste repositorio contiene diversas pruebas y experimentos realizados para aprender y explorar las capacidades del framework [LangChain](https://langchain.com/). Aquí encontrarás ejemplos de cómo utilizar las distintas herramientas que proporciona LangChain para la creación de aplicaciones basadas en inteligencia artificial, flujos de trabajo y manejo de grandes lenguajes de modelo (LLM).\\n\\n\\n## Contenido\\n\\nEl contenido del repositorio permite ejecutar diferentes scripts de Python con el framework de Langchain con la idea de explorar:\\n- Utilizar LLM tanto de forma online como en local\\n- Conexiones a APIs\\n- Desarrollo de agentes (Agents)\\n- Desarrollo de herramientas (Tools)\\n- Trazabilidad con LangSmith \\n\\n\\n## Resultado\\n\\nEl resultado del proyecto es desplegar un frontal en local, que dado el nombre de un usuario de LinkedIn, se capaz de resumir su perfil, y de mostrar 2 datos interesantes de su perfil.\\n\\n[Aspiracional]\\nEn futuras iteraciones se espera generar también de forma automática un mensaje llamativo para contactar con el por LinkedIn, en base al contenido de su perfil.\\n\\n![img.png](img/img.png)\\n\\n\\n## Instalación\\n\\nPara clonar este repositorio y comenzar a experimentar con LangChain, sigue estos pasos:\\n\\n```bash\\ngit clone https://github.com/gmachinromero/exploring-langchain.git\\ncd exploring-langchain\\n```\\n\\nAsegúrate de tener instalado Python 3.11+ y pipenv para crear un entorno con todas las dependencias necesarias:\\n\\n```bash\\npipenv install\\n```\\n\\nPara desplegar el frontal debes ejecutar:\\n```bash\\npython app.py\\n```\\n\\nAdicionalmente necesitarás los token siguientes en un fichero de configuración `.env`:\\n- `OPENAI_API_KEY`\\n- `PROXYCURL_API_KEY`\\n- `TAVILY_API_KEY`\\n- `LANGCHAIN_TRACING_V2`\\n- `LANGCHAIN_ENDPOINT`\\n- `LANGCHAIN_API_KEY`\\n- `LANGCHAIN_PROJECT`\\n\\nDado que el uso de algunas de estas APIs como proxycurl es limitado, se almacenan response tipo en GitHub Gist para poder trabajar de forma continuada sin agotar el número de llamadas límite a la API.\\n\\n## Esquema\\nEl repositorio tiene la siguiente estructura de carpetas:\\n\\n```\\nexploring-langchain/\\n├── agents/\\n│   ├── __init__.py\\n│   └── linkedin_lookup_agent.py\\n├── parsers/\\n│   ├── __init__.py\\n│   └── output_parser.py\\n├── third_party/              \\n│   ├── __init__.py\\n│   └── linkedin.py\\n├── tools/\\n│   ├── __init__.py\\n│   └── tools.py\\n├── templates/                 # Plantilla para el frontal\\n│   └── index.html\\n├── .env                       # Fichero para API_KEYS\\n├── .gitignore                 # Archivos a ignorar por Git\\n├── app.py                     # Desplegar app con linkedin_summarizer.py\\n├── ice_breaker.py             # PoC\\n├── linkedin_summarizer.py     # Código principal\\n├── Pipfile                    # Dependencias del proyecto\\n├── Pipfile.lock               # Dependencias del proyecto\\n└── README.md                  # Descripción general del proyecto\\n```\\n\\n## Recursos\\n- https://python.langchain.com/v0.2/docs/introduction/\\n- Udemy: LangChain- Develop LLM powered applications with LangChain\\n\\n')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = docs[0]\n",
    "\n",
    "print(type(doc))\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75741ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://raw.githubusercontent.com/gmachinromero/exploring-langchain/refs/heads/master/README.md'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad3f559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448dc7c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea1ad1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2717e9a0",
   "metadata": {},
   "source": [
    "# 3 - Document Splitting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2fce424b",
   "metadata": {},
   "source": [
    "Una vez que los documentos han sido cargados correctamente, el siguiente paso es **dividir el texto en fragmentos más pequeños**, adecuados para su posterior vectorización y uso como contexto en el sistema RAG.\n",
    "\n",
    "Esto es necesario porque:\n",
    "\n",
    "- Los modelos de lenguaje tienen un límite de tokens por entrada (context length).\n",
    "- Dividir bien el texto ayuda a preservar la coherencia semántica.\n",
    "- Mejora la calidad de la recuperación al indexar unidades más específicas de información.\n",
    "\n",
    "LangChain proporciona varios **text splitters** que permiten segmentar el contenido de forma flexible. La elección del splitter y de sus parámetros (longitud, solapamiento, método de segmentación) **impacta directamente en la calidad del sistema RAG**.\n",
    "\n",
    "| **Text Splitter**                  | **¿Para qué sirve?**                                                                                                                                                 |\n",
    "|------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| `RecursiveCharacterTextSplitter`   | El splitter más versátil. Divide el texto intentando respetar estructuras jerárquicas (párrafos, frases, palabras). Ideal para mantener coherencia semántica.        |\n",
    "| `CharacterTextSplitter`            | Divide el texto en fragmentos basados en un número fijo de caracteres. Más simple que el `Recursive`, útil cuando no se necesita segmentación inteligente.           |\n",
    "| `TokenTextSplitter`                | Divide el texto en función del número de tokens, en lugar de caracteres. Muy útil si trabajas con límites de contexto medidos en tokens (como modelos OpenAI).       |\n",
    "| `MarkdownHeaderTextSplitter`       | Diseñado específicamente para documentos Markdown. Usa los encabezados (`#`, `##`, etc.) como guías de segmentación jerárquica.                                      |\n",
    "| `NLTKTextSplitter`                 | Utiliza el tokenizer de NLTK para dividir el texto por frases u oraciones. Requiere instalar NLTK. Apto para NLP clásico o textos con puntuación bien estructurada.  |\n",
    "| `SpacyTextSplitter`                | Usa spaCy para dividir por frases u oraciones gramaticales. Más preciso que NLTK pero también más pesado.                                                            |\n",
    "\n",
    "\n",
    "En esta sección exploraremos cómo:\n",
    "\n",
    "- Utilizar splitters como `RecursiveCharacterTextSplitter`.\n",
    "- Controlar la longitud (`chunk_size`) y solapamiento de los fragmentos (`chunk_overlap`).\n",
    "- Evaluar si los splits generados mantienen el significado y el contexto necesarios.\n",
    "\n",
    "> La calidad del *splitting* es tan importante como la del *retrieval* — fragmentos mal segmentados pueden arruinar incluso los mejores embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d37f8a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8946d28e",
   "metadata": {},
   "source": [
    "## 3.1. - RecursiveCharacterTextSplitter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9e8b7b3",
   "metadata": {},
   "source": [
    "El RecursiveCharacterTextSplitter usa varios niveles de separación y además su lógica es recursiva, con un orden de preferencia de separadores:\n",
    "- Saltos de línea dobles o simples\n",
    "- Puntuación fuerte (. ; :)\n",
    "- Comas y espacios\n",
    "\n",
    "Si no puede dividir con lo anterior, corta por tamaño fijo (caracteres)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f51a73de",
   "metadata": {},
   "source": [
    "![](../img/splitter_chunk_size_overlap.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a23f00d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=26,\n",
    "    chunk_overlap=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e57dcd3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijk',\n",
       " 'lmnopqrstuvwxyzabcdefghij',\n",
       " 'ghijklmnopqrstuvwxyzabcdef',\n",
       " 'cdefghijklmnopq.',\n",
       " 'rstuvwxyz']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = 'abcdefghijk lmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopq. rstuvwxyz'\n",
    "r_splitter.split_text(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "588c4200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a b c d e f g h i j k l m', 'l m n o p q r s t u v w x', 'w x y z']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2 = \"a b c d e f g h i j k l m n o p q r s t u v w x y z\"\n",
    "r_splitter.split_text(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "388a49ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "496"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_text = \"\"\"When writing documents, writers will use document structure to group content. \\\n",
    "This can convey to the reader, which idea's are related. For example, closely related ideas \\\n",
    "are in sentances. Similar ideas are in paragraphs. Paragraphs form a document. \\n\\n  \\\n",
    "Paragraphs are often delimited with a carriage return or two carriage returns. \\\n",
    "Carriage returns are the \"backslash n\" you see embedded in this string. \\\n",
    "Sentences have a period at the end, but also, have a space.\\\n",
    "and words are separated by space.\"\"\"\n",
    "\n",
    "len(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "564313eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"When writing documents, writers will use document structure to group content. This can convey to the reader, which idea's are related. For example, closely related ideas are in sentances. Similar ideas are in paragraphs. Paragraphs form a document.\",\n",
       " 'Paragraphs are often delimited with a carriage return or two carriage returns. Carriage returns are the \"backslash n\" you see embedded in this string. Sentences have a period at the end, but also, have a space.and words are separated by space.']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=450,\n",
    "    chunk_overlap=0, \n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "r_splitter.split_text(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f0094d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"When writing documents, writers will use document structure to group content. This can convey to the reader, which idea's are related. For example,\",\n",
       " 'closely related ideas are in sentances. Similar ideas are in paragraphs. Paragraphs form a document.',\n",
       " 'Paragraphs are often delimited with a carriage return or two carriage returns. Carriage returns are the \"backslash n\" you see embedded in this',\n",
       " 'string. Sentences have a period at the end, but also, have a space.and words are separated by space.']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"\\. \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "r_splitter.split_text(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "54f73072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"When writing documents, writers will use document structure to group content. This can convey to the reader, which idea's are related. For example,\",\n",
       " 'closely related ideas are in sentances. Similar ideas are in paragraphs. Paragraphs form a document.',\n",
       " 'Paragraphs are often delimited with a carriage return or two carriage returns. Carriage returns are the \"backslash n\" you see embedded in this',\n",
       " 'string. Sentences have a period at the end, but also, have a space.and words are separated by space.']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "r_splitter.split_text(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "73150e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"../data/docs/cs229_lectures/MachineLearning-Lecture01.pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=150,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f76d54cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Páginas del PDF: 22\n",
      "Chunks del PDF: 78\n"
     ]
    }
   ],
   "source": [
    "print(f\"Páginas del PDF: {len(pages)}\")\n",
    "print(f\"Chunks del PDF: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "bc17c017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk: 0\n",
      "{'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creator': 'PScript5.dll Version 5.2.2', 'creationdate': '2008-07-11T11:25:23-07:00', 'author': '', 'moddate': '2008-07-11T11:25:23-07:00', 'title': '', 'source': '../data/docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'total_pages': 22, 'page': 0, 'page_label': '1'}\n",
      "MachineLearning-Lecture01  \n",
      "Instructor (Andrew Ng): Okay. Good morning. Welcome to CS229, the machine \n",
      "learning class. So what I wanna do today is just spend a little time going over the logistics \n",
      "of the class, and then we'll start to talk a bit about machine learning.  \n",
      "By way of introduction, my name's Andrew Ng and I'll be instructor for this class. And so \n",
      "I personally work in machine learning, and I've worked on it for about 15 years now, and \n",
      "I actually think that machine learning is the most exciting field of all the computer \n",
      "sciences. So I'm actually always excited about teaching this class. Sometimes I actually \n",
      "think that machine learning is not only the most exciting thing in computer science, but \n",
      "the most exciting thing in all of human endeavor, so maybe a little bias there.  \n",
      "I also want to introduce the TAs, who are all graduate students doing research in or \n",
      "related to the machine learning and all aspects of machine learning. Paul Baumstarck\n",
      "Chunk: 1\n",
      "{'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creator': 'PScript5.dll Version 5.2.2', 'creationdate': '2008-07-11T11:25:23-07:00', 'author': '', 'moddate': '2008-07-11T11:25:23-07:00', 'title': '', 'source': '../data/docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'total_pages': 22, 'page': 0, 'page_label': '1'}\n",
      "related to the machine learning and all aspects of machine learning. Paul Baumstarck \n",
      "works in machine learning and computer vision. Catie Chang is actually a neuroscientist \n",
      "who applies machine learning algorithms to try to understand the human brain. Tom Do \n",
      "is another PhD student, works in computational biology and in sort of the basic \n",
      "fundamentals of human learning. Zico Kolter is the head TA — he's head TA two years \n",
      "in a row now — works in machine learning a nd applies them to a bunch of robots. And \n",
      "Daniel Ramage is — I guess he's not here  — Daniel applies l earning algorithms to \n",
      "problems in natural language processing.  \n",
      "So you'll get to know the TAs and me much better throughout this quarter, but just from \n",
      "the sorts of things the TA's do, I hope you can already tell that machine learning is a \n",
      "highly interdisciplinary topic in which just the TAs find learning algorithms to problems \n",
      "in computer vision and biology and robots and language. And machine learning is one of\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(docs[:2]):\n",
    "    print(f\"Chunk: {i}\")\n",
    "    print(doc.metadata)\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06c4fff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exploring-langchain",
   "language": "python",
   "name": "exploring-langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

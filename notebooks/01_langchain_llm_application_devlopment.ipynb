{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ae230d7-2ab1-469e-a6a0-238293c1eeb1",
   "metadata": {},
   "source": [
    "# 0 - Librerías y variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d99484c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Librerías\n",
    "# ------------------------------------------------------------------------------\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d680460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY\n",
      "PROXYCURL_API_KEY\n",
      "TAVILY_API_KEY\n",
      "LANGCHAIN_TRACING_V2\n",
      "LANGCHAIN_ENDPOINT\n",
      "LANGCHAIN_API_KEY\n",
      "LANGCHAIN_PROJECT\n"
     ]
    }
   ],
   "source": [
    "# Variables\n",
    "# ------------------------------------------------------------------------------\n",
    "env_vars = dotenv_values()\n",
    "for key in env_vars.keys():\n",
    "    print(key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b042c85b-397e-45fd-92ff-3177d182c1cb",
   "metadata": {},
   "source": [
    "# 1 - LLM Chat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0a5d7ff",
   "metadata": {},
   "source": [
    "En este apartado se llama a la API de OpenAI directamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f09214c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1+1 es igual a 2.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importar OpenAI\n",
    "from openai import OpenAI\n",
    "\n",
    "# Inicializar el cliente\n",
    "client = OpenAI()\n",
    "\n",
    "# Llamar al LLM\n",
    "messages = [{\"role\": \"user\", \"content\": \"¿Cuánto es 1+1?\"}]\n",
    "    \n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=messages,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f6a6be29",
   "metadata": {},
   "source": [
    "Si quiero modificar el código por ejemplo para utilizar un modelo como LLaMA 3.1 desde Ollama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9887075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La respuesta es dos.\n"
     ]
    }
   ],
   "source": [
    "# Define el endpoint local de Ollama\n",
    "OLLAMA_URL = \"http://localhost:11434/api/chat\"\n",
    "\n",
    "# Llamar al LLM\n",
    "messages = [{\"role\": \"user\", \"content\": \"¿Cuánto es 1+1?\"}]\n",
    "\n",
    "response = requests.post(OLLAMA_URL, json={\n",
    "    \"model\": \"llama3.1\",\n",
    "    \"messages\": messages,\n",
    "    \"temperature\": 0.0,\n",
    "    \"stream\": False\n",
    "})\n",
    "\n",
    "data = response.json()\n",
    "print(data[\"message\"][\"content\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e004c686",
   "metadata": {},
   "source": [
    "Cada vez que quiero apuntar a un LLM diferente, tengo que modificar el código de forma sustancial."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88c74f4a",
   "metadata": {},
   "source": [
    "# 2 - LangChain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c809ced",
   "metadata": {},
   "source": [
    "LangChain es más un framework para construir sistemas que usan modelos de lenguaje (LLMs) que una simple librería, a continuación se muestra una tabla con los bloques principales del framework:\n",
    "\n",
    "| **Bloque**                       | **Para qué sirve**                                                                           |\n",
    "| -------------------------------- | -------------------------------------------------------------------------------------------- |\n",
    "| **1. Modelos**                   | Generan texto o representaciones vectoriales (embeddings) a partir de texto                  |\n",
    "| **2. Prompts**                   | Construyen entradas reutilizables, seguras y controladas para los modelos                    |\n",
    "| **3. Output Parsers**            | Transforman la salida del modelo (texto) en estructuras útiles como JSON, listas u objetos   |\n",
    "| **4. Memorias**                  | Guardan el historial o contexto entre interacciones (conversacionales o no)                  |\n",
    "| **5. Tools y Agents**            | Ejecutan funciones externas y permiten que un agente decida dinámicamente qué hacer y cuándo |\n",
    "| **6. Runnables**                 | Unifican cualquier componente ejecutable en un flujo modular y componible                    |\n",
    "| **7. Retrievers y VectorStores** | Permiten búsquedas semánticas para recuperar información relevante desde grandes corpus      |\n",
    "| **8. Chains**                    | Encadenan pasos fijos de procesamiento en flujos controlados (prompt → modelo → parseo)      |\n",
    "| **9. Callbacks y Tracing**       | Monitorizan, trazan y depuran la ejecución para mejorar observabilidad y debugging           |\n",
    "| **10. Loaders y Splitters**      | Cargan y fragmentan documentos largos para su posterior análisis o búsqueda                  |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa5c9eb1",
   "metadata": {},
   "source": [
    "## 2.1. - Modelos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3eeca3d6",
   "metadata": {},
   "source": [
    "En LangChain, los models (modelos) son los componentes que generan texto o responden a mensajes. Son la parte que realmente interactúa con modelos de lenguaje (LLMs) como los de OpenAI, Anthropic, Cohere, etc. LangChain organiza los modelos según lo que hacen. Los más comunes son:\n",
    "\n",
    "| Tipo de modelo     | Qué hace                                                                    | Clase típica                                |\n",
    "| ------------------ | --------------------------------------------------------------------------- | ------------------------------------------- |\n",
    "| **LLM**            | Genera texto a partir de un *prompt plano*                                  | OpenAI, HuggingFaceHub                      |\n",
    "| **ChatModel**      | Maneja conversaciones con roles (usuario, asistente, sistema)               | ChatOpenAI, ChatAnthropic                   |\n",
    "| **EmbeddingModel** | Convierte texto en vectores (útiles para búsquedas o comparación semántica) | OpenAIEmbeddings, HuggingFaceEmbeddings     |\n",
    "\n",
    "\n",
    "LangChain permite llamar a diferentes modelos, con diferentes APIs, de forma agnóstica. Cambiando ligeramente el código, puedo aprovechar una estructura ya creada para apuntar a otro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6287b970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.25\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a22bf983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "061259f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear los mensajes en el formato de LangChain\n",
    "messages = [\n",
    "    HumanMessage(content=\"¿Cuánto es 1+1?\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db336180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La respuesta a la pregunta de \"¿Cuánto es 1+1?\" es 2.\n"
     ]
    }
   ],
   "source": [
    "# Llamar a un modelo A\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1\",\n",
    "    temperature=0.0\n",
    "    )\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b140df4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1+1 es igual a 2.\n"
     ]
    }
   ],
   "source": [
    "# Llamar a un modelo B\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\", \n",
    "    temperature=0.0\n",
    "    )\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2c85cbe",
   "metadata": {},
   "source": [
    "La estructura es exactamente la misma, se podría incluso encapsular el codigo en una función y pasar el modelo como parámetro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04321608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andrew Ng es un científico e investigador en inteligencia artificial y aprendizaje profundo. Es cofundador de Google Brain y Coursera, y ha ocupado puestos de liderazgo en empresas como Baidu y Stanford University. Es conocido por sus contribuciones al campo de la inteligencia artificial y por su trabajo en la popularización de la educación en línea a través de cursos masivos abiertos en línea (MOOCs).\n"
     ]
    }
   ],
   "source": [
    "def call_llm_model(\n",
    "    model_name: str,\n",
    "    temperature: float,\n",
    "    message: str\n",
    ") -> str:\n",
    "    \n",
    "    if model_name.startswith(\"gpt\"):\n",
    "        llm = ChatOpenAI(\n",
    "            model_name=model_name,\n",
    "            temperature=temperature\n",
    "        )\n",
    "    else:\n",
    "        llm = ChatOllama(\n",
    "            model=model_name,\n",
    "            temperature=temperature\n",
    "        )\n",
    "\n",
    "    response = llm.invoke([HumanMessage(content=message)])\n",
    "    return response.content\n",
    "\n",
    "\n",
    "# Ejemplo de uso:\n",
    "respuesta = call_llm_model(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0.5,\n",
    "    message=\"¿Quién es Andrew Ng?\"\n",
    ")\n",
    "\n",
    "print(respuesta)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d28abe4",
   "metadata": {},
   "source": [
    "En las últimas versiones de LangChain, se ha creado la abstracción `init_chat_model()` para inicilizar modelos de diferentes proveedores desde la misma función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbb162aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "# model = init_chat_model(\"claude-3-5-sonnet-latest\", model_provider=\"anthropic\")\n",
    "# model = init_chat_model(\"mistral-large-latest\", model_provider=\"mistralai\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a9b0129",
   "metadata": {},
   "source": [
    "## 2.2. - Prompt Templates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7a67aee",
   "metadata": {},
   "source": [
    "Las Prompt Templates (plantillas de prompt) son una herramienta de LangChain que te ayuda a crear automáticamente los mensajes que le envías al modelo de lenguaje, de una forma organizada, flexible y reutilizable.\n",
    "\n",
    "Cuando trabajas con modelos como GPT, no sueles enviar directamente el texto del usuario al modelo. Normalmente quieres hacer algo más, como:\n",
    "\n",
    "- Agregar instrucciones específicas (ej.: \"Traduce al francés...\")\n",
    "- Dar un contexto adicional al modelo (ej.: \"Eres un traductor especializado en literatura clásica...\")\n",
    "- Formatear el texto de forma especial (ej.: \"Devuelve el resultado en un .json con la estructura...\")\n",
    "- Usar el mismo formato muchas veces con diferentes datos\n",
    "\n",
    "Ahí es donde entran las prompt templates.\n",
    "\n",
    "Piensa en ellas como plantillas con huecos (como los de un formulario). Tú defines una estructura fija y dejas espacios para completar con datos reales más tarde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd77b09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"Traduce el siguiente texto al {language}: {text}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f88974e3",
   "metadata": {},
   "source": [
    "LangChain te permite definir esta plantilla y luego rellenarla automáticamente con los valores que quieras:\n",
    "- `language = \"francés\"`\n",
    "- `text = \"Hola, ¿cómo estás?\"`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68687fc0",
   "metadata": {},
   "source": [
    "Los principales templates y su propósito son:\n",
    "\n",
    "| Prompt Template                    | ¿Para qué sirve?                                                                                                                                   |\n",
    "| ---------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `ChatPromptTemplate`               | Plantilla para **modelos de chat**. Permite combinar mensajes (`System`, `Human`, etc.) fácilmente. Ideal para `ChatOpenAI`, `ChatAnthropic`, etc. |\n",
    "| `PromptTemplate`                   | Plantilla simple de texto plano. Usado con modelos de lenguaje **no conversacionales** como `OpenAI(model=\"text-davinci-003\")`.                    |\n",
    "| `SystemMessagePromptTemplate`      | Subplantilla usada dentro de `ChatPromptTemplate` para definir el **mensaje del sistema**.                                                         |\n",
    "| `HumanMessagePromptTemplate`       | Subplantilla usada dentro de `ChatPromptTemplate` para el **mensaje del usuario**                                                                  |\n",
    "| `MessagesPlaceholder`              | Placeholder especial dentro de `ChatPromptTemplate` para insertar una lista dinámica de mensajes (ej: historial de conversación).                  |\n",
    "| `AIMessagePromptTemplate`          | Subplantilla para simular un mensaje **anterior del asistente** en el historial.                                                                   |\n",
    "| `FewShotPromptTemplate`            | Plantilla para tareas de **few-shot learning**. Permite definir ejemplos que se combinan con el input.                                             |\n",
    "| `ChatMessagePromptTemplate`        | Versión más flexible que permite definir mensajes de un rol personalizado (ej: `\"role\": \"function\"`).                                              |\n",
    "| `FewShotChatMessagePromptTemplate` | Lo mismo que `FewShotPromptTemplate`, pero adaptado para `ChatPromptTemplate`. Útil si quieres ejemplos en formato chat.                           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74d2b4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15e8084e",
   "metadata": {},
   "source": [
    "\n",
    "| Método            | ¿Qué construye?                             | ¿Cuándo usarlo?                                 |\n",
    "| ----------------- | ------------------------------------------- | ----------------------------------------------- |\n",
    "| `from_template()` | Una sola plantilla de mensaje               | Para casos simples o rápidos                    |\n",
    "| `from_messages()` | Una conversación entera con múltiples roles | Cuando necesitas dar contexto o varios mensajes |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76a4a5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plantilla de traducción\n",
    "template = \"Traduce el siguiente texto al {language}: {text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13abd576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para casos sencillos\n",
    "template_string = \"Traduce el siguiente texto al {language}: {text}\"\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27e3d06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para casos mas complejos: sistema, usuarios, asistente...etc\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un trabajador del campo, oriundo de la alpujarra de Granada.\"),\n",
    "    (\"human\", \"Traduce el siguiente texto al {language}: {text}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8528252a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['language', 'text'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Eres un trabajador del campo, oriundo de la alpujarra de Granada.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language', 'text'], input_types={}, partial_variables={}, template='Traduce el siguiente texto al {language}: {text}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a30e830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Eres un trabajador del campo, oriundo de la alpujarra de Granada.'), additional_kwargs={})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67a5b66d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language', 'text'], input_types={}, partial_variables={}, template='Traduce el siguiente texto al {language}: {text}'), additional_kwargs={})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c124e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['language', 'text']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[1].prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ce1c912",
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"Granaíno\"\n",
    "text = \"¿De dónde vienes?\"\n",
    "\n",
    "translation_prompt = prompt_template.format_messages(\n",
    "    language=language,\n",
    "    text=text\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f363829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Eres un trabajador del campo, oriundo de la alpujarra de Granada.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Traduce el siguiente texto al Granaíno: ¿De dónde vienes?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9bf2683b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'langchain_core.messages.system.SystemMessage'>\n",
      "<class 'langchain_core.messages.human.HumanMessage'>\n"
     ]
    }
   ],
   "source": [
    "print(type(translation_prompt))\n",
    "print(type(translation_prompt[0]))\n",
    "print(type(translation_prompt[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "721168c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¿Ande vienes, majo?'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator_response = llm.invoke(translation_prompt)\n",
    "translator_response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10fdba7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¿De do vienes?'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un noble de la realeza española del siglo XV.\"),\n",
    "    (\"human\", \"Traduce esto al {language}: {text}\")\n",
    "])\n",
    "\n",
    "language = \"Castellano antiguo\"\n",
    "text = \"¿De dónde vienes?\"\n",
    "\n",
    "translation_prompt = prompt_template.format_messages(\n",
    "    language=language,\n",
    "    text=text\n",
    "    )\n",
    "\n",
    "customer_response = llm.invoke(translation_prompt)\n",
    "\n",
    "customer_response.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07abd8fe",
   "metadata": {},
   "source": [
    "## 2.3. - Output Parsers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58faea88",
   "metadata": {},
   "source": [
    "En LangChain, los output parsers son herramientas clave para procesar y estructurar las respuestas que se obtienen de los modelos de lenguaje antes de ser utilizadas en otros pasos del flujo de trabajo. Estos parsers te permiten transformar la salida en bruto de los modelos en un formato más adecuado para tu aplicación, ya sea en forma de texto, datos estructurados o incluso en la ejecución de funciones específicas.\n",
    "\n",
    "A continuación, una tabla con los output parsers más comunes en LangChain y su uso principal:\n",
    "\n",
    "| **Output Parser**        | **¿Para qué sirve?**                                                                                                                                                 |\n",
    "| ------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `JsonOutputParser`       | Convierte la salida del modelo en un objeto JSON (estructura de diccionario o lista). Ideal cuando esperas respuestas estructuradas como JSON o diccionarios.        |\n",
    "| `RegexOutputParser`      | Extrae información utilizando expresiones regulares. Se usa cuando la respuesta del modelo sigue un patrón específico que se puede identificar con regex.            |\n",
    "| `StructuredOutputParser` | Permite parsear la salida en estructuras de datos más complejas. Ideal para cuando necesitas que el modelo devuelva datos tabulares o jerárquicos.                   |\n",
    "| `VariableParser`         | Analiza las respuestas del modelo en busca de valores de variables específicas. Utilizado cuando deseas extraer datos de las respuestas para usarlos en otros pasos. |\n",
    "| `TextOutputParser`       | Convierte la salida en un texto plano procesable. Ideal cuando la salida del modelo es simplemente texto sin estructura.                                             |\n",
    "| `SQLOutputParser`        | Convierte la salida del modelo en consultas SQL estructuradas. Es útil si el modelo está generando consultas a bases de datos.                                       |\n",
    "| `PythonOutputParser`     | Convierte la salida del modelo en código Python ejecutable. Utilizado cuando necesitas generar código a partir de la respuesta del modelo.                           |\n",
    "| `ActionOutputParser`     | Permite que la salida sea convertida en una acción específica o una ejecución de función. Ideal para flujos de trabajo más dinámicos donde se deben ejecutar tareas. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "526e9ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': False, 'delivery_days': 5, 'price_value': 'muy asequible!'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    \"gift\": False,\n",
    "    \"delivery_days\": 5,\n",
    "    \"price_value\": \"muy asequible!\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53c9ce45",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review = \"\"\"\\\n",
    "Este soplador de hojas es bastante increíble. Tiene cuatro configuraciones:\\\n",
    "soplador de vela, brisa suave, ciudad ventosa y tornado. \\\n",
    "Llegó en dos días, justo a tiempo para el regalo de aniversario de mi esposa. \\\n",
    "Creo que a mi esposa le gustó tanto que se quedó sin palabras. \\\n",
    "Hasta ahora he sido el único que lo ha usado, y lo he estado usando cada dos mañanas para limpiar las hojas de nuestro césped. \\\n",
    "Es ligeramente más caro que los otros sopladores de hojas que hay por ahí, pero creo que vale la pena por las características adicionales.\n",
    "\"\"\"\n",
    "\n",
    "review_template = \"\"\"\\\n",
    "Para el siguiente texto, extrae la siguiente información:\n",
    "\n",
    "gift: ¿Fue el artículo comprado como un regalo para otra persona? \\\n",
    "Responde True si es sí, False si no o si no se sabe.\n",
    "\n",
    "delivery_days: ¿Cuántos días tardó en llegar el producto? \\\n",
    "Si esta información no se encuentra, devuelve -1.\n",
    "\n",
    "price_value: Extrae cualquier frase sobre el valor o precio,\\\n",
    "y devuélvelas como una lista de Python separada por comas.\n",
    "\n",
    "Formatea la salida como JSON con las siguientes claves:\n",
    "gift --> boolean\n",
    "delivery_days --> integer\n",
    "price_value --> python list\n",
    "\n",
    "texto: {text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6fcaf224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['text'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='Para el siguiente texto, extrae la siguiente información:\\n\\ngift: ¿Fue el artículo comprado como un regalo para otra persona? Responde True si es sí, False si no o si no se sabe.\\n\\ndelivery_days: ¿Cuántos días tardó en llegar el producto? Si esta información no se encuentra, devuelve -1.\\n\\nprice_value: Extrae cualquier frase sobre el valor o precio,y devuélvelas como una lista de Python separada por comas.\\n\\nFormatea la salida como JSON con las siguientes claves:\\ngift --> boolean\\ndelivery_days --> integer\\nprice_value --> python list\\n\\ntexto: {text}\\n'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "497a6778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7b68e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"gift\": true,\n",
      "    \"delivery_days\": 2,\n",
      "    \"price_value\": [\"ligeramente más caro\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "messages = prompt_template.format_messages(text=customer_review)\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57b33cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "11ca7f06",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Si intentamos acceder a response como si fuera un diccionario...\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m(\u001b[33m'\u001b[39m\u001b[33mgift\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "# Si intentamos acceder a response como si fuera un diccionario...\n",
    "response.content.get('gift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dcbe8c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "90e078cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_schemas=[ResponseSchema(name='gift', description='True si fue un regalo, False si no.', type='string'), ResponseSchema(name='delivery_days', description='Número de días que tardó en llegar el producto.', type='string'), ResponseSchema(name='price_value', description='Frases relacionadas con el valor o precio del producto.', type='string')]\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // True si fue un regalo, False si no.\n",
      "\t\"delivery_days\": string  // Número de días que tardó en llegar el producto.\n",
      "\t\"price_value\": string  // Frases relacionadas con el valor o precio del producto.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Definir los esquemas de respuesta\n",
    "response_schemas = [\n",
    "    ResponseSchema(\n",
    "        name=\"gift\",\n",
    "        description=\"True si fue un regalo, False si no.\"\n",
    "    ),\n",
    "    ResponseSchema(\n",
    "        name=\"delivery_days\",\n",
    "        description=\"Número de días que tardó en llegar el producto.\"\n",
    "    ),\n",
    "    ResponseSchema(\n",
    "        name=\"price_value\",\n",
    "        description=\"Frases relacionadas con el valor o precio del producto.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Crear el parser estructurado\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "print(output_parser)\n",
    "print(output_parser.get_format_instructions())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4d44dc1",
   "metadata": {},
   "source": [
    "Cuando se utiliza `response_schemas`, LangChain siempre interpreta que los resultados son string. Por eso en `review_template` se indica:\n",
    "\n",
    "Formatea la salida como JSON con las siguientes claves:\n",
    "- gift --> boolean\n",
    "- delivery_days --> integer\n",
    "- price_value --> python list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "04a36087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: Para el siguiente texto, extrae la siguiente información:\\n\\ngift: ¿Fue el artículo comprado como un regalo para otra persona? Responde True si es sí, False si no o si no se sabe.\\n\\ndelivery_days: ¿Cuántos días tardó en llegar el producto? Si esta información no se encuentra, devuelve -1.\\n\\nprice_value: Extrae cualquier frase sobre el valor o precio,y devuélvelas como una lista de Python separada por comas.\\n\\nFormatea la salida como JSON con las siguientes claves:\\ngift --> boolean\\ndelivery_days --> integer\\nprice_value --> python list\\n\\ntexto: Este soplador de hojas es bastante increíble. Tiene cuatro configuraciones:soplador de vela, brisa suave, ciudad ventosa y tornado. Llegó en dos días, justo a tiempo para el regalo de aniversario de mi esposa. Creo que a mi esposa le gustó tanto que se quedó sin palabras. Hasta ahora he sido el único que lo ha usado, y lo he estado usando cada dos mañanas para limpiar las hojas de nuestro césped. Es ligeramente más caro que los otros sopladores de hojas que hay por ahí, pero creo que vale la pena por las características adicionales.\\n\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construir el prompt con instrucciones de formato\n",
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "\n",
    "formatted_prompt = prompt_template.format(\n",
    "    text=customer_review,\n",
    "    format_instructions=output_parser.get_format_instructions()\n",
    ")\n",
    "\n",
    "formatted_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cbbafffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llamar al modelo\n",
    "response = llm.invoke([HumanMessage(content=formatted_prompt)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f5ac61c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gift': True, 'delivery_days': 2, 'price_value': ['ligeramente más caro']}\n"
     ]
    }
   ],
   "source": [
    "# Parsear la salida\n",
    "parsed_output = output_parser.parse(response.content)\n",
    "\n",
    "# Mostrar el resultado como diccionario\n",
    "print(parsed_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1fa4da39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clave: gift | Valor: True | Tipo de dato: <class 'bool'>\n",
      "Clave: delivery_days | Valor: 2 | Tipo de dato: <class 'int'>\n",
      "Clave: price_value | Valor: ['ligeramente más caro'] | Tipo de dato: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "for key, value in parsed_output.items():\n",
    "    print(f\"Clave: {key} | Valor: {value} | Tipo de dato: {type(value)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5842a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb5be5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c767a12a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "586b5d87",
   "metadata": {},
   "source": [
    "## 2.4. - LangChain: Memory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9957f70f",
   "metadata": {},
   "source": [
    "En LangChain, la memoria se refiere a la capacidad de una cadena o agente para recordar información entre interacciones. Es útil especialmente en contextos conversacionales o multistep, donde necesitas que el modelo tenga conocimiento del contexto anterior.\n",
    "\n",
    "Por defecto, los modelos como GPT no tienen memoria entre llamadas. LangChain permite gestionar esto incluyendo “memoria” en una cadena (Chain) o un agente (Agent), haciendo que el contexto previo se incluya automáticamente en nuevas solicitudes.\n",
    "\n",
    "A continuación, una tabla con los tipos de memoria más comunes en LangChain y su uso principal:\n",
    "\n",
    "| **Tipo de Memoria**              | **¿Para qué sirve?**                                                                                                                                    |\n",
    "| -------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `ConversationBufferMemory`       | Guarda todo el historial de la conversación como una cadena de texto. Es la forma más simple de mantener el contexto completo de una conversación.      |\n",
    "| `ConversationBufferWindowMemory` | Similar a `ConversationBufferMemory`, pero solo conserva las **últimas *k* interacciones** (ventana deslizante). Útil para limitar el contexto enviado. |\n",
    "| `ConversationTokenBufferMemory`  | Guarda la conversación basándose en un **límite de tokens**, no de turnos. Conserva solo la parte del historial que cabe dentro de un número de tokens. |\n",
    "| `ConversationSummaryMemory`      | Resume el historial de la conversación en un texto corto. Ideal para mantener contexto en conversaciones largas sin exceder el límite de tokens.        |\n",
    "\n",
    "Diferencias clave:\n",
    "\n",
    "| Memoria                          | ¿Recuerda todo?       | ¿Controla tamaño? | ¿Resume contenido? | ¿Ideal para...?                               |\n",
    "| -------------------------------- | --------------------- | ----------------- | ------------------ | --------------------------------------------- |\n",
    "| `ConversationBufferMemory`       | Sí                    | ❌                 | ❌                  | Conversaciones cortas o demostraciones        |\n",
    "| `ConversationBufferWindowMemory` | No (solo últimas *k*) | ✅ (*k*)           | ❌                  | Chats donde solo importa el contexto reciente |\n",
    "| `ConversationTokenBufferMemory`  | No                    | ✅ (*n* tokens)    | ❌                  | Ajustar a límites estrictos de tokens         |\n",
    "| `ConversationSummaryMemory`      | No (resume todo)      | ✅                 | ✅                  | Chats largos sin perder el hilo               |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "92d95f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import (\n",
    "    ConversationBufferMemory,\n",
    "    ConversationBufferWindowMemory,\n",
    "    ConversationTokenBufferMemory,\n",
    "    ConversationSummaryBufferMemory\n",
    ")\n",
    "\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.prompts import PromptTemplate\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a67ead6",
   "metadata": {},
   "source": [
    "A continuación vemos un ejemplo, de como los modelos no incorporan memoria de forma nativa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4edb1421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primera respuesta:\n",
      "¡Hola Guille! ¿En qué puedo ayudarte hoy?\n",
      "\n",
      "Segunda respuesta (sin memoria):\n",
      "Lo siento, pero no tengo la capacidad de saber tu nombre a menos que me lo digas. ¿Cuál es tu nombre?\n"
     ]
    }
   ],
   "source": [
    "# Instanciar modelo\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\", \n",
    "    temperature=0.0\n",
    "    )\n",
    "\n",
    "# Primera interacción\n",
    "message = \"Mi nombre es Guille.\"\n",
    "response = llm.invoke([HumanMessage(content=message)])\n",
    "print(\"Primera respuesta:\")\n",
    "print(response.content)\n",
    "\n",
    "# Segunda interacción sin memoria\n",
    "message = \"¿Cuál es mi nombre?\"\n",
    "response = llm.invoke([HumanMessage(content=message)])\n",
    "print(\"\\nSegunda respuesta (sin memoria):\")\n",
    "print(response.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbf07f88",
   "metadata": {},
   "source": [
    "Vamos a darle memoria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ebc6f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_156115/1346084784.py:6: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n",
      "/tmp/ipykernel_156115/1346084784.py:8: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  conversation = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\", \n",
    "    temperature=0.0\n",
    "    )\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4580f2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hola, mi nombre es Guille\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'¡Hola Guille! ¡Encantado de conocerte! ¿En qué puedo ayudarte hoy?'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hola, mi nombre es Guille\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c43dfa5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hola, mi nombre es Guille\n",
      "AI: ¡Hola Guille! ¡Encantado de conocerte! ¿En qué puedo ayudarte hoy?\n",
      "Human: ¿Cuánto es 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1+1 es igual a 2. ¿Hay algo más en lo que pueda ayudarte, Guille?'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"¿Cuánto es 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3e4d3304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hola, mi nombre es Guille\n",
      "AI: ¡Hola Guille! ¡Encantado de conocerte! ¿En qué puedo ayudarte hoy?\n",
      "Human: ¿Cuánto es 1+1?\n",
      "AI: 1+1 es igual a 2. ¿Hay algo más en lo que pueda ayudarte, Guille?\n",
      "Human: ¿Cuál es mi nombre?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Tu nombre es Guille. ¿Hay algo más que te gustaría saber?'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"¿Cuál es mi nombre?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cd56038f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hola, mi nombre es Guille\n",
      "AI: ¡Hola Guille! ¡Encantado de conocerte! ¿En qué puedo ayudarte hoy?\n",
      "Human: ¿Cuánto es 1+1?\n",
      "AI: 1+1 es igual a 2. ¿Hay algo más en lo que pueda ayudarte, Guille?\n",
      "Human: ¿Cuál es mi nombre?\n",
      "AI: Tu nombre es Guille. ¿Hay algo más que te gustaría saber?\n",
      "{'history': 'Human: Hola, mi nombre es Guille\\nAI: ¡Hola Guille! ¡Encantado de conocerte! ¿En qué puedo ayudarte hoy?\\nHuman: ¿Cuánto es 1+1?\\nAI: 1+1 es igual a 2. ¿Hay algo más en lo que pueda ayudarte, Guille?\\nHuman: ¿Cuál es mi nombre?\\nAI: Tu nombre es Guille. ¿Hay algo más que te gustaría saber?'}\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5c3fc2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hola\n",
      "AI: ¿Qué tal?\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "\n",
    "memory.save_context(\n",
    "    {\"input\": \"Hola\"}, \n",
    "    {\"output\": \"¿Qué tal?\"}\n",
    ")\n",
    "\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1a651c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hola\n",
      "AI: ¿Qué tal?\n",
      "Human: No mucho, aquí andamos\n",
      "AI: Guay\n",
      "{'history': 'Human: Hola\\nAI: ¿Qué tal?\\nHuman: No mucho, aquí andamos\\nAI: Guay'}\n"
     ]
    }
   ],
   "source": [
    "memory.save_context(\n",
    "    {\"input\": \"No mucho, aquí andamos\"}, \n",
    "    {\"output\": \"Guay\"}\n",
    ")\n",
    "\n",
    "print(memory.buffer)\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b0be9615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otras opciones de memoria\n",
    "# memory = ConversationBufferWindowMemory(k=1)\n",
    "# memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n",
    "# memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e7a60d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_156115/4275700198.py:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  summary_memory = ConversationSummaryBufferMemory(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hola, mi nombre es Guille, soy de Madrid y estoy aprendiendo a utilizar LangChain\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "🗨️ Respuesta 1:\n",
      "¡Hola Guille! ¡Qué gusto conocerte! Soy un asistente de inteligencia artificial diseñado para ayudarte con LangChain. ¿En qué puedo ayudarte hoy?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human introduces themselves as Guille from Madrid and mentions they are learning to use LangChain.\n",
      "AI: ¡Hola Guille! ¡Qué gusto conocerte! Soy un asistente de inteligencia artificial diseñado para ayudarte con LangChain. ¿En qué puedo ayudarte hoy?\n",
      "Human: Trabajo como ingeniero de datos.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "🗨️ Respuesta 2:\n",
      "¡Qué interesante! Como ingeniero de datos, seguramente estás familiarizado con la manipulación y análisis de grandes conjuntos de datos. ¿Estás buscando alguna información específica sobre LangChain en relación con tu trabajo?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: Guille from Madrid introduces themselves and mentions they are learning to use LangChain. The AI greets Guille and offers assistance with LangChain. Guille reveals they work as a data engineer. The AI acknowledges Guille's expertise in handling and analyzing large datasets and asks if they are seeking specific information about LangChain for their work.\n",
      "Human: ¿Qué sabes de mí?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "🧠 Respuesta 3 (modelo usando resumen):\n",
      "Sé que te llamas Guille, eres de Madrid y estás aprendiendo a usar LangChain. También sé que trabajas como ingeniero de datos y que tienes experiencia en manejar y analizar grandes conjuntos de datos. ¿Hay algo en particular que te interese saber sobre LangChain para tu trabajo?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Crear la memoria resumida (usa otro LLM para resumir, por defecto GPT-3.5)\n",
    "summary_memory = ConversationSummaryBufferMemory(\n",
    "    llm=ChatOpenAI(\n",
    "        temperature=0,\n",
    "        model_name=\"gpt-3.5-turbo\",\n",
    "    ),\n",
    "    max_token_limit=50\n",
    ")\n",
    "\n",
    "# LLM para el chat (se puede utilizar el mismo LLM para ambas cosas, o no)\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model_name=\"gpt-3.5-turbo\"\n",
    ")\n",
    "\n",
    "# Conversación con memoria\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=summary_memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Interacciones\n",
    "respuesta1 = conversation.predict(input=\"Hola, mi nombre es Guille, soy de Madrid y estoy aprendiendo a utilizar LangChain\")\n",
    "print(\"\\n🗨️ Respuesta 1:\")\n",
    "print(respuesta1)\n",
    "\n",
    "respuesta2 = conversation.predict(input=\"Trabajo como ingeniero de datos.\")\n",
    "print(\"\\n🗨️ Respuesta 2:\")\n",
    "print(respuesta2)\n",
    "\n",
    "respuesta3 = conversation.predict(input=\"¿Qué sabes de mí?\")\n",
    "print(\"\\n🧠 Respuesta 3 (modelo usando resumen):\")\n",
    "print(respuesta3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1fe29f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: Guille from Madrid introduces themselves and mentions they are learning to use LangChain. The AI greets Guille and offers assistance with LangChain. Guille reveals they work as a data engineer. The AI acknowledges Guille's expertise in handling and analyzing large datasets and asks if they are seeking specific information about LangChain for their work. The AI summarizes Guille's background and offers to provide information about LangChain tailored to their expertise and needs.\n",
      "{'history': \"System: Guille from Madrid introduces themselves and mentions they are learning to use LangChain. The AI greets Guille and offers assistance with LangChain. Guille reveals they work as a data engineer. The AI acknowledges Guille's expertise in handling and analyzing large datasets and asks if they are seeking specific information about LangChain for their work. The AI summarizes Guille's background and offers to provide information about LangChain tailored to their expertise and needs.\"}\n"
     ]
    }
   ],
   "source": [
    "print(summary_memory.buffer)\n",
    "print(summary_memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07a436c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fee3b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26807d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a9ffdf6",
   "metadata": {},
   "source": [
    "## 2.5. - Chains"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f154ff8d",
   "metadata": {},
   "source": [
    "En LangChain, una Chain (cadena) es una composición de pasos que conecta modelos de lenguaje con otras herramientas como funciones, bases de datos, prompts, parsers, memoria, etc. En lugar de lanzar una sola petición a un LLM, puedes encadenar operaciones de forma estructurada y reutilizable.\n",
    "\n",
    "En otras palabras, una Chain permite definir flujos de trabajo con lógica.\n",
    "\n",
    "A continuación, una tabla resumen de las Chains más comunes en LangChain y su uso principal:\n",
    "\n",
    "| **Chain**               | **¿Para qué sirve?**                                                                                                                                              |\n",
    "| ----------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `LLMChain`              | Es la cadena más simple. Conecta un `PromptTemplate` con un LLM. Ideal para tareas de entrada/salida básicas.                                                     |\n",
    "| `SequentialChain`       | Permite ejecutar varias `Chains` de forma secuencial, pasando la salida de una como entrada de la siguiente. Útil para flujos paso a paso.                        |\n",
    "| `SimpleSequentialChain` | Variante de `SequentialChain` más sencilla, que solo encadena directamente la salida de un paso como entrada al siguiente (sin nombres de variables intermedias). |\n",
    "| `RouterChain`           | Redirige automáticamente la entrada a distintas sub-chains según su contenido. Ideal para sistemas multi-agente o flujos condicionados.                           |\n",
    "| `ConversationChain`     | Incorpora memoria conversacional (como `BufferMemory`). Ideal para construir asistentes o chatbots con contexto.                                                  |\n",
    "| `TransformChain`        | Aplica una transformación Python entre pasos. Útil para parsear, preprocesar o postprocesar datos entre modelos.                                                  |\n",
    "| `MapReduceChain`        | Divide una tarea entre varios modelos (Map), luego une las respuestas (Reduce). Útil para resumir o analizar muchos documentos.                                   |\n",
    "| `RetrievalQAChain`      | Combina un LLM con un buscador de documentos. Extrae contexto relevante antes de preguntar. Ideal para RAG.                                                       |\n",
    "\n",
    "Las anteriores Chains son desarrolladas por LangChain, pero el usuario también puede definir sus propias chains encadenando diferentes modelos, templates, parsers...etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d86e9384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain, SequentialChain\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cee1461f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nombre</th>\n",
       "      <th>sector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EcoWave</td>\n",
       "      <td>energía</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NeuroByte</td>\n",
       "      <td>tecnología</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PetNanny</td>\n",
       "      <td>mascotas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Foodloop</td>\n",
       "      <td>alimentación</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      nombre        sector\n",
       "0    EcoWave       energía\n",
       "1  NeuroByte    tecnología\n",
       "2   PetNanny      mascotas\n",
       "3   Foodloop  alimentación"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    \"nombre\": [\"EcoWave\", \"NeuroByte\", \"PetNanny\", \"Foodloop\"],\n",
    "    \"sector\": [\"energía\", \"tecnología\", \"mascotas\", \"alimentación\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ebd50c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciar modelo\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0.9\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1dc4886e",
   "metadata": {},
   "source": [
    "### 2.5.1. - LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9d2cba8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_156115/1313909526.py:6: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  slogan_chain = LLMChain(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'nombre':       nombre\n",
       " 0    EcoWave\n",
       " 1  NeuroByte\n",
       " 2   PetNanny\n",
       " 3   Foodloop,\n",
       " 'sector':          sector\n",
       " 0       energía\n",
       " 1    tecnología\n",
       " 2      mascotas\n",
       " 3  alimentación,\n",
       " 'eslogan': '1. EcoWave: \"Haciendo olas verdes por un mundo más sostenible\"\\n2. NeuroByte: \"Conectando mentes para un futuro brillante\"\\n3. PetNanny: \"Cuidando a tu peludo como si fuera nuestro\"\\n4. Foodloop del sector energía: \"Revolucionando la forma en que utilizamos la energía\"\\n5. Foodloop del sector tecnología: \"Alimentando la innovación con cada bocado\"\\n6. Foodloop del sector mascotas: \"Sabores que hacen ronronear a tu mejor amigo\"\\n7. Foodloop del sector alimentación: \"Un ciclo de nutrición sostenible para todos\"'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chain para generar eslogan\n",
    "slogan_prompt = PromptTemplate.from_template(\n",
    "    \"Inventa un eslogan creativo para una startup llamada {nombre} del sector {sector}.\"\n",
    ")\n",
    "\n",
    "slogan_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=slogan_prompt,\n",
    "    output_key=\"eslogan\"\n",
    ")\n",
    "\n",
    "slogan_chain.invoke({\n",
    "    \"nombre\": df[['nombre']],\n",
    "    \"sector\": df[['sector']]\n",
    "})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a0009f6",
   "metadata": {},
   "source": [
    "### 2.5.2. - SimpleSequentialChain\n",
    "\n",
    "Una SimpleSequentialChain es un conjunto de chains secuenciales, donde cada chain coge como input el output de la chain precedente sin indicar el nombre de los inputs/outputs de forma explícita. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8e8dc6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain para generar eslogan\n",
    "slogan_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Inventa un eslogan creativo para una startup llamada {nombre}\"\n",
    ")\n",
    "\n",
    "slogan_chain = LLMChain(llm=llm, prompt=slogan_prompt)\n",
    "\n",
    "\n",
    "# Chain para campaña\n",
    "campaign_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Dado el eslogan {eslogan}, escribe una breve idea para una campaña publicitaria que transmita ese mensaje.\"\n",
    ")\n",
    "\n",
    "campaign_chain = LLMChain(llm=llm, prompt=campaign_prompt)\n",
    "\n",
    "\n",
    "# Flujo combinado\n",
    "overall_simple_chain = SimpleSequentialChain(\n",
    "    chains=[slogan_chain, campaign_chain],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a398b8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_156115/3648311525.py:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  overall_simple_chain.run(df[:1][['nombre']])\n",
      "The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error in LangChainTracer.on_chain_start callback: ValueError('The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().')\n",
      "Error in LangChainTracer.on_chain_start callback: ValueError('The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\"Con EcoWave, haz olas de cambio eco-friendly en el mundo\"\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mUna campaña publicitaria creativa para transmitir el mensaje de EcoWave podría incluir anuncios en redes sociales y sitios web destacando la importancia de hacer cambios sostenibles para proteger nuestro planeta. Se podrían utilizar imágenes impactantes de océanos limpios y playas libres de basura, junto con frases motivadoras como \"Con EcoWave, juntos podemos crear un mundo más verde\" o \"Haz una ola de cambio eco-friendly con EcoWave\". Además, se podrían organizar eventos de limpieza de playas y charlas educativas para involucrar a la comunidad en la causa.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Una campaña publicitaria creativa para transmitir el mensaje de EcoWave podría incluir anuncios en redes sociales y sitios web destacando la importancia de hacer cambios sostenibles para proteger nuestro planeta. Se podrían utilizar imágenes impactantes de océanos limpios y playas libres de basura, junto con frases motivadoras como \"Con EcoWave, juntos podemos crear un mundo más verde\" o \"Haz una ola de cambio eco-friendly con EcoWave\". Además, se podrían organizar eventos de limpieza de playas y charlas educativas para involucrar a la comunidad en la causa.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_simple_chain.run(df[:1][['nombre']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2c7b14a",
   "metadata": {},
   "source": [
    "### 2.5.3. - SequentialChain\n",
    "\n",
    "Similar a SimpleSequentialChain, pero indicando los nombres de inputs y outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "81ce0ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain para generar eslogan\n",
    "slogan_prompt = PromptTemplate.from_template(\n",
    "    \"Inventa un eslogan creativo para una startup llamada {nombre} del sector {sector}.\"\n",
    ")\n",
    "\n",
    "slogan_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=slogan_prompt,\n",
    "    output_key=\"eslogan\"\n",
    ")\n",
    "\n",
    "\n",
    "# Chain para campaña\n",
    "campaign_prompt = PromptTemplate.from_template(\n",
    "    \"Dado el eslogan {eslogan}, escribe una breve idea para una campaña publicitaria que transmita ese mensaje.\"\n",
    ")\n",
    "\n",
    "campaign_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=campaign_prompt,\n",
    "    output_key=\"campaña\"\n",
    ")\n",
    "\n",
    "# Flujo combinado\n",
    "sequential_chain = SequentialChain(\n",
    "    chains=[slogan_chain, campaign_chain],\n",
    "    input_variables=[\"nombre\", \"sector\"],\n",
    "    output_variables=[\"eslogan\", \"campaña\"],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6f72b29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Lista para guardar resultados\n",
    "resultados = []\n",
    "\n",
    "# Aplicar la chain a cada fila\n",
    "for _, fila in df.iterrows():\n",
    "\n",
    "    entrada = {\n",
    "        \"nombre\": fila[\"nombre\"],\n",
    "        \"sector\": fila[\"sector\"]\n",
    "    }\n",
    "\n",
    "    salida = sequential_chain.invoke(entrada)\n",
    "    \n",
    "    resultados.append({\n",
    "        **entrada,\n",
    "        **salida  # esto añade 'eslogan' y 'campaña'\n",
    "    })\n",
    "\n",
    "# Nuevo DataFrame con resultados\n",
    "df_resultado = pd.DataFrame(resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a909b062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nombre</th>\n",
       "      <th>sector</th>\n",
       "      <th>eslogan</th>\n",
       "      <th>campaña</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EcoWave</td>\n",
       "      <td>energía</td>\n",
       "      <td>\"Transformando olas en energía limpia para un ...</td>\n",
       "      <td>Imagínate un mundo donde las olas del mar no s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NeuroByte</td>\n",
       "      <td>tecnología</td>\n",
       "      <td>\"Con NeuroByte, tu mente es el límite. ¡Conect...</td>\n",
       "      <td>La campaña publicitaria podría mostrar a difer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PetNanny</td>\n",
       "      <td>mascotas</td>\n",
       "      <td>\"Tu mejor amigo está en buenas manos con PetNa...</td>\n",
       "      <td>La campaña publicitaria se centraría en mostra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Foodloop</td>\n",
       "      <td>alimentación</td>\n",
       "      <td>\"¡Con Foodloop, cada bocado es un viaje de sab...</td>\n",
       "      <td>La campaña publicitaria podría consistir en un...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      nombre        sector                                            eslogan  \\\n",
       "0    EcoWave       energía  \"Transformando olas en energía limpia para un ...   \n",
       "1  NeuroByte    tecnología  \"Con NeuroByte, tu mente es el límite. ¡Conect...   \n",
       "2   PetNanny      mascotas  \"Tu mejor amigo está en buenas manos con PetNa...   \n",
       "3   Foodloop  alimentación  \"¡Con Foodloop, cada bocado es un viaje de sab...   \n",
       "\n",
       "                                             campaña  \n",
       "0  Imagínate un mundo donde las olas del mar no s...  \n",
       "1  La campaña publicitaria podría mostrar a difer...  \n",
       "2  La campaña publicitaria se centraría en mostra...  \n",
       "3  La campaña publicitaria podría consistir en un...  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_resultado.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7230b1c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61a7317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167b755f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be7e233f",
   "metadata": {},
   "source": [
    "## 2.6. - Vector Stores y Retrievers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "636168ab",
   "metadata": {},
   "source": [
    "**Vector Store:**\n",
    "\n",
    "Un vector store es una base de datos optimizada para almacenar y buscar vectores, que son representaciones numéricas de datos, como texto.\n",
    "\n",
    "Cuando conviertes texto en vectores usando un modelo de embeddings, esos vectores se pueden almacenar en un vector store. Posteriormente, puedes hacer búsquedas por similitud: das un vector (por ejemplo, el de una pregunta del usuario) y el sistema devuelve los vectores más cercanos, que corresponden a los textos más relevantes.\n",
    "\n",
    "Es la infraestructura que permite búsquedas semánticas rápidas y eficientes.\n",
    "\n",
    "**Retrievers:**\n",
    "\n",
    "Un retriever es un componente que, dado un texto de entrada (como una pregunta), recupera los documentos más relevantes desde alguna fuente. Es una abstracción de LangChain que encapsula la lógica de recuperación, sin preocuparse por cómo están almacenados los datos. El objetivo es simplemente: \"dame lo más relevante que tengas sobre esta consulta\".\n",
    "\n",
    "Puede estar respaldado por un vector store, un motor de búsqueda clásico, una API externa, o cualquier otra fuente.\n",
    "\n",
    "Cuando juntas un vector store con un retriever, lo que haces es usar el vector store como backend para que el retriever recupere documentos basándose en similitud semántica. De este modo se puede implementar un RAG, y darle un conocimiento extra al LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "334a4764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "569bbe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loader de LangChain que lee el CSV y convierte cada fila en un documento\n",
    "file = './data/OutdoorClothingCatalog_1000.csv'\n",
    "loader = CSVLoader(file_path=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "55ec1853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.csv_loader.CSVLoader at 0x7fb218169410>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f64a3ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './data/OutdoorClothingCatalog_1000.csv', 'row': 0}, page_content=\": 0\\nname: Women's Campside Oxfords\\ndescription: This ultracomfortable lace-to-toe Oxford boasts a super-soft canvas, thick cushioning, and quality construction for a broken-in feel from the first time you put them on. \\n\\nSize & Fit: Order regular shoe size. For half sizes not offered, order up to next whole size. \\n\\nSpecs: Approx. weight: 1 lb.1 oz. per pair. \\n\\nConstruction: Soft canvas material for a broken-in feel and look. Comfortable EVA innersole with Cleansport NXT® antimicrobial odor control. Vintage hunt, fish and camping motif on innersole. Moderate arch contour of innersole. EVA foam midsole for cushioning and support. Chain-tread-inspired molded rubber outsole with modified chain-tread pattern. Imported. \\n\\nQuestions? Please contact us for any inquiries.\"),\n",
       " Document(metadata={'source': './data/OutdoorClothingCatalog_1000.csv', 'row': 1}, page_content=': 1\\nname: Recycled Waterhog Dog Mat, Chevron Weave\\ndescription: Protect your floors from spills and splashing with our ultradurable recycled Waterhog dog mat made right here in the USA. \\n\\nSpecs\\nSmall - Dimensions: 18\" x 28\". \\nMedium - Dimensions: 22.5\" x 34.5\".\\n\\nWhy We Love It\\nMother nature, wet shoes and muddy paws have met their match with our Recycled Waterhog mats. Ruggedly constructed from recycled plastic materials, these ultratough mats help keep dirt and water off your floors and plastic out of landfills, trails and oceans. Now, that\\'s a win-win for everyone.\\n\\nFabric & Care\\nVacuum or hose clean.\\n\\nConstruction\\n24 oz. polyester fabric made from 94% recycled materials.\\nRubber backing.\\n\\nAdditional Features\\nFeatures an -exclusive design.\\nFeatures thick and thin fibers for scraping dirt and absorbing water.\\nDries quickly and resists fading, rotting, mildew and shedding.\\nUse indoors or out.\\nMade in the USA.\\n\\nHave questions? Reach out to our customer service team with any questions you may have.')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "docs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9d37b324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n",
      "[-0.04553837701678276, 0.014218204654753208, -0.010751419700682163, -0.01405520923435688, -0.018155166879296303]\n"
     ]
    }
   ],
   "source": [
    "# Instanciar embedding\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Convertir string a vector mediante un embedding\n",
    "embed = embeddings.embed_query(\"Hola mi nombre es Guille\")\n",
    "\n",
    "# Visualizar embedding\n",
    "print(len(embed))\n",
    "print(embed[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "72a07ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gmachin/.local/share/virtualenvs/exploring-langchain-UOzDUSui/lib/python3.11/site-packages/pydantic/_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "# Crear VectorStore en memoria\n",
    "db = DocArrayInMemorySearch.from_documents(\n",
    "    docs, \n",
    "    embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8d740d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# Query en lenguaje natural\n",
    "query = \"Please suggest a shirt with sunblocking\"\n",
    "\n",
    "# Llamar al VectorStore\n",
    "docs_result = db.similarity_search(query, k=4)\n",
    "\n",
    "# Documentos relevantes\n",
    "print(len(docs_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "97053224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': './data/OutdoorClothingCatalog_1000.csv', 'row': 255}, page_content=': 255\\nname: Sun Shield Shirt by\\ndescription: \"Block the sun, not the fun – our high-performance sun shirt is guaranteed to protect from harmful UV rays. \\n\\nSize & Fit: Slightly Fitted: Softly shapes the body. Falls at hip.\\n\\nFabric & Care: 78% nylon, 22% Lycra Xtra Life fiber. UPF 50+ rated – the highest rated sun protection possible. Handwash, line dry.\\n\\nAdditional Features: Wicks moisture for quick-drying comfort. Fits comfortably over your favorite swimsuit. Abrasion resistant for season after season of wear. Imported.\\n\\nSun Protection That Won\\'t Wear Off\\nOur high-performance fabric provides SPF 50+ sun protection, blocking 98% of the sun\\'s harmful rays. This fabric is recommended by The Skin Cancer Foundation as an effective UV protectant.')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7318c147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': './data/OutdoorClothingCatalog_1000.csv', 'row': 374}, page_content=\": 374\\nname: Men's Plaid Tropic Shirt, Short-Sleeve\\ndescription: Our Ultracomfortable sun protection is rated to UPF 50+, helping you stay cool and dry. Originally designed for fishing, this lightest hot-weather shirt offers UPF 50+ coverage and is great for extended travel. SunSmart technology blocks 98% of the sun's harmful UV rays, while the high-performance fabric is wrinkle-free and quickly evaporates perspiration. Made with 52% polyester and 48% nylon, this shirt is machine washable and dryable. Additional features include front and back cape venting, two front bellows pockets and an imported design. With UPF 50+ coverage, you can limit sun exposure and feel secure with the highest rated sun protection available.\")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_result[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3ca00951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encadenar todas las respuestas en un string\n",
    "qdocs = \"\".join([docs_result[i].page_content for i in range(len(docs_result))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "87d5ca60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "': 255\\nname: Sun Shield Shirt by\\ndescription: \"Block the sun, not the fun – our high-performance sun shirt is guaranteed to protect from harmful UV rays. \\n\\nSize & Fit: Slightly Fitted: Softly shapes the body. Falls at hip.\\n\\nFabric & Care: 78% nylon, 22% Lycra Xtra Life fiber. UPF 50+ rated – the highest rated sun protection possible. Handwash, line dry.\\n\\nAdditional Features: Wicks moisture for quick-drying comfort. Fits comfortably over your favorite swimsuit. Abrasion resistant for season after season of wear. Imported.\\n\\nSun Protection That Won\\'t Wear Off\\nOur high-performance fabric provides SPF 50+ sun protection, blocking 98% of the sun\\'s harmful rays. This fabric is recommended by The Skin Cancer Foundation as an effective UV protectant.: 374\\nname: Men\\'s Plaid Tropic Shirt, Short-Sleeve\\ndescription: Our Ultracomfortable sun protection is rated to UPF 50+, helping you stay cool and dry. Originally designed for fishing, this lightest hot-weather shirt offers UPF 50+ coverage and is great for extended travel. SunSmart technology blocks 98% of the sun\\'s harmful UV rays, while the high-performance fabric is wrinkle-free and quickly evaporates perspiration. Made with 52% polyester and 48% nylon, this shirt is machine washable and dryable. Additional features include front and back cape venting, two front bellows pockets and an imported design. With UPF 50+ coverage, you can limit sun exposure and feel secure with the highest rated sun protection available.: 535\\nname: Men\\'s TropicVibe Shirt, Short-Sleeve\\ndescription: This Men’s sun-protection shirt with built-in UPF 50+ has the lightweight feel you want and the coverage you need when the air is hot and the UV rays are strong. Size & Fit: Traditional Fit: Relaxed through the chest, sleeve and waist. Fabric & Care: Shell: 71% Nylon, 29% Polyester. Lining: 100% Polyester knit mesh. UPF 50+ rated – the highest rated sun protection possible. Machine wash and dry. Additional Features: Wrinkle resistant. Front and back cape venting lets in cool breezes. Two front bellows pockets. Imported.\\n\\nSun Protection That Won\\'t Wear Off: Our high-performance fabric provides SPF 50+ sun protection, blocking 98% of the sun\\'s harmful rays.: 618\\nname: Men\\'s Tropical Plaid Short-Sleeve Shirt\\ndescription: Our lightest hot-weather shirt is rated UPF 50+ for superior protection from the sun\\'s UV rays. With a traditional fit that is relaxed through the chest, sleeve, and waist, this fabric is made of 100% polyester and is wrinkle-resistant. With front and back cape venting that lets in cool breezes and two front bellows pockets, this shirt is imported and provides the highest rated sun protection possible. \\n\\nSun Protection That Won\\'t Wear Off. Our high-performance fabric provides SPF 50+ sun protection, blocking 98% of the sun\\'s harmful rays.'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "306f4b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancia LLM\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.0,\n",
    "    model=\"gpt-3.5-turbo\"\n",
    "    )\n",
    "\n",
    "# Prompt\n",
    "prompt = f\"\"\"\n",
    "I will give you a list of product entries. Each one starts with 'name:' and includes a 'description:'.\n",
    "Extract all shirts with sun protection (UPF/UV) and return a markdown table with:\n",
    "\n",
    "| Name | Summary of Sun Protection Features |\n",
    "\n",
    "Here are the entries:\n",
    "\n",
    "{qdocs}\n",
    "\"\"\"\n",
    "\n",
    "# Invocar al LLM con el prompt\n",
    "response = llm.invoke(prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "76cf1652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"| Name | Summary of Sun Protection Features |\\n| --- | --- |\\n| Sun Shield Shirt | UPF 50+ rated sun protection, blocks 98% of harmful UV rays |\\n| Men's Plaid Tropic Shirt | UPF 50+ rated sun protection, blocks 98% of harmful UV rays |\\n| Men's TropicVibe Shirt | UPF 50+ rated sun protection, blocks 98% of harmful UV rays |\\n| Men's Tropical Plaid Short-Sleeve Shirt | UPF 50+ rated sun protection, blocks 98% of harmful UV rays |\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2d0ae057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Name | Summary of Sun Protection Features |\n",
       "| --- | --- |\n",
       "| Sun Shield Shirt | UPF 50+ rated sun protection, blocks 98% of harmful UV rays |\n",
       "| Men's Plaid Tropic Shirt | UPF 50+ rated sun protection, blocks 98% of harmful UV rays |\n",
       "| Men's TropicVibe Shirt | UPF 50+ rated sun protection, blocks 98% of harmful UV rays |\n",
       "| Men's Tropical Plaid Short-Sleeve Shirt | UPF 50+ rated sun protection, blocks 98% of harmful UV rays |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "20d19cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Convertir VectorStore en un retriever compatible con LangChain\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# RAG\n",
    "qa_stuff = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "query =  \"\"\"\n",
    "I will give you a list of product entries. Each one starts with 'name:' and includes a 'description:'.\n",
    "Extract all shirts with sun protection (UPF/UV) and return a markdown table with:\n",
    "\n",
    "| Name | Summary of Sun Protection Features |\n",
    "\"\"\"\n",
    "\n",
    "response = qa_stuff.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9b75dd6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Name | Summary of Sun Protection Features |\n",
       "| --- | --- |\n",
       "| Sun Shield Shirt | SPF 50+ sun protection, blocks 98% of harmful rays |\n",
       "| Men's Tropical Plaid Short-Sleeve Shirt | SPF 50+ sun protection, blocks 98% of harmful rays |\n",
       "| Men's TropicVibe Shirt, Short-Sleeve | SPF 50+ sun protection, blocks 98% of harmful rays |\n",
       "| Men's Plaid Tropic Shirt, Short-Sleeve | UPF 50+ sun protection, blocks 98% of harmful rays |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ad8166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999dd232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a23d93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1c58552",
   "metadata": {},
   "source": [
    "## 2.7. - Agents y Tools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12733c36",
   "metadata": {},
   "source": [
    "**Tools:**\n",
    "\n",
    "Una Tool en LangChain es una función externa que un modelo de lenguaje puede usar como si fuera una \"extensión de sus capacidades\". Las tools permiten que el LLM haga cosas que no puede hacer solo, como:\n",
    "- Consultar una API externa (por ejemplo, una búsqueda en Google).\n",
    "- Realizar cálculos matemáticos.\n",
    "- Recuperar documentos con un retriever.\n",
    "- Llamar a un sistema de base de datos, ejecutando SQL.\n",
    "\n",
    "Cada Tool tiene tres partes básicas:\n",
    "- Nombre\n",
    "- Descripción (natural language prompt)\n",
    "- Función ejecutable\n",
    "\n",
    "Las tools son como comandos que el LLM puede invocar si se le entrena para reconocerlas y saber cuándo usarlas.\n",
    "\n",
    "Importante: Las Tools por sí solas no hacen nada. Deben ser invocadas explícitamente, ya sea manualmente o por medio de un agente.\n",
    "\n",
    "\n",
    "**Agents:**\n",
    "\n",
    "Un Agent en LangChain es como un asistente inteligente que sabe tomar decisiones paso a paso. No solo genera texto como los modelos normales, sino que también puede pensar qué necesita hacer, elegir herramientas disponibles y usarlas para conseguir una respuesta completa. Es una de las piezas más avanzadas del framework.\n",
    "\n",
    "El agente se comporta como un sistema reflexivo. Se le da un objetivo general (por ejemplo, “responde cualquier pregunta usando las herramientas disponibles”) y él decide, paso a paso:\n",
    "- Qué información necesita para cumplir el objetivo.\n",
    "- Qué Tool puede ayudar a conseguirla.\n",
    "- Qué input debe darle a esa Tool.\n",
    "- Qué hacer con la respuesta obtenida.\n",
    "- Si necesita seguir usando otras Tools o ya puede generar la respuesta final.\n",
    "\n",
    "Este proceso ocurre en bucles tipo:\n",
    "\n",
    "```\n",
    "Thought: Necesito buscar datos sobre X.\n",
    "Action: Llamo a la Tool \"GoogleSearch\" con input \"X\"\n",
    "Observation: La tool devuelve la info.\n",
    "... (nuevo pensamiento)\n",
    "Final Answer: Aquí está la respuesta completa.\n",
    "```\n",
    "\n",
    "Este patrón se llama \"actuar → observar → razonar → actuar\" y permite a los agentes resolver problemas complejos de forma iterativa, con autonomía limitada.\n",
    "\n",
    "Por eso, los Agents no son simplemente una herramienta más, sino el orquestador que decide qué hacer, cuándo y cómo. Ellos ejecutan lógica de alto nivel usando las tools como extensiones de sus capacidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bb513928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "from langchain.agents import load_tools, initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
    "from langchain_experimental.tools import PythonREPLTool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b273cd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_156115/2320382436.py:11: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent= initialize_agent(\n"
     ]
    }
   ],
   "source": [
    "# Instancia LLM\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.0,\n",
    "    model=\"gpt-3.5-turbo\"\n",
    "    )\n",
    "\n",
    "# Instanciar tools\n",
    "tools = load_tools([\"llm-math\",\"wikipedia\"], llm=llm)\n",
    "\n",
    "# Instanciar Agent\n",
    "agent= initialize_agent(\n",
    "    tools, \n",
    "    llm, \n",
    "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    handle_parsing_errors=True,\n",
    "    verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "075e7d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: We can use a calculator to find 25% of 300.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Calculator\",\n",
      "  \"action_input\": \"25% of 300\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAnswer: 75.0\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: 75.0\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the 25% of 300?', 'output': '75.0'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo calculadora\n",
    "agent.invoke(\"What is the 25% of 300?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "958b4669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I can use Wikipedia to find out which book Tom M. Mitchell wrote.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"wikipedia\",\n",
      "  \"action_input\": \"Tom M. Mitchell\"\n",
      "}\n",
      "```\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gmachin/.local/share/virtualenvs/exploring-langchain-UOzDUSui/lib/python3.11/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /home/gmachin/.local/share/virtualenvs/exploring-langchain-UOzDUSui/lib/python3.11/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mPage: Tom M. Mitchell\n",
      "Summary: Tom Michael Mitchell (born August 9, 1951) is an American computer scientist and the Founders University Professor at Carnegie Mellon University (CMU). He is a founder and former chair of the Machine Learning Department at CMU. Mitchell is known for his contributions to the advancement of machine learning, artificial intelligence, and cognitive neuroscience and is the author of the textbook Machine Learning. He is a member of the United States National Academy of Engineering since 2010. He is also a Fellow of the American Academy of Arts and Sciences, the American Association for the Advancement of Science and a Fellow and past president of the Association for the Advancement of Artificial Intelligence. In October 2018, Mitchell was appointed as the Interim Dean of the School of Computer Science at Carnegie Mellon.\n",
      "\n",
      "Page: Tom Mitchell (Australian footballer)\n",
      "Summary: Thomas Mitchell (born 31 May 1993) is a professional Australian rules footballer playing for the Collingwood Football Club in the Australian Football League (AFL). He previously played for the Sydney Swans from 2012 to 2016, and the Hawthorn Football Club between 2017 and 2022. Mitchell won the Brownlow Medal as the league's best and fairest player in 2018 and set the record for the most disposals in a VFL/AFL match, accruing 54 in a game against Collingwood during that season. He would later join them in 2023, en route to winning the 2023 AFL Grand Final and his first AFL premiership.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have found the information about the book written by Tom M. Mitchell.\n",
      "Final Answer: The book written by Tom M. Mitchell is \"Machine Learning.\"\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Tom M. Mitchell is an American computer scientist and the Founders University Professor at Carnegie Mellon University (CMU)what book did he write?',\n",
       " 'output': 'The book written by Tom M. Mitchell is \"Machine Learning.\"'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo wikipedia\n",
    "agent.invoke(\"Tom M. Mitchell is an American computer scientist \\\n",
    "and the Founders University Professor at Carnegie Mellon University (CMU)\\\n",
    "what book did he write?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8615743d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mWe can use the `sorted()` function in Python to sort the list of customers based on last name first and then first name.\n",
      "Action: Python_REPL\n",
      "Action Input: sorted([['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']], key=lambda x: (x[1], x[0]))\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe customers are now sorted by last name and then first name.\n",
      "Final Answer: [['Jen', 'Ayai'], ['Harrison', 'Chase'], ['Lang', 'Chain'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Dolly', 'Too']]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\",\n",
       " 'output': \"[['Jen', 'Ayai'], ['Harrison', 'Chase'], ['Lang', 'Chain'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Dolly', 'Too']]\"}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instanciar Agent\n",
    "agent = create_python_agent(\n",
    "    llm,\n",
    "    tool=PythonREPLTool(),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "customer_list = [\n",
    "    [\"Harrison\", \"Chase\"], \n",
    "    [\"Lang\", \"Chain\"],\n",
    "    [\"Dolly\", \"Too\"],\n",
    "    [\"Elle\", \"Elem\"], \n",
    "    [\"Geoff\",\"Fusion\"], \n",
    "    [\"Trance\",\"Former\"],\n",
    "    [\"Jen\",\"Ayai\"]\n",
    "]\n",
    "\n",
    "agent.invoke(f\"\"\"Sort these customers by \\\n",
    "last name and then first name \\\n",
    "and print the output: {customer_list}\"\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b53f9db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\",\n",
      "  \"agent_scratchpad\": \"\",\n",
      "  \"stop\": [\n",
      "    \"\\nObservation:\",\n",
      "    \"\\n\\tObservation:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an agent designed to write and execute python code to answer questions.\\nYou have access to a python REPL, which you can use to execute python code.\\nIf you get an error, debug your code and try again.\\nOnly use the output of your code to answer the question. \\nYou might know the answer without running any code, but you should still run the code to get the answer.\\nIf it does not seem like you can write code to answer the question, just return \\\"I don't know\\\" as the answer.\\n\\n\\nPython_REPL - A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [Python_REPL]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\nThought:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatOpenAI] [5.58s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"We can use the `sorted()` function in Python to sort the list of customers based on last name first and then first name.\\nAction: Python_REPL\\nAction Input: sorted([['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']], key=lambda x: (x[1], x[0]))\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 104,\n",
      "                \"prompt_tokens\": 328,\n",
      "                \"total_tokens\": 432,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"accepted_prediction_tokens\": 0,\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"reasoning_tokens\": 0,\n",
      "                  \"rejected_prediction_tokens\": 0\n",
      "                },\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"cached_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"id\": \"chatcmpl-BjRQxLblLXAcrKkZf2n2zj5YpkQW1\",\n",
      "              \"service_tier\": \"default\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run--a21b27bf-a5f8-4225-a8e3-dd5330b687da-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 328,\n",
      "              \"output_tokens\": 104,\n",
      "              \"total_tokens\": 432,\n",
      "              \"input_token_details\": {\n",
      "                \"audio\": 0,\n",
      "                \"cache_read\": 0\n",
      "              },\n",
      "              \"output_token_details\": {\n",
      "                \"audio\": 0,\n",
      "                \"reasoning\": 0\n",
      "              }\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        },\n",
      "        \"text\": \"We can use the `sorted()` function in Python to sort the list of customers based on last name first and then first name.\\nAction: Python_REPL\\nAction Input: sorted([['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']], key=lambda x: (x[1], x[0]))\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 104,\n",
      "      \"prompt_tokens\": 328,\n",
      "      \"total_tokens\": 432,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"accepted_prediction_tokens\": 0,\n",
      "        \"audio_tokens\": 0,\n",
      "        \"reasoning_tokens\": 0,\n",
      "        \"rejected_prediction_tokens\": 0\n",
      "      },\n",
      "      \"prompt_tokens_details\": {\n",
      "        \"audio_tokens\": 0,\n",
      "        \"cached_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null,\n",
      "    \"id\": \"chatcmpl-BjRQxLblLXAcrKkZf2n2zj5YpkQW1\",\n",
      "    \"service_tier\": \"default\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] [5.58s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"We can use the `sorted()` function in Python to sort the list of customers based on last name first and then first name.\\nAction: Python_REPL\\nAction Input: sorted([['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']], key=lambda x: (x[1], x[0]))\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[chain:AgentExecutor > tool:Python_REPL] Entering Tool run with input:\n",
      "\u001b[0m\"sorted([['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']], key=lambda x: (x[1], x[0]))\"\n",
      "\u001b[36;1m\u001b[1;3m[tool/end]\u001b[0m \u001b[1m[chain:AgentExecutor > tool:Python_REPL] [3ms] Exiting Tool run with output:\n",
      "\u001b[0m\"\"\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\",\n",
      "  \"agent_scratchpad\": \"We can use the `sorted()` function in Python to sort the list of customers based on last name first and then first name.\\nAction: Python_REPL\\nAction Input: sorted([['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']], key=lambda x: (x[1], x[0]))\\nObservation: \\nThought:\",\n",
      "  \"stop\": [\n",
      "    \"\\nObservation:\",\n",
      "    \"\\n\\tObservation:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an agent designed to write and execute python code to answer questions.\\nYou have access to a python REPL, which you can use to execute python code.\\nIf you get an error, debug your code and try again.\\nOnly use the output of your code to answer the question. \\nYou might know the answer without running any code, but you should still run the code to get the answer.\\nIf it does not seem like you can write code to answer the question, just return \\\"I don't know\\\" as the answer.\\n\\n\\nPython_REPL - A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [Python_REPL]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\nThought:We can use the `sorted()` function in Python to sort the list of customers based on last name first and then first name.\\nAction: Python_REPL\\nAction Input: sorted([['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']], key=lambda x: (x[1], x[0]))\\nObservation: \\nThought:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatOpenAI] [1.02s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"I now know the final answer\\nFinal Answer: [['Jen', 'Ayai'], ['Harrison', 'Chase'], ['Lang', 'Chain'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Dolly', 'Too']]\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 61,\n",
      "                \"prompt_tokens\": 435,\n",
      "                \"total_tokens\": 496,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"accepted_prediction_tokens\": 0,\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"reasoning_tokens\": 0,\n",
      "                  \"rejected_prediction_tokens\": 0\n",
      "                },\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"cached_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"id\": \"chatcmpl-BjRR2We991Z8mVn7OqzSf3bDEN2gQ\",\n",
      "              \"service_tier\": \"default\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run--7a396885-6bd7-40d0-bef6-0f0edbf41718-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 435,\n",
      "              \"output_tokens\": 61,\n",
      "              \"total_tokens\": 496,\n",
      "              \"input_token_details\": {\n",
      "                \"audio\": 0,\n",
      "                \"cache_read\": 0\n",
      "              },\n",
      "              \"output_token_details\": {\n",
      "                \"audio\": 0,\n",
      "                \"reasoning\": 0\n",
      "              }\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        },\n",
      "        \"text\": \"I now know the final answer\\nFinal Answer: [['Jen', 'Ayai'], ['Harrison', 'Chase'], ['Lang', 'Chain'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Dolly', 'Too']]\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 61,\n",
      "      \"prompt_tokens\": 435,\n",
      "      \"total_tokens\": 496,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"accepted_prediction_tokens\": 0,\n",
      "        \"audio_tokens\": 0,\n",
      "        \"reasoning_tokens\": 0,\n",
      "        \"rejected_prediction_tokens\": 0\n",
      "      },\n",
      "      \"prompt_tokens_details\": {\n",
      "        \"audio_tokens\": 0,\n",
      "        \"cached_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null,\n",
      "    \"id\": \"chatcmpl-BjRR2We991Z8mVn7OqzSf3bDEN2gQ\",\n",
      "    \"service_tier\": \"default\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] [1.02s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"I now know the final answer\\nFinal Answer: [['Jen', 'Ayai'], ['Harrison', 'Chase'], ['Lang', 'Chain'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Dolly', 'Too']]\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor] [6.61s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"[['Jen', 'Ayai'], ['Harrison', 'Chase'], ['Lang', 'Chain'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Dolly', 'Too']]\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "langchain.debug=True\n",
    "\n",
    "agent.invoke(f\"\"\"Sort these customers by \\\n",
    "last name and then first name \\\n",
    "and print the output: {customer_list}\"\"\") \n",
    "\n",
    "langchain.debug=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ac977749",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ChatAgent does not support multi-input tool time.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[89]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[33;03m    Returns todays date, use this for any questions related to knowing todays\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m    date.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[33;03m    return todays date - any date mathmatics should occur outside this function.\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(date.today())\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m agent= \u001b[43minitialize_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtime\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mAgentType\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCHAT_ZERO_SHOT_REACT_DESCRIPTION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhandle_parsing_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m agent.invoke(\u001b[33m\"\u001b[39m\u001b[33mwhats the date today?\u001b[39m\u001b[33m\"\u001b[39m) \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/exploring-langchain-UOzDUSui/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:191\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    190\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/exploring-langchain-UOzDUSui/lib/python3.11/site-packages/langchain/agents/initialize.py:74\u001b[39m, in \u001b[36minitialize_agent\u001b[39m\u001b[34m(tools, llm, agent, callback_manager, agent_path, agent_kwargs, tags, **kwargs)\u001b[39m\n\u001b[32m     72\u001b[39m     agent_cls = AGENT_TO_CLASS[agent]\n\u001b[32m     73\u001b[39m     agent_kwargs = agent_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     agent_obj = \u001b[43magent_cls\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_llm_and_tools\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m        \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43magent_kwargs\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m agent_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     78\u001b[39m     agent_obj = load_agent(\n\u001b[32m     79\u001b[39m         agent_path, llm=llm, tools=tools, callback_manager=callback_manager\n\u001b[32m     80\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/exploring-langchain-UOzDUSui/lib/python3.11/site-packages/langchain/agents/chat/base.py:159\u001b[39m, in \u001b[36mChatAgent.from_llm_and_tools\u001b[39m\u001b[34m(cls, llm, tools, callback_manager, output_parser, system_message_prefix, system_message_suffix, human_message, format_instructions, input_variables, **kwargs)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_llm_and_tools\u001b[39m(\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    137\u001b[39m     **kwargs: Any,\n\u001b[32m    138\u001b[39m ) -> Agent:\n\u001b[32m    139\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Construct an agent from an LLM and tools.\u001b[39;00m\n\u001b[32m    140\u001b[39m \n\u001b[32m    141\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    157\u001b[39m \u001b[33;03m        An agent.\u001b[39;00m\n\u001b[32m    158\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    160\u001b[39m     prompt = \u001b[38;5;28mcls\u001b[39m.create_prompt(\n\u001b[32m    161\u001b[39m         tools,\n\u001b[32m    162\u001b[39m         system_message_prefix=system_message_prefix,\n\u001b[32m   (...)\u001b[39m\u001b[32m    166\u001b[39m         input_variables=input_variables,\n\u001b[32m    167\u001b[39m     )\n\u001b[32m    168\u001b[39m     llm_chain = LLMChain(\n\u001b[32m    169\u001b[39m         llm=llm,\n\u001b[32m    170\u001b[39m         prompt=prompt,\n\u001b[32m    171\u001b[39m         callback_manager=callback_manager,\n\u001b[32m    172\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/exploring-langchain-UOzDUSui/lib/python3.11/site-packages/langchain/agents/chat/base.py:73\u001b[39m, in \u001b[36mChatAgent._validate_tools\u001b[39m\u001b[34m(cls, tools)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_validate_tools\u001b[39m(\u001b[38;5;28mcls\u001b[39m, tools: Sequence[BaseTool]) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     72\u001b[39m     \u001b[38;5;28msuper\u001b[39m()._validate_tools(tools)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[43mvalidate_tools_single_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/exploring-langchain-UOzDUSui/lib/python3.11/site-packages/langchain/agents/utils.py:18\u001b[39m, in \u001b[36mvalidate_tools_single_input\u001b[39m\u001b[34m(class_name, tools)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m tools:\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tool.is_single_input:\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     19\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not support multi-input tool \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtool.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     20\u001b[39m         )\n",
      "\u001b[31mValueError\u001b[39m: ChatAgent does not support multi-input tool time."
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "from langchain.agents import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def time(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns todays date, use this for any questions related to knowing todays\n",
    "    date.\n",
    "    \n",
    "    The input should always be an empty string, and this function will always\n",
    "    return todays date - any date mathmatics should occur outside this function.\n",
    "    \"\"\"\n",
    "\n",
    "    return str(date.today())\n",
    "\n",
    "\n",
    "agent= initialize_agent(\n",
    "    tools + [time], \n",
    "    llm, \n",
    "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    handle_parsing_errors=True,\n",
    "    verbose = True)\n",
    "\n",
    "\n",
    "agent.invoke(\"whats the date today?\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb72225c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe3b234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6d3f34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exploring-langchain",
   "language": "python",
   "name": "exploring-langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10 (main, Oct 17 2024, 12:14:22) [GCC 9.4.0]"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

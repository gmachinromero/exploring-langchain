{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ae230d7-2ab1-469e-a6a0-238293c1eeb1",
   "metadata": {},
   "source": [
    "# 0 - Librer√≠as y variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d99484c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Librer√≠as\n",
    "# ------------------------------------------------------------------------------\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d680460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY\n",
      "PROXYCURL_API_KEY\n",
      "TAVILY_API_KEY\n",
      "LANGCHAIN_TRACING_V2\n",
      "LANGCHAIN_ENDPOINT\n",
      "LANGCHAIN_API_KEY\n",
      "LANGCHAIN_PROJECT\n"
     ]
    }
   ],
   "source": [
    "# Variables\n",
    "# ------------------------------------------------------------------------------\n",
    "env_vars = dotenv_values()\n",
    "for key in env_vars.keys():\n",
    "    print(key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b042c85b-397e-45fd-92ff-3177d182c1cb",
   "metadata": {},
   "source": [
    "# 1 - ChatLLM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0a5d7ff",
   "metadata": {},
   "source": [
    "En este apartado se llama a la API de OpenAI directamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a9fac69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f09214c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1+1 es igual a 2.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inicializar el cliente\n",
    "client = OpenAI()\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"¬øCu√°nto es 1+1?\"}]\n",
    "    \n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=messages,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f6a6be29",
   "metadata": {},
   "source": [
    "Si quiero modificar el c√≥digo por ejemplo para utilizar un modelo como LLaMA 3.1 desde Ollama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9887075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La respuesta a la pregunta es 2.\n"
     ]
    }
   ],
   "source": [
    "# Define el endpoint local de Ollama\n",
    "OLLAMA_URL = \"http://localhost:11434/api/chat\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"¬øCu√°nto es 1+1?\"}]\n",
    "\n",
    "response = requests.post(OLLAMA_URL, json={\n",
    "    \"model\": \"llama3.1\",\n",
    "    \"messages\": messages,\n",
    "    \"temperature\": 0.0,\n",
    "    \"stream\": False\n",
    "})\n",
    "\n",
    "data = response.json()\n",
    "print(data[\"message\"][\"content\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e004c686",
   "metadata": {},
   "source": [
    "Cada vez que quiero apuntar a un LLM diferente, tendr√≠a que modificar el c√≥digo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88c74f4a",
   "metadata": {},
   "source": [
    "# 2 - LangChain: Chat API"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c809ced",
   "metadata": {},
   "source": [
    "En este apartado se va a replicar lo se ha hecho en el punto anterior, pero desde la API de LangChain. Adicionalemnte se va ahondar en los siguientes conceptos de LangChain y sus ventajas:\n",
    "- Modelos\n",
    "- Prompts Templates\n",
    "- Output Parsers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa5c9eb1",
   "metadata": {},
   "source": [
    "## 2.1. - Modelos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3eeca3d6",
   "metadata": {},
   "source": [
    "En LangChain, los models (modelos) son los componentes que generan texto o responden a mensajes. Son la parte que realmente interact√∫a con modelos de lenguaje (LLMs) como los de OpenAI, Anthropic, Cohere, etc. LangChain organiza los modelos seg√∫n lo que hacen. Los m√°s comunes son:\n",
    "\n",
    "| Tipo de modelo     | Qu√© hace                                                                    | Clase t√≠pica                                |\n",
    "| ------------------ | --------------------------------------------------------------------------- | ------------------------------------------- |\n",
    "| **LLM**            | Genera texto a partir de un *prompt plano*                                  | OpenAI, HuggingFaceHub                      |\n",
    "| **ChatModel**      | Maneja conversaciones con roles (usuario, asistente, sistema)               | ChatOpenAI, ChatAnthropic                   |\n",
    "| **EmbeddingModel** | Convierte texto en vectores (√∫tiles para b√∫squedas o comparaci√≥n sem√°ntica) | OpenAIEmbeddings, HuggingFaceEmbeddings     |\n",
    "\n",
    "\n",
    "LangChain permite llamar a diferentes modelos, con diferentes APIs, de forma agn√≥stica. Cambiando ligeramente el c√≥digo, puedo aprovechar una estructura ya creada para apuntar a otro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6287b970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.25\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a22bf983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "061259f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear los mensajes en el formato de LangChain\n",
    "messages = [\n",
    "    HumanMessage(content=\"¬øCu√°nto es 1+1?\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db336180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La respuesta a la pregunta de \"¬øCu√°nto es 1+1?\" es 2.\n"
     ]
    }
   ],
   "source": [
    "# Llamar a un modelo A\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1\",\n",
    "    temperature=0.0\n",
    "    )\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b140df4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1+1 es igual a 2.\n"
     ]
    }
   ],
   "source": [
    "# Llamar a un modelo B\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\", \n",
    "    temperature=0.0\n",
    "    )\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2c85cbe",
   "metadata": {},
   "source": [
    "La estructura es exactamente la misma, se podr√≠a incluso encapsular el codigo en una funci√≥n y pasar el modelo como par√°metro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04321608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andrew Ng es un cient√≠fico de la computaci√≥n y empresario estadounidense de origen chino. Es conocido por ser uno de los pioneros en el campo de la inteligencia artificial y el aprendizaje profundo. Ha sido profesor en la Universidad de Stanford, cofundador de Google Brain, y fundador de Coursera y deeplearning.ai. Tambi√©n ha ocupado cargos en empresas como Baidu y Uber. Es una figura influyente en el mundo de la inteligencia artificial y ha contribuido significativamente al desarrollo de esta disciplina.\n"
     ]
    }
   ],
   "source": [
    "def call_llm_model(\n",
    "    model_name: str,\n",
    "    temperature: float,\n",
    "    message: str\n",
    ") -> str:\n",
    "    \n",
    "    if model_name.startswith(\"gpt\"):\n",
    "        llm = ChatOpenAI(\n",
    "            model_name=model_name,\n",
    "            temperature=temperature\n",
    "        )\n",
    "    else:\n",
    "        llm = ChatOllama(\n",
    "            model=model_name,\n",
    "            temperature=temperature\n",
    "        )\n",
    "\n",
    "    response = llm.invoke([HumanMessage(content=message)])\n",
    "    return response.content\n",
    "\n",
    "\n",
    "# Ejemplo de uso:\n",
    "respuesta = call_llm_model(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0.5,\n",
    "    message=\"¬øQui√©n es Andrew Ng?\"\n",
    ")\n",
    "\n",
    "print(respuesta)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d28abe4",
   "metadata": {},
   "source": [
    "En las √∫ltimas versiones de LangChain, se ha creado una abstracci√≥n para inicilizar modelos de diferentes proveedores desde la misma funci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbb162aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "# model = init_chat_model(\"claude-3-5-sonnet-latest\", model_provider=\"anthropic\")\n",
    "# model = init_chat_model(\"mistral-large-latest\", model_provider=\"mistralai\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a9b0129",
   "metadata": {},
   "source": [
    "## 2.2. - Prompt Templates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7a67aee",
   "metadata": {},
   "source": [
    "Las Prompt Templates (plantillas de prompt) son una herramienta de LangChain que te ayuda a crear autom√°ticamente los mensajes que le env√≠as al modelo de lenguaje, de una forma organizada, flexible y reutilizable.\n",
    "\n",
    "Cuando trabajas con modelos como GPT, no sueles enviar directamente el texto del usuario al modelo. Normalmente quieres hacer algo m√°s, como:\n",
    "\n",
    "- Agregar instrucciones espec√≠ficas (ej.: \"Traduce al franc√©s...\")\n",
    "- Dar un contexto adicional al modelo (ej.: \"Eres un traductor especializado en literatura cl√°sica...\")\n",
    "- Formatear el texto de forma especial (ej.: \"Devuelve el resultado en un .json con la estructura...\")\n",
    "- Usar el mismo formato muchas veces con diferentes datos\n",
    "\n",
    "Ah√≠ es donde entran las prompt templates.\n",
    "\n",
    "Piensa en ellas como plantillas con huecos (como los de un formulario). T√∫ defines una estructura fija y dejas espacios para completar con datos reales m√°s tarde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd77b09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"Traduce el siguiente texto al {language}: {text}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f88974e3",
   "metadata": {},
   "source": [
    "LangChain te permite definir esta plantilla y luego rellenarla autom√°ticamente con los valores que quieras:\n",
    "- `language = \"franc√©s\"`\n",
    "- `text = \"Hola, ¬øc√≥mo est√°s?\"`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68687fc0",
   "metadata": {},
   "source": [
    "los principales templates y su prop√≥sito son:\n",
    "\n",
    "| Prompt Template                    | ¬øPara qu√© sirve?                                                                                                                                   |\n",
    "| ---------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `ChatPromptTemplate`               | Plantilla para **modelos de chat**. Permite combinar mensajes (`System`, `Human`, etc.) f√°cilmente. Ideal para `ChatOpenAI`, `ChatAnthropic`, etc. |\n",
    "| `PromptTemplate`                   | Plantilla simple de texto plano. Usado con modelos de lenguaje **no conversacionales** como `OpenAI(model=\"text-davinci-003\")`.                    |\n",
    "| `SystemMessagePromptTemplate`      | Subplantilla usada dentro de `ChatPromptTemplate` para definir el **mensaje del sistema**.                                                         |\n",
    "| `HumanMessagePromptTemplate`       | Subplantilla usada dentro de `ChatPromptTemplate` para el **mensaje del usuario**                                                                  |\n",
    "| `MessagesPlaceholder`              | Placeholder especial dentro de `ChatPromptTemplate` para insertar una lista din√°mica de mensajes (ej: historial de conversaci√≥n).                  |\n",
    "| `AIMessagePromptTemplate`          | Subplantilla para simular un mensaje **anterior del asistente** en el historial.                                                                   |\n",
    "| `FewShotPromptTemplate`            | Plantilla para tareas de **few-shot learning**. Permite definir ejemplos que se combinan con el input.                                             |\n",
    "| `ChatMessagePromptTemplate`        | Versi√≥n m√°s flexible que permite definir mensajes de un rol personalizado (ej: `\"role\": \"function\"`).                                              |\n",
    "| `FewShotChatMessagePromptTemplate` | Lo mismo que `FewShotPromptTemplate`, pero adaptado para `ChatPromptTemplate`. √ötil si quieres ejemplos en formato chat.                           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74d2b4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15e8084e",
   "metadata": {},
   "source": [
    "\n",
    "| M√©todo            | ¬øQu√© construye?                             | ¬øCu√°ndo usarlo?                                 |\n",
    "| ----------------- | ------------------------------------------- | ----------------------------------------------- |\n",
    "| `from_template()` | Una sola plantilla de mensaje               | Para casos simples o r√°pidos                    |\n",
    "| `from_messages()` | Una conversaci√≥n entera con m√∫ltiples roles | Cuando necesitas dar contexto o varios mensajes |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76a4a5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plantilla de traducci√≥n\n",
    "template = \"Traduce el siguiente texto al {language}: {text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "13abd576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para casos sencillos\n",
    "template_string = \"Traduce el siguiente texto al {language}: {text}\"\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27e3d06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para casos mas complejos: sistema, usuarios, asistente...etc\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un trabajador del campo, oriundo de la alpujarra de Granada.\"),\n",
    "    (\"human\", \"Traduce el siguiente texto al {language}: {text}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8528252a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['language', 'text'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Eres un trabajador del campo, oriundo de la alpujarra de Granada.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language', 'text'], input_types={}, partial_variables={}, template='Traduce el siguiente texto al {language}: {text}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a30e830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Eres un trabajador del campo, oriundo de la alpujarra de Granada.'), additional_kwargs={})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67a5b66d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language', 'text'], input_types={}, partial_variables={}, template='Traduce el siguiente texto al {language}: {text}'), additional_kwargs={})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c124e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['language', 'text']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[1].prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ce1c912",
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"Grana√≠no\"\n",
    "text = \"¬øDe d√≥nde vienes?\"\n",
    "\n",
    "translation_prompt = prompt_template.format_messages(\n",
    "    language=language,\n",
    "    text=text\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f363829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Eres un trabajador del campo, oriundo de la alpujarra de Granada.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Traduce el siguiente texto al Grana√≠no: ¬øDe d√≥nde vienes?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9bf2683b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'langchain_core.messages.system.SystemMessage'>\n",
      "<class 'langchain_core.messages.human.HumanMessage'>\n"
     ]
    }
   ],
   "source": [
    "print(type(translation_prompt))\n",
    "print(type(translation_prompt[0]))\n",
    "print(type(translation_prompt[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "721168c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¬øAnde vienes, majo?'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator_response = llm.invoke(translation_prompt)\n",
    "\n",
    "translator_response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10fdba7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¬øDe do vienes?'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un noble de la realeza espa√±ola del siglo XV.\"),\n",
    "    (\"human\", \"Traduce esto al {language}: {text}\")\n",
    "])\n",
    "\n",
    "language = \"Castellano antiguo\"\n",
    "text = \"¬øDe d√≥nde vienes?\"\n",
    "\n",
    "translation_prompt = prompt_template.format_messages(\n",
    "    language=language,\n",
    "    text=text\n",
    "    )\n",
    "\n",
    "customer_response = llm.invoke(translation_prompt)\n",
    "\n",
    "customer_response.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07abd8fe",
   "metadata": {},
   "source": [
    "## 2.3. - Output Parsers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58faea88",
   "metadata": {},
   "source": [
    "En LangChain, los output parsers son herramientas clave para procesar y estructurar las respuestas que se obtienen de los modelos de lenguaje antes de ser utilizadas en otros pasos del flujo de trabajo. Estos parsers te permiten transformar la salida en bruto de los modelos en un formato m√°s adecuado para tu aplicaci√≥n, ya sea en forma de texto, datos estructurados o incluso en la ejecuci√≥n de funciones espec√≠ficas.\n",
    "\n",
    "A continuaci√≥n, una tabla con los output parsers m√°s comunes en LangChain y su uso principal:\n",
    "\n",
    "| **Output Parser**        | **¬øPara qu√© sirve?**                                                                                                                                                 |\n",
    "| ------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `JsonOutputParser`       | Convierte la salida del modelo en un objeto JSON (estructura de diccionario o lista). Ideal cuando esperas respuestas estructuradas como JSON o diccionarios.        |\n",
    "| `RegexOutputParser`      | Extrae informaci√≥n utilizando expresiones regulares. Se usa cuando la respuesta del modelo sigue un patr√≥n espec√≠fico que se puede identificar con regex.            |\n",
    "| `StructuredOutputParser` | Permite parsear la salida en estructuras de datos m√°s complejas. Ideal para cuando necesitas que el modelo devuelva datos tabulares o jer√°rquicos.                   |\n",
    "| `VariableParser`         | Analiza las respuestas del modelo en busca de valores de variables espec√≠ficas. Utilizado cuando deseas extraer datos de las respuestas para usarlos en otros pasos. |\n",
    "| `TextOutputParser`       | Convierte la salida en un texto plano procesable. Ideal cuando la salida del modelo es simplemente texto sin estructura.                                             |\n",
    "| `SQLOutputParser`        | Convierte la salida del modelo en consultas SQL estructuradas. Es √∫til si el modelo est√° generando consultas a bases de datos.                                       |\n",
    "| `PythonOutputParser`     | Convierte la salida del modelo en c√≥digo Python ejecutable. Utilizado cuando necesitas generar c√≥digo a partir de la respuesta del modelo.                           |\n",
    "| `ActionOutputParser`     | Permite que la salida sea convertida en una acci√≥n espec√≠fica o una ejecuci√≥n de funci√≥n. Ideal para flujos de trabajo m√°s din√°micos donde se deben ejecutar tareas. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "526e9ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': False, 'delivery_days': 5, 'price_value': 'muy asequible!'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    \"gift\": False,\n",
    "    \"delivery_days\": 5,\n",
    "    \"price_value\": \"muy asequible!\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "53c9ce45",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review = \"\"\"\\\n",
    "Este soplador de hojas es bastante incre√≠ble. Tiene cuatro configuraciones:\\\n",
    "soplador de vela, brisa suave, ciudad ventosa y tornado. \\\n",
    "Lleg√≥ en dos d√≠as, justo a tiempo para el regalo de aniversario de mi esposa. \\\n",
    "Creo que a mi esposa le gust√≥ tanto que se qued√≥ sin palabras. \\\n",
    "Hasta ahora he sido el √∫nico que lo ha usado, y lo he estado usando cada dos ma√±anas para limpiar las hojas de nuestro c√©sped. \\\n",
    "Es ligeramente m√°s caro que los otros sopladores de hojas que hay por ah√≠, pero creo que vale la pena por las caracter√≠sticas adicionales.\n",
    "\"\"\"\n",
    "\n",
    "review_template = \"\"\"\\\n",
    "Para el siguiente texto, extrae la siguiente informaci√≥n:\n",
    "\n",
    "gift: ¬øFue el art√≠culo comprado como un regalo para otra persona? \\\n",
    "Responde True si es s√≠, False si no o si no se sabe.\n",
    "\n",
    "delivery_days: ¬øCu√°ntos d√≠as tard√≥ en llegar el producto? \\\n",
    "Si esta informaci√≥n no se encuentra, devuelve -1.\n",
    "\n",
    "price_value: Extrae cualquier frase sobre el valor o precio,\\\n",
    "y devu√©lvelas como una lista de Python separada por comas.\n",
    "\n",
    "Formatea la salida como JSON con las siguientes claves:\n",
    "gift --> boolean\n",
    "delivery_days --> integer\n",
    "price_value --> python list\n",
    "\n",
    "texto: {text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6fcaf224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['text'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='Para el siguiente texto, extrae la siguiente informaci√≥n:\\n\\ngift: ¬øFue el art√≠culo comprado como un regalo para otra persona? Responde True si es s√≠, False si no o si no se sabe.\\n\\ndelivery_days: ¬øCu√°ntos d√≠as tard√≥ en llegar el producto? Si esta informaci√≥n no se encuentra, devuelve -1.\\n\\nprice_value: Extrae cualquier frase sobre el valor o precio,y devu√©lvelas como una lista de Python separada por comas.\\n\\nFormatea la salida como JSON con las siguientes claves:\\ngift --> boolean\\ndelivery_days --> integer\\nprice_value --> python list\\n\\ntexto: {text}\\n'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "497a6778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c7b68e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"gift\": true,\n",
      "    \"delivery_days\": 2,\n",
      "    \"price_value\": [\"ligeramente m√°s caro\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "messages = prompt_template.format_messages(text=customer_review)\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "57b33cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "11ca7f06",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Si intentamos acceder a response como si fuera un diccionario...\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m(\u001b[33m'\u001b[39m\u001b[33mgift\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "# Si intentamos acceder a response como si fuera un diccionario...\n",
    "response.content.get('gift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dcbe8c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "90e078cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_schemas=[ResponseSchema(name='gift', description='True si fue un regalo, False si no.', type='string'), ResponseSchema(name='delivery_days', description='N√∫mero de d√≠as que tard√≥ en llegar el producto.', type='string'), ResponseSchema(name='price_value', description='Frases relacionadas con el valor o precio del producto.', type='string')]\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // True si fue un regalo, False si no.\n",
      "\t\"delivery_days\": string  // N√∫mero de d√≠as que tard√≥ en llegar el producto.\n",
      "\t\"price_value\": string  // Frases relacionadas con el valor o precio del producto.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Definir los esquemas de respuesta\n",
    "response_schemas = [\n",
    "    ResponseSchema(\n",
    "        name=\"gift\",\n",
    "        description=\"True si fue un regalo, False si no.\"\n",
    "    ),\n",
    "    ResponseSchema(\n",
    "        name=\"delivery_days\",\n",
    "        description=\"N√∫mero de d√≠as que tard√≥ en llegar el producto.\"\n",
    "    ),\n",
    "    ResponseSchema(\n",
    "        name=\"price_value\",\n",
    "        description=\"Frases relacionadas con el valor o precio del producto.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Crear el parser estructurado\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "print(output_parser)\n",
    "print(output_parser.get_format_instructions())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4d44dc1",
   "metadata": {},
   "source": [
    "Cuando se utiliza `response_schemas`, LangChain siempre interpreta que los resultados son string. Por eso en `review_template` se indica:\n",
    "\n",
    "Formatea la salida como JSON con las siguientes claves:\n",
    "- gift --> boolean\n",
    "- delivery_days --> integer\n",
    "- price_value --> python list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "04a36087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: Para el siguiente texto, extrae la siguiente informaci√≥n:\\n\\ngift: ¬øFue el art√≠culo comprado como un regalo para otra persona? Responde True si es s√≠, False si no o si no se sabe.\\n\\ndelivery_days: ¬øCu√°ntos d√≠as tard√≥ en llegar el producto? Si esta informaci√≥n no se encuentra, devuelve -1.\\n\\nprice_value: Extrae cualquier frase sobre el valor o precio,y devu√©lvelas como una lista de Python separada por comas.\\n\\nFormatea la salida como JSON con las siguientes claves:\\ngift --> boolean\\ndelivery_days --> integer\\nprice_value --> python list\\n\\ntexto: Este soplador de hojas es bastante incre√≠ble. Tiene cuatro configuraciones:soplador de vela, brisa suave, ciudad ventosa y tornado. Lleg√≥ en dos d√≠as, justo a tiempo para el regalo de aniversario de mi esposa. Creo que a mi esposa le gust√≥ tanto que se qued√≥ sin palabras. Hasta ahora he sido el √∫nico que lo ha usado, y lo he estado usando cada dos ma√±anas para limpiar las hojas de nuestro c√©sped. Es ligeramente m√°s caro que los otros sopladores de hojas que hay por ah√≠, pero creo que vale la pena por las caracter√≠sticas adicionales.\\n\\n'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construir el prompt con instrucciones de formato\n",
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "\n",
    "formatted_prompt = prompt_template.format(\n",
    "    text=customer_review,\n",
    "    format_instructions=output_parser.get_format_instructions()\n",
    ")\n",
    "\n",
    "formatted_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cbbafffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llamar al modelo\n",
    "response = llm.invoke([HumanMessage(content=formatted_prompt)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f5ac61c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gift': True, 'delivery_days': 2, 'price_value': ['ligeramente m√°s caro']}\n"
     ]
    }
   ],
   "source": [
    "# Parsear la salida\n",
    "parsed_output = output_parser.parse(response.content)\n",
    "\n",
    "# Mostrar el resultado como diccionario\n",
    "print(parsed_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1fa4da39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clave: gift | Valor: True | Tipo de dato: <class 'bool'>\n",
      "Clave: delivery_days | Valor: 2 | Tipo de dato: <class 'int'>\n",
      "Clave: price_value | Valor: ['ligeramente m√°s caro'] | Tipo de dato: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "for key, value in parsed_output.items():\n",
    "    print(f\"Clave: {key} | Valor: {value} | Tipo de dato: {type(value)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5842a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb5be5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c767a12a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "586b5d87",
   "metadata": {},
   "source": [
    "# 3 - LangChain: Memory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9957f70f",
   "metadata": {},
   "source": [
    "En LangChain, la memoria se refiere a la capacidad de una cadena o agente para recordar informaci√≥n entre interacciones. Es √∫til especialmente en contextos conversacionales o multistep, donde necesitas que el modelo tenga conocimiento del contexto anterior.\n",
    "\n",
    "Por defecto, los modelos como GPT no tienen memoria entre llamadas. LangChain permite gestionar esto incluyendo ‚Äúmemoria‚Äù en una cadena (Chain) o un agente (Agent), haciendo que el contexto previo se incluya autom√°ticamente en nuevas solicitudes.\n",
    "\n",
    "A continuaci√≥n, una tabla con los tipos de memoria m√°s comunes en LangChain y su uso principal:\n",
    "\n",
    "| **Tipo de Memoria**              | **¬øPara qu√© sirve?**                                                                                                                                    |\n",
    "| -------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `ConversationBufferMemory`       | Guarda todo el historial de la conversaci√≥n como una cadena de texto. Es la forma m√°s simple de mantener el contexto completo de una conversaci√≥n.      |\n",
    "| `ConversationBufferWindowMemory` | Similar a `ConversationBufferMemory`, pero solo conserva las **√∫ltimas *k* interacciones** (ventana deslizante). √ötil para limitar el contexto enviado. |\n",
    "| `ConversationTokenBufferMemory`  | Guarda la conversaci√≥n bas√°ndose en un **l√≠mite de tokens**, no de turnos. Conserva solo la parte del historial que cabe dentro de un n√∫mero de tokens. |\n",
    "| `ConversationSummaryMemory`      | Resume el historial de la conversaci√≥n en un texto corto. Ideal para mantener contexto en conversaciones largas sin exceder el l√≠mite de tokens.        |\n",
    "\n",
    "Diferencias clave:\n",
    "\n",
    "| Memoria                          | ¬øRecuerda todo?       | ¬øControla tama√±o? | ¬øResume contenido? | ¬øIdeal para...?                               |\n",
    "| -------------------------------- | --------------------- | ----------------- | ------------------ | --------------------------------------------- |\n",
    "| `ConversationBufferMemory`       | S√≠                    | ‚ùå                 | ‚ùå                  | Conversaciones cortas o demostraciones        |\n",
    "| `ConversationBufferWindowMemory` | No (solo √∫ltimas *k*) | ‚úÖ (*k*)           | ‚ùå                  | Chats donde solo importa el contexto reciente |\n",
    "| `ConversationTokenBufferMemory`  | No                    | ‚úÖ (*n* tokens)    | ‚ùå                  | Ajustar a l√≠mites estrictos de tokens         |\n",
    "| `ConversationSummaryMemory`      | No (resume todo)      | ‚úÖ                 | ‚úÖ                  | Chats largos sin perder el hilo               |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "92d95f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import (\n",
    "    ConversationBufferMemory,\n",
    "    ConversationBufferWindowMemory,\n",
    "    ConversationTokenBufferMemory,\n",
    "    ConversationSummaryBufferMemory\n",
    ")\n",
    "\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.prompts import PromptTemplate\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a67ead6",
   "metadata": {},
   "source": [
    "A continuaci√≥n vemos un ejemplo, de como los modelos no incorporan memoria de forma nativa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4edb1421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primera respuesta:\n",
      "¬°Hola Guille! ¬øEn qu√© puedo ayudarte hoy?\n",
      "\n",
      "Segunda respuesta (sin memoria):\n",
      "Lo siento, no tengo la capacidad de saber tu nombre a menos que me lo digas. ¬øCu√°l es tu nombre?\n"
     ]
    }
   ],
   "source": [
    "# Instanciar modelo\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\", \n",
    "    temperature=0.0\n",
    "    )\n",
    "\n",
    "# Primera interacci√≥n\n",
    "message = \"Mi nombre es Guille.\"\n",
    "response = llm.invoke([HumanMessage(content=message)])\n",
    "print(\"Primera respuesta:\")\n",
    "print(response.content)\n",
    "\n",
    "# Segunda interacci√≥n sin memoria\n",
    "message = \"¬øCu√°l es mi nombre?\"\n",
    "response = llm.invoke([HumanMessage(content=message)])\n",
    "print(\"\\nSegunda respuesta (sin memoria):\")\n",
    "print(response.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbf07f88",
   "metadata": {},
   "source": [
    "Vamos a darle memoria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1ebc6f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\", \n",
    "    temperature=0.0\n",
    "    )\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4580f2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hola, mi nombre es Guille\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'¬°Hola Guille! ¬°Encantado de conocerte! ¬øEn qu√© puedo ayudarte hoy?'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hola, mi nombre es Guille\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c43dfa5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hola, mi nombre es Guille\n",
      "AI: ¬°Hola Guille! ¬°Encantado de conocerte! ¬øEn qu√© puedo ayudarte hoy?\n",
      "Human: ¬øCu√°nto es 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1+1 es igual a 2. ¬øHay algo m√°s en lo que pueda ayudarte, Guille?'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"¬øCu√°nto es 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3e4d3304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hola, mi nombre es Guille\n",
      "AI: ¬°Hola Guille! ¬°Encantado de conocerte! ¬øEn qu√© puedo ayudarte hoy?\n",
      "Human: ¬øCu√°nto es 1+1?\n",
      "AI: 1+1 es igual a 2. ¬øHay algo m√°s en lo que pueda ayudarte, Guille?\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Tu nombre es Guille. ¬øHay algo m√°s en lo que pueda ayudarte?'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"¬øCu√°l es mi nombre?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cd56038f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hola, mi nombre es Guille\n",
      "AI: ¬°Hola Guille! ¬°Encantado de conocerte! ¬øEn qu√© puedo ayudarte hoy?\n",
      "Human: ¬øCu√°nto es 1+1?\n",
      "AI: 1+1 es igual a 2. ¬øHay algo m√°s en lo que pueda ayudarte, Guille?\n",
      "Human: What is my name?\n",
      "AI: Tu nombre es Guille. ¬øHay algo m√°s en lo que pueda ayudarte?\n",
      "{'history': 'Human: Hola, mi nombre es Guille\\nAI: ¬°Hola Guille! ¬°Encantado de conocerte! ¬øEn qu√© puedo ayudarte hoy?\\nHuman: ¬øCu√°nto es 1+1?\\nAI: 1+1 es igual a 2. ¬øHay algo m√°s en lo que pueda ayudarte, Guille?\\nHuman: What is my name?\\nAI: Tu nombre es Guille. ¬øHay algo m√°s en lo que pueda ayudarte?'}\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5c3fc2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "\n",
    "memory.save_context(\n",
    "    {\"input\": \"Hola\"}, \n",
    "    {\"output\": \"¬øQu√© tal?\"}\n",
    ")\n",
    "\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1a651c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n",
      "Human: Not much, just hanging\n",
      "AI: Cool\n",
      "{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}\n"
     ]
    }
   ],
   "source": [
    "memory.save_context(\n",
    "    {\"input\": \"No mucho, aqu√≠ andamos\"}, \n",
    "    {\"output\": \"Guay\"}\n",
    ")\n",
    "\n",
    "print(memory.buffer)\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0be9615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otras opciones de memoria\n",
    "# memory = ConversationBufferWindowMemory(k=1)\n",
    "# memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n",
    "# memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e7a60d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hola, mi nombre es Guille, soy de Madrid y estoy aprendiendo a utilizar LangChain\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "üó®Ô∏è Respuesta 1:\n",
      "¬°Hola Guille! ¬°Qu√© gusto conocerte! LangChain es una plataforma incre√≠ble para aprender idiomas. ¬øEn qu√© idioma est√°s interesado en aprender?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human introduces themselves as Guille from Madrid and mentions they are learning to use LangChain.\n",
      "AI: ¬°Hola Guille! ¬°Qu√© gusto conocerte! LangChain es una plataforma incre√≠ble para aprender idiomas. ¬øEn qu√© idioma est√°s interesado en aprender?\n",
      "Human: Trabajo como ingeniero de datos.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "üó®Ô∏è Respuesta 2:\n",
      "¬°Qu√© interesante! Como ingeniero de datos, seguramente tienes un gran inter√©s en el an√°lisis de datos y la programaci√≥n. ¬øEst√°s buscando mejorar tus habilidades en alg√∫n lenguaje de programaci√≥n en particular?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human introduces themselves as Guille from Madrid and mentions they are learning to use LangChain. The AI greets Guille and praises LangChain as a great platform for learning languages. The AI asks Guille what language they are interested in learning. Guille responds that they work as a data engineer. The AI finds this interesting and asks if Guille is looking to improve their skills in a specific programming language.\n",
      "Human: ¬øQu√© sabes de m√≠?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "üß† Respuesta 3 (modelo usando resumen):\n",
      "Hola Guille de Madrid! S√© que est√°s aprendiendo a usar LangChain, una plataforma genial para aprender idiomas. Tambi√©n s√© que trabajas como ingeniero de datos. ¬øEst√°s interesado en mejorar tus habilidades en alg√∫n lenguaje de programaci√≥n en particular?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Crear la memoria resumida (usa otro LLM para resumir, por defecto GPT-3.5)\n",
    "summary_memory = ConversationSummaryBufferMemory(\n",
    "    llm=ChatOpenAI(\n",
    "        temperature=0,\n",
    "        model_name=\"gpt-3.5-turbo\",\n",
    "    ),\n",
    "    max_token_limit=50\n",
    ")\n",
    "\n",
    "# LLM para el chat (se puede utilizar el mismo LLM para ambas cosas, o no)\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model_name=\"gpt-3.5-turbo\"\n",
    ")\n",
    "\n",
    "# Conversaci√≥n con memoria\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=summary_memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Interacciones\n",
    "respuesta1 = conversation.predict(input=\"Hola, mi nombre es Guille, soy de Madrid y estoy aprendiendo a utilizar LangChain\")\n",
    "print(\"\\nüó®Ô∏è Respuesta 1:\")\n",
    "print(respuesta1)\n",
    "\n",
    "respuesta2 = conversation.predict(input=\"Trabajo como ingeniero de datos.\")\n",
    "print(\"\\nüó®Ô∏è Respuesta 2:\")\n",
    "print(respuesta2)\n",
    "\n",
    "respuesta3 = conversation.predict(input=\"¬øQu√© sabes de m√≠?\")\n",
    "print(\"\\nüß† Respuesta 3 (modelo usando resumen):\")\n",
    "print(respuesta3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1fe29f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: The human introduces themselves as Guille from Madrid and mentions they are learning to use LangChain. The AI greets Guille and praises LangChain as a great platform for learning languages. The AI asks Guille what language they are interested in learning. Guille responds that they work as a data engineer. The AI finds this interesting and asks if Guille is looking to improve their skills in a specific programming language. The AI then greets Guille in Spanish and summarizes what it knows about Guille, including their use of LangChain and their job as a data engineer. The AI also asks if Guille is interested in improving their skills in a specific programming language.\n",
      "{'history': 'System: The human introduces themselves as Guille from Madrid and mentions they are learning to use LangChain. The AI greets Guille and praises LangChain as a great platform for learning languages. The AI asks Guille what language they are interested in learning. Guille responds that they work as a data engineer. The AI finds this interesting and asks if Guille is looking to improve their skills in a specific programming language. The AI then greets Guille in Spanish and summarizes what it knows about Guille, including their use of LangChain and their job as a data engineer. The AI also asks if Guille is interested in improving their skills in a specific programming language.'}\n"
     ]
    }
   ],
   "source": [
    "print(summary_memory.buffer)\n",
    "print(summary_memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07a436c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fee3b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26807d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a9ffdf6",
   "metadata": {},
   "source": [
    "# 4 - Chains"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f154ff8d",
   "metadata": {},
   "source": [
    "En LangChain, una Chain (cadena) es una composici√≥n de pasos que conecta modelos de lenguaje con otras herramientas como funciones, bases de datos, prompts, parsers, memoria, etc. En lugar de lanzar una sola petici√≥n a un LLM, puedes encadenar operaciones de forma estructurada y reutilizable.\n",
    "\n",
    "En otras palabras, una Chain permite definir flujos de trabajo con l√≥gica.\n",
    "\n",
    "A continuaci√≥n, una tabla resumen de las Chains m√°s comunes en LangChain y su uso principal:\n",
    "\n",
    "| **Chain**               | **¬øPara qu√© sirve?**                                                                                                                                              |\n",
    "| ----------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `LLMChain`              | Es la cadena m√°s simple. Conecta un `PromptTemplate` con un LLM. Ideal para tareas de entrada/salida b√°sicas.                                                     |\n",
    "| `SequentialChain`       | Permite ejecutar varias `Chains` de forma secuencial, pasando la salida de una como entrada de la siguiente. √ötil para flujos paso a paso.                        |\n",
    "| `SimpleSequentialChain` | Variante de `SequentialChain` m√°s sencilla, que solo encadena directamente la salida de un paso como entrada al siguiente (sin nombres de variables intermedias). |\n",
    "| `RouterChain`           | Redirige autom√°ticamente la entrada a distintas sub-chains seg√∫n su contenido. Ideal para sistemas multi-agente o flujos condicionados.                           |\n",
    "| `ConversationChain`     | Incorpora memoria conversacional (como `BufferMemory`). Ideal para construir asistentes o chatbots con contexto.                                                  |\n",
    "| `TransformChain`        | Aplica una transformaci√≥n Python entre pasos. √ötil para parsear, preprocesar o postprocesar datos entre modelos.                                                  |\n",
    "| `MapReduceChain`        | Divide una tarea entre varios modelos (Map), luego une las respuestas (Reduce). √ötil para resumir o analizar muchos documentos.                                   |\n",
    "| `RetrievalQAChain`      | Combina un LLM con un buscador de documentos. Extrae contexto relevante antes de preguntar. Ideal para RAG.                                                       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d86e9384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain, SequentialChain\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "cee1461f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nombre</th>\n",
       "      <th>sector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EcoWave</td>\n",
       "      <td>energ√≠a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NeuroByte</td>\n",
       "      <td>tecnolog√≠a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PetNanny</td>\n",
       "      <td>mascotas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Foodloop</td>\n",
       "      <td>alimentaci√≥n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      nombre        sector\n",
       "0    EcoWave       energ√≠a\n",
       "1  NeuroByte    tecnolog√≠a\n",
       "2   PetNanny      mascotas\n",
       "3   Foodloop  alimentaci√≥n"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    \"nombre\": [\"EcoWave\", \"NeuroByte\", \"PetNanny\", \"Foodloop\"],\n",
    "    \"sector\": [\"energ√≠a\", \"tecnolog√≠a\", \"mascotas\", \"alimentaci√≥n\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ebd50c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciar modelo\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0.9\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1dc4886e",
   "metadata": {},
   "source": [
    "## 4.1. - LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9d2cba8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nombre':       nombre\n",
       " 0    EcoWave\n",
       " 1  NeuroByte\n",
       " 2   PetNanny\n",
       " 3   Foodloop,\n",
       " 'sector':          sector\n",
       " 0       energ√≠a\n",
       " 1    tecnolog√≠a\n",
       " 2      mascotas\n",
       " 3  alimentaci√≥n,\n",
       " 'eslogan': '1. EcoWave: \"Un mar de soluciones sostenibles\"\\n2. NeuroByte: \"Conectando mentes brillantes\"\\n3. PetNanny: \"Cuidando a tus peluditos como si fueran nuestros\"\\n4. Foodloop: \"Tu vuelta al mundo en cada bocado\"'}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chain para generar eslogan\n",
    "slogan_prompt = PromptTemplate.from_template(\n",
    "    \"Inventa un eslogan creativo para una startup llamada {nombre} del sector {sector}.\"\n",
    ")\n",
    "\n",
    "slogan_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=slogan_prompt,\n",
    "    output_key=\"eslogan\"\n",
    ")\n",
    "\n",
    "slogan_chain.invoke({\n",
    "    \"nombre\": df[['nombre']],\n",
    "    \"sector\": df[['sector']]\n",
    "})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a0009f6",
   "metadata": {},
   "source": [
    "## 4.2. - SimpleSequentialChain\n",
    "\n",
    "Una SimpleSequentialChain es un conjunto de chains secuenciales, donde cada chain coge como input el output de la chain precedente sin indicar el nombre de los inputs/outputs de forma expl√≠cita. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8e8dc6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain para generar eslogan\n",
    "slogan_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Inventa un eslogan creativo para una startup llamada {nombre}\"\n",
    ")\n",
    "\n",
    "slogan_chain = LLMChain(llm=llm, prompt=slogan_prompt)\n",
    "\n",
    "\n",
    "# Chain para campa√±a\n",
    "campaign_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Dado el eslogan {eslogan}, escribe una breve idea para una campa√±a publicitaria que transmita ese mensaje.\"\n",
    ")\n",
    "\n",
    "campaign_chain = LLMChain(llm=llm, prompt=campaign_prompt)\n",
    "\n",
    "\n",
    "# Flujo combinado\n",
    "overall_simple_chain = SimpleSequentialChain(\n",
    "    chains=[slogan_chain, campaign_chain],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a398b8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in LangChainTracer.on_chain_start callback: ValueError('The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().')\n",
      "Error in LangChainTracer.on_chain_start callback: ValueError('The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\"Surfeando hacia un futuro sostenible con EcoWave\"\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mLa campa√±a publicitaria consistir√≠a en mostrar a personas surfistas disfrutando de las olas en un entorno natural y limpio, con un mensaje que invite a cuidar el medio ambiente y a adoptar pr√°cticas sostenibles en nuestro d√≠a a d√≠a. Se podr√≠an incluir im√°genes de playas limpias, animales marinos felices y surfistas comprometidos con la protecci√≥n del oc√©ano. La idea es asociar la pr√°ctica del surf con la responsabilidad ambiental y promover un futuro sostenible para las generaciones venideras.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'La campa√±a publicitaria consistir√≠a en mostrar a personas surfistas disfrutando de las olas en un entorno natural y limpio, con un mensaje que invite a cuidar el medio ambiente y a adoptar pr√°cticas sostenibles en nuestro d√≠a a d√≠a. Se podr√≠an incluir im√°genes de playas limpias, animales marinos felices y surfistas comprometidos con la protecci√≥n del oc√©ano. La idea es asociar la pr√°ctica del surf con la responsabilidad ambiental y promover un futuro sostenible para las generaciones venideras.'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_simple_chain.run(df[:1][['nombre']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2c7b14a",
   "metadata": {},
   "source": [
    "## 4.3. - SequentialChain\n",
    "\n",
    "Similar a SimpleSequentialChain, pero indicando los nombres de inputs y outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "81ce0ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain para generar eslogan\n",
    "slogan_prompt = PromptTemplate.from_template(\n",
    "    \"Inventa un eslogan creativo para una startup llamada {nombre} del sector {sector}.\"\n",
    ")\n",
    "\n",
    "slogan_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=slogan_prompt,\n",
    "    output_key=\"eslogan\"\n",
    ")\n",
    "\n",
    "\n",
    "# Chain para campa√±a\n",
    "campaign_prompt = PromptTemplate.from_template(\n",
    "    \"Dado el eslogan {eslogan}, escribe una breve idea para una campa√±a publicitaria que transmita ese mensaje.\"\n",
    ")\n",
    "\n",
    "campaign_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=campaign_prompt,\n",
    "    output_key=\"campa√±a\"\n",
    ")\n",
    "\n",
    "# Flujo combinado\n",
    "sequential_chain = SequentialChain(\n",
    "    chains=[slogan_chain, campaign_chain],\n",
    "    input_variables=[\"nombre\", \"sector\"],\n",
    "    output_variables=[\"eslogan\", \"campa√±a\"],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "6f72b29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Lista para guardar resultados\n",
    "resultados = []\n",
    "\n",
    "# Aplicar la chain a cada fila\n",
    "for _, fila in df.iterrows():\n",
    "    entrada = {\n",
    "        \"nombre\": fila[\"nombre\"],\n",
    "        \"sector\": fila[\"sector\"]\n",
    "    }\n",
    "    salida = sequential_chain.invoke(entrada)\n",
    "    resultados.append({\n",
    "        **entrada,\n",
    "        **salida  # esto a√±ade 'eslogan' y 'campa√±a'\n",
    "    })\n",
    "\n",
    "# Nuevo DataFrame con resultados\n",
    "df_resultado = pd.DataFrame(resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "a909b062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nombre</th>\n",
       "      <th>sector</th>\n",
       "      <th>eslogan</th>\n",
       "      <th>campa√±a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EcoWave</td>\n",
       "      <td>energ√≠a</td>\n",
       "      <td>\"EcoWave: Energ√≠a sostenible, olas de cambio p...</td>\n",
       "      <td>La campa√±a publicitaria podr√≠a centrarse en mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NeuroByte</td>\n",
       "      <td>tecnolog√≠a</td>\n",
       "      <td>\"NeuroByte: conectando mentes, creando futuro\"</td>\n",
       "      <td>La campa√±a publicitaria consistir√° en una seri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PetNanny</td>\n",
       "      <td>mascotas</td>\n",
       "      <td>\"PetNanny: cuidando a tus peludos como si fuer...</td>\n",
       "      <td>Nuestra campa√±a publicitaria se llama \"Familia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Foodloop</td>\n",
       "      <td>alimentaci√≥n</td>\n",
       "      <td>\"¬°Dale la vuelta a tus comidas con Foodloop! D...</td>\n",
       "      <td>La campa√±a publicitaria podr√≠a incluir im√°gene...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      nombre        sector                                            eslogan  \\\n",
       "0    EcoWave       energ√≠a  \"EcoWave: Energ√≠a sostenible, olas de cambio p...   \n",
       "1  NeuroByte    tecnolog√≠a     \"NeuroByte: conectando mentes, creando futuro\"   \n",
       "2   PetNanny      mascotas  \"PetNanny: cuidando a tus peludos como si fuer...   \n",
       "3   Foodloop  alimentaci√≥n  \"¬°Dale la vuelta a tus comidas con Foodloop! D...   \n",
       "\n",
       "                                             campa√±a  \n",
       "0  La campa√±a publicitaria podr√≠a centrarse en mo...  \n",
       "1  La campa√±a publicitaria consistir√° en una seri...  \n",
       "2  Nuestra campa√±a publicitaria se llama \"Familia...  \n",
       "3  La campa√±a publicitaria podr√≠a incluir im√°gene...  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_resultado.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7230b1c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61a7317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167b755f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7e233f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exploring-langchain",
   "language": "python",
   "name": "exploring-langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ae230d7-2ab1-469e-a6a0-238293c1eeb1",
   "metadata": {},
   "source": [
    "# 0 - Librerías y variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d99484c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Librerías\n",
    "# ------------------------------------------------------------------------------\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d680460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY\n",
      "PROXYCURL_API_KEY\n",
      "TAVILY_API_KEY\n",
      "LANGCHAIN_TRACING_V2\n",
      "LANGCHAIN_ENDPOINT\n",
      "LANGCHAIN_API_KEY\n",
      "LANGCHAIN_PROJECT\n"
     ]
    }
   ],
   "source": [
    "# Variables\n",
    "# ------------------------------------------------------------------------------\n",
    "env_vars = dotenv_values()\n",
    "for key in env_vars.keys():\n",
    "    print(key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b042c85b-397e-45fd-92ff-3177d182c1cb",
   "metadata": {},
   "source": [
    "# 1 - LLM Chat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0a5d7ff",
   "metadata": {},
   "source": [
    "En este apartado se llama a la API de OpenAI directamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f09214c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1+1 es igual a 2.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importar OpenAI\n",
    "from openai import OpenAI\n",
    "\n",
    "# Inicializar el cliente\n",
    "client = OpenAI()\n",
    "\n",
    "# Llamar al LLM\n",
    "messages = [{\"role\": \"user\", \"content\": \"¿Cuánto es 1+1?\"}]\n",
    "    \n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=messages,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f6a6be29",
   "metadata": {},
   "source": [
    "Si quiero modificar el código por ejemplo para utilizar un modelo como LLaMA 3.1 desde Ollama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9887075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La respuesta a la pregunta es 2.\n"
     ]
    }
   ],
   "source": [
    "# Define el endpoint local de Ollama\n",
    "OLLAMA_URL = \"http://localhost:11434/api/chat\"\n",
    "\n",
    "# Llamar al LLM\n",
    "messages = [{\"role\": \"user\", \"content\": \"¿Cuánto es 1+1?\"}]\n",
    "\n",
    "response = requests.post(OLLAMA_URL, json={\n",
    "    \"model\": \"llama3.1\",\n",
    "    \"messages\": messages,\n",
    "    \"temperature\": 0.0,\n",
    "    \"stream\": False\n",
    "})\n",
    "\n",
    "data = response.json()\n",
    "print(data[\"message\"][\"content\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e004c686",
   "metadata": {},
   "source": [
    "Cada vez que quiero apuntar a un LLM diferente, tengo que modificar el código de forma sustancial."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88c74f4a",
   "metadata": {},
   "source": [
    "# 2 - LangChain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c809ced",
   "metadata": {},
   "source": [
    "LangChain es más un framework para construir sistemas que usan modelos de lenguaje (LLMs) que una simple librería, a continuación se muestra una tabla con los bloques principales del framework:\n",
    "\n",
    "| **Bloque**                       | **Para qué sirve**                                                                           |\n",
    "| -------------------------------- | -------------------------------------------------------------------------------------------- |\n",
    "| **1. Modelos**                   | Generan texto o representaciones vectoriales (embeddings) a partir de texto                  |\n",
    "| **2. Prompts**                   | Construyen entradas reutilizables, seguras y controladas para los modelos                    |\n",
    "| **3. Output Parsers**            | Transforman la salida del modelo (texto) en estructuras útiles como JSON, listas u objetos   |\n",
    "| **4. Memorias**                  | Guardan el historial o contexto entre interacciones (conversacionales o no)                  |\n",
    "| **5. Tools y Agents**            | Ejecutan funciones externas y permiten que un agente decida dinámicamente qué hacer y cuándo |\n",
    "| **6. Runnables**                 | Unifican cualquier componente ejecutable en un flujo modular y componible                    |\n",
    "| **7. Retrievers y VectorStores** | Permiten búsquedas semánticas para recuperar información relevante desde grandes corpus      |\n",
    "| **8. Chains**                    | Encadenan pasos fijos de procesamiento en flujos controlados (prompt → modelo → parseo)      |\n",
    "| **9. Callbacks y Tracing**       | Monitorizan, trazan y depuran la ejecución para mejorar observabilidad y debugging           |\n",
    "| **10. Loaders y Splitters**      | Cargan y fragmentan documentos largos para su posterior análisis o búsqueda                  |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa5c9eb1",
   "metadata": {},
   "source": [
    "## 2.1. - Modelos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3eeca3d6",
   "metadata": {},
   "source": [
    "En LangChain, los models (modelos) son los componentes que generan texto o responden a mensajes. Son la parte que realmente interactúa con modelos de lenguaje (LLMs) como los de OpenAI, Anthropic, Cohere, etc. LangChain organiza los modelos según lo que hacen. Los más comunes son:\n",
    "\n",
    "| Tipo de modelo     | Qué hace                                                                    | Clase típica                                |\n",
    "| ------------------ | --------------------------------------------------------------------------- | ------------------------------------------- |\n",
    "| **LLM**            | Genera texto a partir de un *prompt plano*                                  | OpenAI, HuggingFaceHub                      |\n",
    "| **ChatModel**      | Maneja conversaciones con roles (usuario, asistente, sistema)               | ChatOpenAI, ChatAnthropic                   |\n",
    "| **EmbeddingModel** | Convierte texto en vectores (útiles para búsquedas o comparación semántica) | OpenAIEmbeddings, HuggingFaceEmbeddings     |\n",
    "\n",
    "\n",
    "LangChain permite llamar a diferentes modelos, con diferentes APIs, de forma agnóstica. Cambiando ligeramente el código, puedo aprovechar una estructura ya creada para apuntar a otro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6287b970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.25\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a22bf983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "061259f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear los mensajes en el formato de LangChain\n",
    "messages = [\n",
    "    HumanMessage(content=\"¿Cuánto es 1+1?\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db336180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La respuesta a la pregunta de \"¿Cuánto es 1+1?\" es 2.\n"
     ]
    }
   ],
   "source": [
    "# Llamar a un modelo A\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1\",\n",
    "    temperature=0.0\n",
    "    )\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b140df4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1+1 es igual a 2.\n"
     ]
    }
   ],
   "source": [
    "# Llamar a un modelo B\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\", \n",
    "    temperature=0.0\n",
    "    )\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2c85cbe",
   "metadata": {},
   "source": [
    "La estructura es exactamente la misma, se podría incluso encapsular el codigo en una función y pasar el modelo como parámetro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04321608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andrew Ng es un científico de la computación e investigador en inteligencia artificial. Es conocido por su trabajo pionero en el campo del aprendizaje profundo y por ser uno de los cofundadores de Google Brain, así como de Coursera. Ng también ha sido profesor en la Universidad de Stanford y en la Universidad de California en Berkeley. Es una figura destacada en el campo de la inteligencia artificial y ha realizado importantes contribuciones en áreas como el reconocimiento de voz, la visión por computadora y la robótica.\n"
     ]
    }
   ],
   "source": [
    "def call_llm_model(\n",
    "    model_name: str,\n",
    "    temperature: float,\n",
    "    message: str\n",
    ") -> str:\n",
    "    \n",
    "    if model_name.startswith(\"gpt\"):\n",
    "        llm = ChatOpenAI(\n",
    "            model_name=model_name,\n",
    "            temperature=temperature\n",
    "        )\n",
    "    else:\n",
    "        llm = ChatOllama(\n",
    "            model=model_name,\n",
    "            temperature=temperature\n",
    "        )\n",
    "\n",
    "    response = llm.invoke([HumanMessage(content=message)])\n",
    "    return response.content\n",
    "\n",
    "\n",
    "# Ejemplo de uso:\n",
    "respuesta = call_llm_model(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0.5,\n",
    "    message=\"¿Quién es Andrew Ng?\"\n",
    ")\n",
    "\n",
    "print(respuesta)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d28abe4",
   "metadata": {},
   "source": [
    "En las últimas versiones de LangChain, se ha creado la abstracción `init_chat_model()` para inicilizar modelos de diferentes proveedores desde la misma función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbb162aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "# model = init_chat_model(\"claude-3-5-sonnet-latest\", model_provider=\"anthropic\")\n",
    "# model = init_chat_model(\"mistral-large-latest\", model_provider=\"mistralai\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a9b0129",
   "metadata": {},
   "source": [
    "## 2.2. - Prompt Templates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7a67aee",
   "metadata": {},
   "source": [
    "Las Prompt Templates (plantillas de prompt) son una herramienta de LangChain que te ayuda a crear automáticamente los mensajes que le envías al modelo de lenguaje, de una forma organizada, flexible y reutilizable.\n",
    "\n",
    "Cuando trabajas con modelos como GPT, no sueles enviar directamente el texto del usuario al modelo. Normalmente quieres hacer algo más, como:\n",
    "\n",
    "- Agregar instrucciones específicas (ej.: \"Traduce al francés...\")\n",
    "- Dar un contexto adicional al modelo (ej.: \"Eres un traductor especializado en literatura clásica...\")\n",
    "- Formatear el texto de forma especial (ej.: \"Devuelve el resultado en un .json con la estructura...\")\n",
    "- Usar el mismo formato muchas veces con diferentes datos\n",
    "\n",
    "Ahí es donde entran las prompt templates.\n",
    "\n",
    "Piensa en ellas como plantillas con huecos (como los de un formulario). Tú defines una estructura fija y dejas espacios para completar con datos reales más tarde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd77b09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"Traduce el siguiente texto al {language}: {text}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f88974e3",
   "metadata": {},
   "source": [
    "LangChain te permite definir esta plantilla y luego rellenarla automáticamente con los valores que quieras:\n",
    "- `language = \"francés\"`\n",
    "- `text = \"Hola, ¿cómo estás?\"`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68687fc0",
   "metadata": {},
   "source": [
    "Los principales templates y su propósito son:\n",
    "\n",
    "| Prompt Template                    | ¿Para qué sirve?                                                                                                                                   |\n",
    "| ---------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `ChatPromptTemplate`               | Plantilla para **modelos de chat**. Permite combinar mensajes (`System`, `Human`, etc.) fácilmente. Ideal para `ChatOpenAI`, `ChatAnthropic`, etc. |\n",
    "| `PromptTemplate`                   | Plantilla simple de texto plano. Usado con modelos de lenguaje **no conversacionales** como `OpenAI(model=\"text-davinci-003\")`.                    |\n",
    "| `SystemMessagePromptTemplate`      | Subplantilla usada dentro de `ChatPromptTemplate` para definir el **mensaje del sistema**.                                                         |\n",
    "| `HumanMessagePromptTemplate`       | Subplantilla usada dentro de `ChatPromptTemplate` para el **mensaje del usuario**                                                                  |\n",
    "| `MessagesPlaceholder`              | Placeholder especial dentro de `ChatPromptTemplate` para insertar una lista dinámica de mensajes (ej: historial de conversación).                  |\n",
    "| `AIMessagePromptTemplate`          | Subplantilla para simular un mensaje **anterior del asistente** en el historial.                                                                   |\n",
    "| `FewShotPromptTemplate`            | Plantilla para tareas de **few-shot learning**. Permite definir ejemplos que se combinan con el input.                                             |\n",
    "| `ChatMessagePromptTemplate`        | Versión más flexible que permite definir mensajes de un rol personalizado (ej: `\"role\": \"function\"`).                                              |\n",
    "| `FewShotChatMessagePromptTemplate` | Lo mismo que `FewShotPromptTemplate`, pero adaptado para `ChatPromptTemplate`. Útil si quieres ejemplos en formato chat.                           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74d2b4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15e8084e",
   "metadata": {},
   "source": [
    "\n",
    "| Método            | ¿Qué construye?                             | ¿Cuándo usarlo?                                 |\n",
    "| ----------------- | ------------------------------------------- | ----------------------------------------------- |\n",
    "| `from_template()` | Una sola plantilla de mensaje               | Para casos simples o rápidos                    |\n",
    "| `from_messages()` | Una conversación entera con múltiples roles | Cuando necesitas dar contexto o varios mensajes |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76a4a5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plantilla de traducción\n",
    "template = \"Traduce el siguiente texto al {language}: {text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13abd576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para casos sencillos\n",
    "template_string = \"Traduce el siguiente texto al {language}: {text}\"\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27e3d06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para casos mas complejos: sistema, usuarios, asistente...etc\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un trabajador del campo, oriundo de la alpujarra de Granada.\"),\n",
    "    (\"human\", \"Traduce el siguiente texto al {language}: {text}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8528252a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['language', 'text'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Eres un trabajador del campo, oriundo de la alpujarra de Granada.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language', 'text'], input_types={}, partial_variables={}, template='Traduce el siguiente texto al {language}: {text}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a30e830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Eres un trabajador del campo, oriundo de la alpujarra de Granada.'), additional_kwargs={})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67a5b66d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language', 'text'], input_types={}, partial_variables={}, template='Traduce el siguiente texto al {language}: {text}'), additional_kwargs={})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c124e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['language', 'text']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[1].prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ce1c912",
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"Granaíno\"\n",
    "text = \"¿De dónde vienes?\"\n",
    "\n",
    "translation_prompt = prompt_template.format_messages(\n",
    "    language=language,\n",
    "    text=text\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f363829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Eres un trabajador del campo, oriundo de la alpujarra de Granada.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Traduce el siguiente texto al Granaíno: ¿De dónde vienes?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9bf2683b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'langchain_core.messages.system.SystemMessage'>\n",
      "<class 'langchain_core.messages.human.HumanMessage'>\n"
     ]
    }
   ],
   "source": [
    "print(type(translation_prompt))\n",
    "print(type(translation_prompt[0]))\n",
    "print(type(translation_prompt[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "721168c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¿Ande vienes, majo?'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator_response = llm.invoke(translation_prompt)\n",
    "\n",
    "translator_response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10fdba7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¿De do vienes?'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un noble de la realeza española del siglo XV.\"),\n",
    "    (\"human\", \"Traduce esto al {language}: {text}\")\n",
    "])\n",
    "\n",
    "language = \"Castellano antiguo\"\n",
    "text = \"¿De dónde vienes?\"\n",
    "\n",
    "translation_prompt = prompt_template.format_messages(\n",
    "    language=language,\n",
    "    text=text\n",
    "    )\n",
    "\n",
    "customer_response = llm.invoke(translation_prompt)\n",
    "\n",
    "customer_response.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07abd8fe",
   "metadata": {},
   "source": [
    "## 2.3. - Output Parsers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58faea88",
   "metadata": {},
   "source": [
    "En LangChain, los output parsers son herramientas clave para procesar y estructurar las respuestas que se obtienen de los modelos de lenguaje antes de ser utilizadas en otros pasos del flujo de trabajo. Estos parsers te permiten transformar la salida en bruto de los modelos en un formato más adecuado para tu aplicación, ya sea en forma de texto, datos estructurados o incluso en la ejecución de funciones específicas.\n",
    "\n",
    "A continuación, una tabla con los output parsers más comunes en LangChain y su uso principal:\n",
    "\n",
    "| **Output Parser**        | **¿Para qué sirve?**                                                                                                                                                 |\n",
    "| ------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `JsonOutputParser`       | Convierte la salida del modelo en un objeto JSON (estructura de diccionario o lista). Ideal cuando esperas respuestas estructuradas como JSON o diccionarios.        |\n",
    "| `RegexOutputParser`      | Extrae información utilizando expresiones regulares. Se usa cuando la respuesta del modelo sigue un patrón específico que se puede identificar con regex.            |\n",
    "| `StructuredOutputParser` | Permite parsear la salida en estructuras de datos más complejas. Ideal para cuando necesitas que el modelo devuelva datos tabulares o jerárquicos.                   |\n",
    "| `VariableParser`         | Analiza las respuestas del modelo en busca de valores de variables específicas. Utilizado cuando deseas extraer datos de las respuestas para usarlos en otros pasos. |\n",
    "| `TextOutputParser`       | Convierte la salida en un texto plano procesable. Ideal cuando la salida del modelo es simplemente texto sin estructura.                                             |\n",
    "| `SQLOutputParser`        | Convierte la salida del modelo en consultas SQL estructuradas. Es útil si el modelo está generando consultas a bases de datos.                                       |\n",
    "| `PythonOutputParser`     | Convierte la salida del modelo en código Python ejecutable. Utilizado cuando necesitas generar código a partir de la respuesta del modelo.                           |\n",
    "| `ActionOutputParser`     | Permite que la salida sea convertida en una acción específica o una ejecución de función. Ideal para flujos de trabajo más dinámicos donde se deben ejecutar tareas. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "526e9ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': False, 'delivery_days': 5, 'price_value': 'muy asequible!'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    \"gift\": False,\n",
    "    \"delivery_days\": 5,\n",
    "    \"price_value\": \"muy asequible!\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "53c9ce45",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review = \"\"\"\\\n",
    "Este soplador de hojas es bastante increíble. Tiene cuatro configuraciones:\\\n",
    "soplador de vela, brisa suave, ciudad ventosa y tornado. \\\n",
    "Llegó en dos días, justo a tiempo para el regalo de aniversario de mi esposa. \\\n",
    "Creo que a mi esposa le gustó tanto que se quedó sin palabras. \\\n",
    "Hasta ahora he sido el único que lo ha usado, y lo he estado usando cada dos mañanas para limpiar las hojas de nuestro césped. \\\n",
    "Es ligeramente más caro que los otros sopladores de hojas que hay por ahí, pero creo que vale la pena por las características adicionales.\n",
    "\"\"\"\n",
    "\n",
    "review_template = \"\"\"\\\n",
    "Para el siguiente texto, extrae la siguiente información:\n",
    "\n",
    "gift: ¿Fue el artículo comprado como un regalo para otra persona? \\\n",
    "Responde True si es sí, False si no o si no se sabe.\n",
    "\n",
    "delivery_days: ¿Cuántos días tardó en llegar el producto? \\\n",
    "Si esta información no se encuentra, devuelve -1.\n",
    "\n",
    "price_value: Extrae cualquier frase sobre el valor o precio,\\\n",
    "y devuélvelas como una lista de Python separada por comas.\n",
    "\n",
    "Formatea la salida como JSON con las siguientes claves:\n",
    "gift --> boolean\n",
    "delivery_days --> integer\n",
    "price_value --> python list\n",
    "\n",
    "texto: {text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6fcaf224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['text'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='Para el siguiente texto, extrae la siguiente información:\\n\\ngift: ¿Fue el artículo comprado como un regalo para otra persona? Responde True si es sí, False si no o si no se sabe.\\n\\ndelivery_days: ¿Cuántos días tardó en llegar el producto? Si esta información no se encuentra, devuelve -1.\\n\\nprice_value: Extrae cualquier frase sobre el valor o precio,y devuélvelas como una lista de Python separada por comas.\\n\\nFormatea la salida como JSON con las siguientes claves:\\ngift --> boolean\\ndelivery_days --> integer\\nprice_value --> python list\\n\\ntexto: {text}\\n'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "497a6778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c7b68e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"gift\": true,\n",
      "    \"delivery_days\": 2,\n",
      "    \"price_value\": [\"ligeramente más caro\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "messages = prompt_template.format_messages(text=customer_review)\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57b33cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "11ca7f06",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Si intentamos acceder a response como si fuera un diccionario...\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m(\u001b[33m'\u001b[39m\u001b[33mgift\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "# Si intentamos acceder a response como si fuera un diccionario...\n",
    "response.content.get('gift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbe8c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e078cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_schemas=[ResponseSchema(name='gift', description='True si fue un regalo, False si no.', type='string'), ResponseSchema(name='delivery_days', description='Número de días que tardó en llegar el producto.', type='string'), ResponseSchema(name='price_value', description='Frases relacionadas con el valor o precio del producto.', type='string')]\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // True si fue un regalo, False si no.\n",
      "\t\"delivery_days\": string  // Número de días que tardó en llegar el producto.\n",
      "\t\"price_value\": string  // Frases relacionadas con el valor o precio del producto.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Definir los esquemas de respuesta\n",
    "response_schemas = [\n",
    "    ResponseSchema(\n",
    "        name=\"gift\",\n",
    "        description=\"True si fue un regalo, False si no.\"\n",
    "    ),\n",
    "    ResponseSchema(\n",
    "        name=\"delivery_days\",\n",
    "        description=\"Número de días que tardó en llegar el producto.\"\n",
    "    ),\n",
    "    ResponseSchema(\n",
    "        name=\"price_value\",\n",
    "        description=\"Frases relacionadas con el valor o precio del producto.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Crear el parser estructurado\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "print(output_parser)\n",
    "print(output_parser.get_format_instructions())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4d44dc1",
   "metadata": {},
   "source": [
    "Cuando se utiliza `response_schemas`, LangChain siempre interpreta que los resultados son string. Por eso en `review_template` se indica:\n",
    "\n",
    "Formatea la salida como JSON con las siguientes claves:\n",
    "- gift --> boolean\n",
    "- delivery_days --> integer\n",
    "- price_value --> python list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a36087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: Para el siguiente texto, extrae la siguiente información:\\n\\ngift: ¿Fue el artículo comprado como un regalo para otra persona? Responde True si es sí, False si no o si no se sabe.\\n\\ndelivery_days: ¿Cuántos días tardó en llegar el producto? Si esta información no se encuentra, devuelve -1.\\n\\nprice_value: Extrae cualquier frase sobre el valor o precio,y devuélvelas como una lista de Python separada por comas.\\n\\nFormatea la salida como JSON con las siguientes claves:\\ngift --> boolean\\ndelivery_days --> integer\\nprice_value --> python list\\n\\ntexto: Este soplador de hojas es bastante increíble. Tiene cuatro configuraciones:soplador de vela, brisa suave, ciudad ventosa y tornado. Llegó en dos días, justo a tiempo para el regalo de aniversario de mi esposa. Creo que a mi esposa le gustó tanto que se quedó sin palabras. Hasta ahora he sido el único que lo ha usado, y lo he estado usando cada dos mañanas para limpiar las hojas de nuestro césped. Es ligeramente más caro que los otros sopladores de hojas que hay por ahí, pero creo que vale la pena por las características adicionales.\\n\\n'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construir el prompt con instrucciones de formato\n",
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "\n",
    "formatted_prompt = prompt_template.format(\n",
    "    text=customer_review,\n",
    "    format_instructions=output_parser.get_format_instructions()\n",
    ")\n",
    "\n",
    "formatted_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbafffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llamar al modelo\n",
    "response = llm.invoke([HumanMessage(content=formatted_prompt)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ac61c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gift': True, 'delivery_days': 2, 'price_value': ['ligeramente más caro']}\n"
     ]
    }
   ],
   "source": [
    "# Parsear la salida\n",
    "parsed_output = output_parser.parse(response.content)\n",
    "\n",
    "# Mostrar el resultado como diccionario\n",
    "print(parsed_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa4da39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clave: gift | Valor: True | Tipo de dato: <class 'bool'>\n",
      "Clave: delivery_days | Valor: 2 | Tipo de dato: <class 'int'>\n",
      "Clave: price_value | Valor: ['ligeramente más caro'] | Tipo de dato: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "for key, value in parsed_output.items():\n",
    "    print(f\"Clave: {key} | Valor: {value} | Tipo de dato: {type(value)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5842a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb5be5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c767a12a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "586b5d87",
   "metadata": {},
   "source": [
    "## 2.4. - LangChain: Memory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9957f70f",
   "metadata": {},
   "source": [
    "En LangChain, la memoria se refiere a la capacidad de una cadena o agente para recordar información entre interacciones. Es útil especialmente en contextos conversacionales o multistep, donde necesitas que el modelo tenga conocimiento del contexto anterior.\n",
    "\n",
    "Por defecto, los modelos como GPT no tienen memoria entre llamadas. LangChain permite gestionar esto incluyendo “memoria” en una cadena (Chain) o un agente (Agent), haciendo que el contexto previo se incluya automáticamente en nuevas solicitudes.\n",
    "\n",
    "A continuación, una tabla con los tipos de memoria más comunes en LangChain y su uso principal:\n",
    "\n",
    "| **Tipo de Memoria**              | **¿Para qué sirve?**                                                                                                                                    |\n",
    "| -------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `ConversationBufferMemory`       | Guarda todo el historial de la conversación como una cadena de texto. Es la forma más simple de mantener el contexto completo de una conversación.      |\n",
    "| `ConversationBufferWindowMemory` | Similar a `ConversationBufferMemory`, pero solo conserva las **últimas *k* interacciones** (ventana deslizante). Útil para limitar el contexto enviado. |\n",
    "| `ConversationTokenBufferMemory`  | Guarda la conversación basándose en un **límite de tokens**, no de turnos. Conserva solo la parte del historial que cabe dentro de un número de tokens. |\n",
    "| `ConversationSummaryMemory`      | Resume el historial de la conversación en un texto corto. Ideal para mantener contexto en conversaciones largas sin exceder el límite de tokens.        |\n",
    "\n",
    "Diferencias clave:\n",
    "\n",
    "| Memoria                          | ¿Recuerda todo?       | ¿Controla tamaño? | ¿Resume contenido? | ¿Ideal para...?                               |\n",
    "| -------------------------------- | --------------------- | ----------------- | ------------------ | --------------------------------------------- |\n",
    "| `ConversationBufferMemory`       | Sí                    | ❌                 | ❌                  | Conversaciones cortas o demostraciones        |\n",
    "| `ConversationBufferWindowMemory` | No (solo últimas *k*) | ✅ (*k*)           | ❌                  | Chats donde solo importa el contexto reciente |\n",
    "| `ConversationTokenBufferMemory`  | No                    | ✅ (*n* tokens)    | ❌                  | Ajustar a límites estrictos de tokens         |\n",
    "| `ConversationSummaryMemory`      | No (resume todo)      | ✅                 | ✅                  | Chats largos sin perder el hilo               |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d95f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import (\n",
    "    ConversationBufferMemory,\n",
    "    ConversationBufferWindowMemory,\n",
    "    ConversationTokenBufferMemory,\n",
    "    ConversationSummaryBufferMemory\n",
    ")\n",
    "\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.prompts import PromptTemplate\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a67ead6",
   "metadata": {},
   "source": [
    "A continuación vemos un ejemplo, de como los modelos no incorporan memoria de forma nativa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edb1421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primera respuesta:\n",
      "¡Hola Guille! ¿En qué puedo ayudarte hoy?\n",
      "\n",
      "Segunda respuesta (sin memoria):\n",
      "Lo siento, no tengo la capacidad de saber tu nombre a menos que me lo digas. ¿Cuál es tu nombre?\n"
     ]
    }
   ],
   "source": [
    "# Instanciar modelo\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\", \n",
    "    temperature=0.0\n",
    "    )\n",
    "\n",
    "# Primera interacción\n",
    "message = \"Mi nombre es Guille.\"\n",
    "response = llm.invoke([HumanMessage(content=message)])\n",
    "print(\"Primera respuesta:\")\n",
    "print(response.content)\n",
    "\n",
    "# Segunda interacción sin memoria\n",
    "message = \"¿Cuál es mi nombre?\"\n",
    "response = llm.invoke([HumanMessage(content=message)])\n",
    "print(\"\\nSegunda respuesta (sin memoria):\")\n",
    "print(response.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbf07f88",
   "metadata": {},
   "source": [
    "Vamos a darle memoria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebc6f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_224496/1346084784.py:6: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n",
      "/tmp/ipykernel_224496/1346084784.py:8: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  conversation = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\", \n",
    "    temperature=0.0\n",
    "    )\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4580f2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hola, mi nombre es Guille\n",
      "AI: ¡Hola Guille! ¿Cómo estás hoy? Soy un asistente de inteligencia artificial aquí para ayudarte en lo que necesites. ¿En qué puedo ayudarte hoy?\n",
      "Human: Hola, mi nombre es Guille\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'¡Hola Guille! Me alegra conocerte. ¿Cómo puedo ayudarte hoy?'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hola, mi nombre es Guille\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43dfa5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hola, mi nombre es Guille\n",
      "AI: ¡Hola Guille! ¿Cómo estás hoy? Soy un asistente de inteligencia artificial aquí para ayudarte en lo que necesites. ¿En qué puedo ayudarte hoy?\n",
      "Human: Hola, mi nombre es Guille\n",
      "AI: ¡Hola Guille! Me alegra conocerte. ¿Cómo puedo ayudarte hoy?\n",
      "Human: ¿Cuánto es 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1 + 1 es igual a 2.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"¿Cuánto es 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4d3304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hola, mi nombre es Guille\n",
      "AI: ¡Hola Guille! ¿Cómo estás hoy? Soy un asistente de inteligencia artificial aquí para ayudarte en lo que necesites. ¿En qué puedo ayudarte hoy?\n",
      "Human: Hola, mi nombre es Guille\n",
      "AI: ¡Hola Guille! Me alegra conocerte. ¿Cómo puedo ayudarte hoy?\n",
      "Human: ¿Cuánto es 1+1?\n",
      "AI: 1 + 1 es igual a 2.\n",
      "Human: ¿Cuál es mi nombre?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Tu nombre es Guille.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"¿Cuál es mi nombre?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd56038f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hola, mi nombre es Guille\n",
      "AI: ¡Hola Guille! ¡Encantado de conocerte! ¿En qué puedo ayudarte hoy?\n",
      "Human: ¿Cuánto es 1+1?\n",
      "AI: 1+1 es igual a 2. ¿Hay algo más en lo que pueda ayudarte, Guille?\n",
      "Human: What is my name?\n",
      "AI: Tu nombre es Guille. ¿Hay algo más en lo que pueda ayudarte?\n",
      "{'history': 'Human: Hola, mi nombre es Guille\\nAI: ¡Hola Guille! ¡Encantado de conocerte! ¿En qué puedo ayudarte hoy?\\nHuman: ¿Cuánto es 1+1?\\nAI: 1+1 es igual a 2. ¿Hay algo más en lo que pueda ayudarte, Guille?\\nHuman: What is my name?\\nAI: Tu nombre es Guille. ¿Hay algo más en lo que pueda ayudarte?'}\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3fc2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "\n",
    "memory.save_context(\n",
    "    {\"input\": \"Hola\"}, \n",
    "    {\"output\": \"¿Qué tal?\"}\n",
    ")\n",
    "\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a651c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n",
      "Human: Not much, just hanging\n",
      "AI: Cool\n",
      "{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}\n"
     ]
    }
   ],
   "source": [
    "memory.save_context(\n",
    "    {\"input\": \"No mucho, aquí andamos\"}, \n",
    "    {\"output\": \"Guay\"}\n",
    ")\n",
    "\n",
    "print(memory.buffer)\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0be9615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otras opciones de memoria\n",
    "# memory = ConversationBufferWindowMemory(k=1)\n",
    "# memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n",
    "# memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a60d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hola, mi nombre es Guille, soy de Madrid y estoy aprendiendo a utilizar LangChain\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "🗨️ Respuesta 1:\n",
      "¡Hola Guille! ¡Qué gusto conocerte! LangChain es una plataforma increíble para aprender idiomas. ¿En qué idioma estás interesado en aprender?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human introduces themselves as Guille from Madrid and mentions they are learning to use LangChain.\n",
      "AI: ¡Hola Guille! ¡Qué gusto conocerte! LangChain es una plataforma increíble para aprender idiomas. ¿En qué idioma estás interesado en aprender?\n",
      "Human: Trabajo como ingeniero de datos.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "🗨️ Respuesta 2:\n",
      "¡Qué interesante! Como ingeniero de datos, seguramente tienes un gran interés en el análisis de datos y la programación. ¿Estás buscando mejorar tus habilidades en algún lenguaje de programación en particular?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human introduces themselves as Guille from Madrid and mentions they are learning to use LangChain. The AI greets Guille and praises LangChain as a great platform for learning languages. The AI asks Guille what language they are interested in learning. Guille responds that they work as a data engineer. The AI finds this interesting and asks if Guille is looking to improve their skills in a specific programming language.\n",
      "Human: ¿Qué sabes de mí?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "🧠 Respuesta 3 (modelo usando resumen):\n",
      "Hola Guille de Madrid! Sé que estás aprendiendo a usar LangChain, una plataforma genial para aprender idiomas. También sé que trabajas como ingeniero de datos. ¿Estás interesado en mejorar tus habilidades en algún lenguaje de programación en particular?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Crear la memoria resumida (usa otro LLM para resumir, por defecto GPT-3.5)\n",
    "summary_memory = ConversationSummaryBufferMemory(\n",
    "    llm=ChatOpenAI(\n",
    "        temperature=0,\n",
    "        model_name=\"gpt-3.5-turbo\",\n",
    "    ),\n",
    "    max_token_limit=50\n",
    ")\n",
    "\n",
    "# LLM para el chat (se puede utilizar el mismo LLM para ambas cosas, o no)\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model_name=\"gpt-3.5-turbo\"\n",
    ")\n",
    "\n",
    "# Conversación con memoria\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=summary_memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Interacciones\n",
    "respuesta1 = conversation.predict(input=\"Hola, mi nombre es Guille, soy de Madrid y estoy aprendiendo a utilizar LangChain\")\n",
    "print(\"\\n🗨️ Respuesta 1:\")\n",
    "print(respuesta1)\n",
    "\n",
    "respuesta2 = conversation.predict(input=\"Trabajo como ingeniero de datos.\")\n",
    "print(\"\\n🗨️ Respuesta 2:\")\n",
    "print(respuesta2)\n",
    "\n",
    "respuesta3 = conversation.predict(input=\"¿Qué sabes de mí?\")\n",
    "print(\"\\n🧠 Respuesta 3 (modelo usando resumen):\")\n",
    "print(respuesta3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe29f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: The human introduces themselves as Guille from Madrid and mentions they are learning to use LangChain. The AI greets Guille and praises LangChain as a great platform for learning languages. The AI asks Guille what language they are interested in learning. Guille responds that they work as a data engineer. The AI finds this interesting and asks if Guille is looking to improve their skills in a specific programming language. The AI then greets Guille in Spanish and summarizes what it knows about Guille, including their use of LangChain and their job as a data engineer. The AI also asks if Guille is interested in improving their skills in a specific programming language.\n",
      "{'history': 'System: The human introduces themselves as Guille from Madrid and mentions they are learning to use LangChain. The AI greets Guille and praises LangChain as a great platform for learning languages. The AI asks Guille what language they are interested in learning. Guille responds that they work as a data engineer. The AI finds this interesting and asks if Guille is looking to improve their skills in a specific programming language. The AI then greets Guille in Spanish and summarizes what it knows about Guille, including their use of LangChain and their job as a data engineer. The AI also asks if Guille is interested in improving their skills in a specific programming language.'}\n"
     ]
    }
   ],
   "source": [
    "print(summary_memory.buffer)\n",
    "print(summary_memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07a436c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fee3b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26807d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a9ffdf6",
   "metadata": {},
   "source": [
    "## 2.5. - Chains"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f154ff8d",
   "metadata": {},
   "source": [
    "En LangChain, una Chain (cadena) es una composición de pasos que conecta modelos de lenguaje con otras herramientas como funciones, bases de datos, prompts, parsers, memoria, etc. En lugar de lanzar una sola petición a un LLM, puedes encadenar operaciones de forma estructurada y reutilizable.\n",
    "\n",
    "En otras palabras, una Chain permite definir flujos de trabajo con lógica.\n",
    "\n",
    "A continuación, una tabla resumen de las Chains más comunes en LangChain y su uso principal:\n",
    "\n",
    "| **Chain**               | **¿Para qué sirve?**                                                                                                                                              |\n",
    "| ----------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `LLMChain`              | Es la cadena más simple. Conecta un `PromptTemplate` con un LLM. Ideal para tareas de entrada/salida básicas.                                                     |\n",
    "| `SequentialChain`       | Permite ejecutar varias `Chains` de forma secuencial, pasando la salida de una como entrada de la siguiente. Útil para flujos paso a paso.                        |\n",
    "| `SimpleSequentialChain` | Variante de `SequentialChain` más sencilla, que solo encadena directamente la salida de un paso como entrada al siguiente (sin nombres de variables intermedias). |\n",
    "| `RouterChain`           | Redirige automáticamente la entrada a distintas sub-chains según su contenido. Ideal para sistemas multi-agente o flujos condicionados.                           |\n",
    "| `ConversationChain`     | Incorpora memoria conversacional (como `BufferMemory`). Ideal para construir asistentes o chatbots con contexto.                                                  |\n",
    "| `TransformChain`        | Aplica una transformación Python entre pasos. Útil para parsear, preprocesar o postprocesar datos entre modelos.                                                  |\n",
    "| `MapReduceChain`        | Divide una tarea entre varios modelos (Map), luego une las respuestas (Reduce). Útil para resumir o analizar muchos documentos.                                   |\n",
    "| `RetrievalQAChain`      | Combina un LLM con un buscador de documentos. Extrae contexto relevante antes de preguntar. Ideal para RAG.                                                       |\n",
    "\n",
    "Las anteriores Chains son desarrolladas por LangChain, pero el usuario también puede definir sus propias chains encadenando diferentes modelos, templates, parsers...etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86e9384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain, SequentialChain\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee1461f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nombre</th>\n",
       "      <th>sector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EcoWave</td>\n",
       "      <td>energía</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NeuroByte</td>\n",
       "      <td>tecnología</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PetNanny</td>\n",
       "      <td>mascotas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Foodloop</td>\n",
       "      <td>alimentación</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      nombre        sector\n",
       "0    EcoWave       energía\n",
       "1  NeuroByte    tecnología\n",
       "2   PetNanny      mascotas\n",
       "3   Foodloop  alimentación"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    \"nombre\": [\"EcoWave\", \"NeuroByte\", \"PetNanny\", \"Foodloop\"],\n",
    "    \"sector\": [\"energía\", \"tecnología\", \"mascotas\", \"alimentación\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd50c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciar modelo\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0.9\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1dc4886e",
   "metadata": {},
   "source": [
    "### 2.5.1. - LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2cba8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nombre':       nombre\n",
       " 0    EcoWave\n",
       " 1  NeuroByte\n",
       " 2   PetNanny\n",
       " 3   Foodloop,\n",
       " 'sector':          sector\n",
       " 0       energía\n",
       " 1    tecnología\n",
       " 2      mascotas\n",
       " 3  alimentación,\n",
       " 'eslogan': '1. EcoWave: \"Un mar de soluciones sostenibles\"\\n2. NeuroByte: \"Conectando mentes brillantes\"\\n3. PetNanny: \"Cuidando a tus peluditos como si fueran nuestros\"\\n4. Foodloop: \"Tu vuelta al mundo en cada bocado\"'}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chain para generar eslogan\n",
    "slogan_prompt = PromptTemplate.from_template(\n",
    "    \"Inventa un eslogan creativo para una startup llamada {nombre} del sector {sector}.\"\n",
    ")\n",
    "\n",
    "slogan_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=slogan_prompt,\n",
    "    output_key=\"eslogan\"\n",
    ")\n",
    "\n",
    "slogan_chain.invoke({\n",
    "    \"nombre\": df[['nombre']],\n",
    "    \"sector\": df[['sector']]\n",
    "})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a0009f6",
   "metadata": {},
   "source": [
    "### 2.5.2. - SimpleSequentialChain\n",
    "\n",
    "Una SimpleSequentialChain es un conjunto de chains secuenciales, donde cada chain coge como input el output de la chain precedente sin indicar el nombre de los inputs/outputs de forma explícita. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8dc6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain para generar eslogan\n",
    "slogan_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Inventa un eslogan creativo para una startup llamada {nombre}\"\n",
    ")\n",
    "\n",
    "slogan_chain = LLMChain(llm=llm, prompt=slogan_prompt)\n",
    "\n",
    "\n",
    "# Chain para campaña\n",
    "campaign_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Dado el eslogan {eslogan}, escribe una breve idea para una campaña publicitaria que transmita ese mensaje.\"\n",
    ")\n",
    "\n",
    "campaign_chain = LLMChain(llm=llm, prompt=campaign_prompt)\n",
    "\n",
    "\n",
    "# Flujo combinado\n",
    "overall_simple_chain = SimpleSequentialChain(\n",
    "    chains=[slogan_chain, campaign_chain],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a398b8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in LangChainTracer.on_chain_start callback: ValueError('The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().')\n",
      "Error in LangChainTracer.on_chain_start callback: ValueError('The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\"Surfeando hacia un futuro sostenible con EcoWave\"\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mLa campaña publicitaria consistiría en mostrar a personas surfistas disfrutando de las olas en un entorno natural y limpio, con un mensaje que invite a cuidar el medio ambiente y a adoptar prácticas sostenibles en nuestro día a día. Se podrían incluir imágenes de playas limpias, animales marinos felices y surfistas comprometidos con la protección del océano. La idea es asociar la práctica del surf con la responsabilidad ambiental y promover un futuro sostenible para las generaciones venideras.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'La campaña publicitaria consistiría en mostrar a personas surfistas disfrutando de las olas en un entorno natural y limpio, con un mensaje que invite a cuidar el medio ambiente y a adoptar prácticas sostenibles en nuestro día a día. Se podrían incluir imágenes de playas limpias, animales marinos felices y surfistas comprometidos con la protección del océano. La idea es asociar la práctica del surf con la responsabilidad ambiental y promover un futuro sostenible para las generaciones venideras.'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_simple_chain.run(df[:1][['nombre']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2c7b14a",
   "metadata": {},
   "source": [
    "### 2.5.3. - SequentialChain\n",
    "\n",
    "Similar a SimpleSequentialChain, pero indicando los nombres de inputs y outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ce0ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain para generar eslogan\n",
    "slogan_prompt = PromptTemplate.from_template(\n",
    "    \"Inventa un eslogan creativo para una startup llamada {nombre} del sector {sector}.\"\n",
    ")\n",
    "\n",
    "slogan_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=slogan_prompt,\n",
    "    output_key=\"eslogan\"\n",
    ")\n",
    "\n",
    "\n",
    "# Chain para campaña\n",
    "campaign_prompt = PromptTemplate.from_template(\n",
    "    \"Dado el eslogan {eslogan}, escribe una breve idea para una campaña publicitaria que transmita ese mensaje.\"\n",
    ")\n",
    "\n",
    "campaign_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=campaign_prompt,\n",
    "    output_key=\"campaña\"\n",
    ")\n",
    "\n",
    "# Flujo combinado\n",
    "sequential_chain = SequentialChain(\n",
    "    chains=[slogan_chain, campaign_chain],\n",
    "    input_variables=[\"nombre\", \"sector\"],\n",
    "    output_variables=[\"eslogan\", \"campaña\"],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f72b29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Lista para guardar resultados\n",
    "resultados = []\n",
    "\n",
    "# Aplicar la chain a cada fila\n",
    "for _, fila in df.iterrows():\n",
    "\n",
    "    entrada = {\n",
    "        \"nombre\": fila[\"nombre\"],\n",
    "        \"sector\": fila[\"sector\"]\n",
    "    }\n",
    "\n",
    "    salida = sequential_chain.invoke(entrada)\n",
    "    \n",
    "    resultados.append({\n",
    "        **entrada,\n",
    "        **salida  # esto añade 'eslogan' y 'campaña'\n",
    "    })\n",
    "\n",
    "# Nuevo DataFrame con resultados\n",
    "df_resultado = pd.DataFrame(resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a909b062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nombre</th>\n",
       "      <th>sector</th>\n",
       "      <th>eslogan</th>\n",
       "      <th>campaña</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EcoWave</td>\n",
       "      <td>energía</td>\n",
       "      <td>\"EcoWave: Energía sostenible, olas de cambio p...</td>\n",
       "      <td>La campaña publicitaria podría centrarse en mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NeuroByte</td>\n",
       "      <td>tecnología</td>\n",
       "      <td>\"NeuroByte: conectando mentes, creando futuro\"</td>\n",
       "      <td>La campaña publicitaria consistirá en una seri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PetNanny</td>\n",
       "      <td>mascotas</td>\n",
       "      <td>\"PetNanny: cuidando a tus peludos como si fuer...</td>\n",
       "      <td>Nuestra campaña publicitaria se llama \"Familia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Foodloop</td>\n",
       "      <td>alimentación</td>\n",
       "      <td>\"¡Dale la vuelta a tus comidas con Foodloop! D...</td>\n",
       "      <td>La campaña publicitaria podría incluir imágene...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      nombre        sector                                            eslogan  \\\n",
       "0    EcoWave       energía  \"EcoWave: Energía sostenible, olas de cambio p...   \n",
       "1  NeuroByte    tecnología     \"NeuroByte: conectando mentes, creando futuro\"   \n",
       "2   PetNanny      mascotas  \"PetNanny: cuidando a tus peludos como si fuer...   \n",
       "3   Foodloop  alimentación  \"¡Dale la vuelta a tus comidas con Foodloop! D...   \n",
       "\n",
       "                                             campaña  \n",
       "0  La campaña publicitaria podría centrarse en mo...  \n",
       "1  La campaña publicitaria consistirá en una seri...  \n",
       "2  Nuestra campaña publicitaria se llama \"Familia...  \n",
       "3  La campaña publicitaria podría incluir imágene...  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_resultado.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7230b1c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61a7317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167b755f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be7e233f",
   "metadata": {},
   "source": [
    "## 2.6. - Vector Stores y Retrievers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "636168ab",
   "metadata": {},
   "source": [
    "**Vector Store:**\n",
    "\n",
    "Un vector store es una base de datos optimizada para almacenar y buscar vectores, que son representaciones numéricas de datos, como texto.\n",
    "\n",
    "Cuando conviertes texto en vectores usando un modelo de embeddings, esos vectores se pueden almacenar en un vector store. Posteriormente, puedes hacer búsquedas por similitud: das un vector (por ejemplo, el de una pregunta del usuario) y el sistema devuelve los vectores más cercanos, que corresponden a los textos más relevantes.\n",
    "\n",
    "Es la infraestructura que permite búsquedas semánticas rápidas y eficientes.\n",
    "\n",
    "**Retrievers:**\n",
    "\n",
    "Un retriever es un componente que, dado un texto de entrada (como una pregunta), recupera los documentos más relevantes desde alguna fuente. Es una abstracción de LangChain que encapsula la lógica de recuperación, sin preocuparse por cómo están almacenados los datos. El objetivo es simplemente: \"dame lo más relevante que tengas sobre esta consulta\".\n",
    "\n",
    "Puede estar respaldado por un vector store, un motor de búsqueda clásico, una API externa, o cualquier otra fuente.\n",
    "\n",
    "Cuando juntas un vector store con un retriever, lo que haces es usar el vector store como backend para que el retriever recupere documentos basándose en similitud semántica. De este modo se puede implementar un RAG, y darle un conocimiento extra al LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "334a4764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "569bbe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loader de LangChain que lee el CSV y convierte cada fila en un documento\n",
    "file = './data/OutdoorClothingCatalog_1000.csv'\n",
    "loader = CSVLoader(file_path=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "55ec1853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.csv_loader.CSVLoader at 0x7f6224f9b390>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f64a3ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './data/OutdoorClothingCatalog_1000.csv', 'row': 0}, page_content=\": 0\\nname: Women's Campside Oxfords\\ndescription: This ultracomfortable lace-to-toe Oxford boasts a super-soft canvas, thick cushioning, and quality construction for a broken-in feel from the first time you put them on. \\n\\nSize & Fit: Order regular shoe size. For half sizes not offered, order up to next whole size. \\n\\nSpecs: Approx. weight: 1 lb.1 oz. per pair. \\n\\nConstruction: Soft canvas material for a broken-in feel and look. Comfortable EVA innersole with Cleansport NXT® antimicrobial odor control. Vintage hunt, fish and camping motif on innersole. Moderate arch contour of innersole. EVA foam midsole for cushioning and support. Chain-tread-inspired molded rubber outsole with modified chain-tread pattern. Imported. \\n\\nQuestions? Please contact us for any inquiries.\"),\n",
       " Document(metadata={'source': './data/OutdoorClothingCatalog_1000.csv', 'row': 1}, page_content=': 1\\nname: Recycled Waterhog Dog Mat, Chevron Weave\\ndescription: Protect your floors from spills and splashing with our ultradurable recycled Waterhog dog mat made right here in the USA. \\n\\nSpecs\\nSmall - Dimensions: 18\" x 28\". \\nMedium - Dimensions: 22.5\" x 34.5\".\\n\\nWhy We Love It\\nMother nature, wet shoes and muddy paws have met their match with our Recycled Waterhog mats. Ruggedly constructed from recycled plastic materials, these ultratough mats help keep dirt and water off your floors and plastic out of landfills, trails and oceans. Now, that\\'s a win-win for everyone.\\n\\nFabric & Care\\nVacuum or hose clean.\\n\\nConstruction\\n24 oz. polyester fabric made from 94% recycled materials.\\nRubber backing.\\n\\nAdditional Features\\nFeatures an -exclusive design.\\nFeatures thick and thin fibers for scraping dirt and absorbing water.\\nDries quickly and resists fading, rotting, mildew and shedding.\\nUse indoors or out.\\nMade in the USA.\\n\\nHave questions? Reach out to our customer service team with any questions you may have.')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "docs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9d37b324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n",
      "[-0.045542046427726746, 0.014181733131408691, -0.010796173475682735, -0.014056342653930187, -0.01816917024552822]\n"
     ]
    }
   ],
   "source": [
    "# Instanciar embedding\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Convertir string a vector mediante un embedding\n",
    "embed = embeddings.embed_query(\"Hola mi nombre es Guille\")\n",
    "\n",
    "# Visualizar embedding\n",
    "print(len(embed))\n",
    "print(embed[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "72a07ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gmachin/.local/share/virtualenvs/exploring-langchain-UOzDUSui/lib/python3.11/site-packages/pydantic/_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "# Crear VectorStore en memoria\n",
    "db = DocArrayInMemorySearch.from_documents(\n",
    "    docs, \n",
    "    embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8d740d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# Query en lenguaje natural\n",
    "query = \"Please suggest a shirt with sunblocking\"\n",
    "\n",
    "# Llamar al VectorStore\n",
    "docs_result = db.similarity_search(query, k=4)\n",
    "\n",
    "# Documentos relevantes\n",
    "print(len(docs_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "97053224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': './data/OutdoorClothingCatalog_1000.csv', 'row': 255}, page_content=': 255\\nname: Sun Shield Shirt by\\ndescription: \"Block the sun, not the fun – our high-performance sun shirt is guaranteed to protect from harmful UV rays. \\n\\nSize & Fit: Slightly Fitted: Softly shapes the body. Falls at hip.\\n\\nFabric & Care: 78% nylon, 22% Lycra Xtra Life fiber. UPF 50+ rated – the highest rated sun protection possible. Handwash, line dry.\\n\\nAdditional Features: Wicks moisture for quick-drying comfort. Fits comfortably over your favorite swimsuit. Abrasion resistant for season after season of wear. Imported.\\n\\nSun Protection That Won\\'t Wear Off\\nOur high-performance fabric provides SPF 50+ sun protection, blocking 98% of the sun\\'s harmful rays. This fabric is recommended by The Skin Cancer Foundation as an effective UV protectant.')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7318c147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': './data/OutdoorClothingCatalog_1000.csv', 'row': 374}, page_content=\": 374\\nname: Men's Plaid Tropic Shirt, Short-Sleeve\\ndescription: Our Ultracomfortable sun protection is rated to UPF 50+, helping you stay cool and dry. Originally designed for fishing, this lightest hot-weather shirt offers UPF 50+ coverage and is great for extended travel. SunSmart technology blocks 98% of the sun's harmful UV rays, while the high-performance fabric is wrinkle-free and quickly evaporates perspiration. Made with 52% polyester and 48% nylon, this shirt is machine washable and dryable. Additional features include front and back cape venting, two front bellows pockets and an imported design. With UPF 50+ coverage, you can limit sun exposure and feel secure with the highest rated sun protection available.\")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_result[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3ca00951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encadenar todas las respuestas en un string\n",
    "qdocs = \"\".join([docs_result[i].page_content for i in range(len(docs_result))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "87d5ca60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "': 255\\nname: Sun Shield Shirt by\\ndescription: \"Block the sun, not the fun – our high-performance sun shirt is guaranteed to protect from harmful UV rays. \\n\\nSize & Fit: Slightly Fitted: Softly shapes the body. Falls at hip.\\n\\nFabric & Care: 78% nylon, 22% Lycra Xtra Life fiber. UPF 50+ rated – the highest rated sun protection possible. Handwash, line dry.\\n\\nAdditional Features: Wicks moisture for quick-drying comfort. Fits comfortably over your favorite swimsuit. Abrasion resistant for season after season of wear. Imported.\\n\\nSun Protection That Won\\'t Wear Off\\nOur high-performance fabric provides SPF 50+ sun protection, blocking 98% of the sun\\'s harmful rays. This fabric is recommended by The Skin Cancer Foundation as an effective UV protectant.: 374\\nname: Men\\'s Plaid Tropic Shirt, Short-Sleeve\\ndescription: Our Ultracomfortable sun protection is rated to UPF 50+, helping you stay cool and dry. Originally designed for fishing, this lightest hot-weather shirt offers UPF 50+ coverage and is great for extended travel. SunSmart technology blocks 98% of the sun\\'s harmful UV rays, while the high-performance fabric is wrinkle-free and quickly evaporates perspiration. Made with 52% polyester and 48% nylon, this shirt is machine washable and dryable. Additional features include front and back cape venting, two front bellows pockets and an imported design. With UPF 50+ coverage, you can limit sun exposure and feel secure with the highest rated sun protection available.: 535\\nname: Men\\'s TropicVibe Shirt, Short-Sleeve\\ndescription: This Men’s sun-protection shirt with built-in UPF 50+ has the lightweight feel you want and the coverage you need when the air is hot and the UV rays are strong. Size & Fit: Traditional Fit: Relaxed through the chest, sleeve and waist. Fabric & Care: Shell: 71% Nylon, 29% Polyester. Lining: 100% Polyester knit mesh. UPF 50+ rated – the highest rated sun protection possible. Machine wash and dry. Additional Features: Wrinkle resistant. Front and back cape venting lets in cool breezes. Two front bellows pockets. Imported.\\n\\nSun Protection That Won\\'t Wear Off: Our high-performance fabric provides SPF 50+ sun protection, blocking 98% of the sun\\'s harmful rays.: 618\\nname: Men\\'s Tropical Plaid Short-Sleeve Shirt\\ndescription: Our lightest hot-weather shirt is rated UPF 50+ for superior protection from the sun\\'s UV rays. With a traditional fit that is relaxed through the chest, sleeve, and waist, this fabric is made of 100% polyester and is wrinkle-resistant. With front and back cape venting that lets in cool breezes and two front bellows pockets, this shirt is imported and provides the highest rated sun protection possible. \\n\\nSun Protection That Won\\'t Wear Off. Our high-performance fabric provides SPF 50+ sun protection, blocking 98% of the sun\\'s harmful rays.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "306f4b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancia LLM\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.0,\n",
    "    model=\"gpt-3.5-turbo\"\n",
    "    )\n",
    "\n",
    "# Prompt\n",
    "prompt = f\"\"\"\n",
    "I will give you a list of product entries. Each one starts with 'name:' and includes a 'description:'.\n",
    "Extract all shirts with sun protection (UPF/UV) and return a markdown table with:\n",
    "\n",
    "| Name | Summary of Sun Protection Features |\n",
    "\n",
    "Here are the entries:\n",
    "\n",
    "{qdocs}\n",
    "\"\"\"\n",
    "\n",
    "# Invocar al LLM con el prompt\n",
    "response = llm.invoke(prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "76cf1652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"| Name | Summary of Sun Protection Features |\\n| --- | --- |\\n| Sun Shield Shirt | UPF 50+ rated sun protection, blocks 98% of harmful UV rays |\\n| Men's Plaid Tropic Shirt | UPF 50+ rated sun protection, blocks 98% of harmful UV rays |\\n| Men's TropicVibe Shirt | UPF 50+ rated sun protection, blocks 98% of harmful UV rays |\\n| Men's Tropical Plaid Short-Sleeve Shirt | UPF 50+ rated sun protection, blocks 98% of harmful UV rays |\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2d0ae057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Name | Summary of Sun Protection Features |\n",
       "| --- | --- |\n",
       "| Sun Shield Shirt | UPF 50+ rated sun protection, blocks 98% of harmful UV rays |\n",
       "| Men's Plaid Tropic Shirt | UPF 50+ rated sun protection, blocks 98% of harmful UV rays |\n",
       "| Men's TropicVibe Shirt | UPF 50+ rated sun protection, blocks 98% of harmful UV rays |\n",
       "| Men's Tropical Plaid Short-Sleeve Shirt | UPF 50+ rated sun protection, blocks 98% of harmful UV rays |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "20d19cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir VectorStore en un retriever compatible con LangChain\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# RAG\n",
    "qa_stuff = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "query =  \"\"\"\n",
    "I will give you a list of product entries. Each one starts with 'name:' and includes a 'description:'.\n",
    "Extract all shirts with sun protection (UPF/UV) and return a markdown table with:\n",
    "\n",
    "| Name | Summary of Sun Protection Features |\n",
    "\"\"\"\n",
    "\n",
    "response = qa_stuff.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9b75dd6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Name | Summary of Sun Protection Features |\n",
       "| --- | --- |\n",
       "| Sun Shield Shirt by | SPF 50+ sun protection, blocks 98% of harmful rays |\n",
       "| Men's Tropical Plaid Short-Sleeve Shirt | SPF 50+ sun protection, blocks 98% of harmful rays |\n",
       "| Men's TropicVibe Shirt, Short-Sleeve | SPF 50+ sun protection, blocks 98% of harmful rays |\n",
       "| Men's Plaid Tropic Shirt, Short-Sleeve | UPF 50+ sun protection, blocks 98% of harmful rays |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ad8166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999dd232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a23d93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c58552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb513928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb72225c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exploring-langchain",
   "language": "python",
   "name": "exploring-langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

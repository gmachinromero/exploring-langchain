{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ae230d7-2ab1-469e-a6a0-238293c1eeb1",
   "metadata": {},
   "source": [
    "# 0. Librerías y variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d99484c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d680460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY\n",
      "PROXYCURL_API_KEY\n",
      "TAVILY_API_KEY\n",
      "LANGCHAIN_TRACING_V2\n",
      "LANGCHAIN_ENDPOINT\n",
      "LANGCHAIN_API_KEY\n",
      "LANGCHAIN_PROJECT\n"
     ]
    }
   ],
   "source": [
    "env_vars = dotenv_values()\n",
    "for key in env_vars.keys():\n",
    "    print(key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b042c85b-397e-45fd-92ff-3177d182c1cb",
   "metadata": {},
   "source": [
    "# 1. OpenAI: Chat API"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0a5d7ff",
   "metadata": {},
   "source": [
    "En este apartado se llama a la API de OpenAI directamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4a9fac69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f09214c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-BWkfhRmicb3QmFwbJdVQAdrleJCzv', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='1+1 es igual a 2.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1747145601, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=10, prompt_tokens=17, total_tokens=27, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inicializar el cliente\n",
    "client = OpenAI()\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"¿Cuánto es 1+1?\"}]\n",
    "    \n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=messages,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a6e4a541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1+1 es igual a 2.'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50b5a3cf",
   "metadata": {},
   "source": [
    "Con f-strings, se pueden crear prompts modulares, que permitan variar el valor de las variables para variar el resultado de forma sencilla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9e0d6b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para parsear la respuesta\n",
    "def get_completion(prompt):\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "45e2d5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traduce el siguiente texto al Granaíno: ¿De dónde vienes?\n"
     ]
    }
   ],
   "source": [
    "language = \"Granaíno\"\n",
    "text = \"¿De dónde vienes?\"\n",
    "\n",
    "prompt = f\"Traduce el siguiente texto al {language}: {text}\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "4733f3c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¿Ande vienes, chiquillo?'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88c74f4a",
   "metadata": {},
   "source": [
    "# 2. LangChain: Chat API"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c809ced",
   "metadata": {},
   "source": [
    "En este apartado se va a replicar lo se ha hecho en el punto anterior, pero desde la API de LangChain. Adicionalemnte se va ahondar en 3 conceptos de LangChain:\n",
    "- Modelos\n",
    "- Prompts\n",
    "- OutputParsers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3eeca3d6",
   "metadata": {},
   "source": [
    "LangChain permite llamar a diferentes modelos, con diferentes APIs, de forma agnóstica. Cmabiando ligeramente el código, puedo aprovechar una estructura ya creada para apuntar a otro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a22bf983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "061259f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear los mensajes en el formato de LangChain\n",
    "messages = [\n",
    "    HumanMessage(content=\"¿Cuánto es 1+1?\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "db336180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La respuesta a la pregunta de \"¿Cuánto es 1+1?\" es 2.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOllama(\n",
    "    model=\"llama3.1\",\n",
    "    temperature=0.0\n",
    "    )\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "b140df4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1+1 es igual a 2.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\", \n",
    "    temperature=0.0\n",
    "    )\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cbb162aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "# model = init_chat_model(\"claude-3-5-sonnet-latest\", model_provider=\"anthropic\")\n",
    "# model = init_chat_model(\"mistral-large-latest\", model_provider=\"mistralai\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7a67aee",
   "metadata": {},
   "source": [
    "Las Prompt Templates (plantillas de prompt) son una herramienta de LangChain que te ayuda a crear automáticamente los mensajes que le envías al modelo de lenguaje, de una forma organizada, flexible y reutilizable.\n",
    "\n",
    "Cuando trabajas con modelos como GPT, no sueles enviar directamente el texto del usuario al modelo. Normalmente quieres hacer algo más, como:\n",
    "\n",
    "- Agregar instrucciones específicas (por ejemplo: \"Traduce al francés...\")\n",
    "- Dar un contexto adicional al modelo\n",
    "- Formatear el texto de forma especial\n",
    "- Usar el mismo formato muchas veces con diferentes datos\n",
    "\n",
    "Ahí es donde entran las prompt templates.\n",
    "\n",
    "Piensa en ellas como plantillas con huecos (como los de un formulario). Tú defines una estructura fija y dejas espacios para completar con datos reales más tarde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "cd77b09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"Traduce el siguiente texto al {language}: {text}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f88974e3",
   "metadata": {},
   "source": [
    "LangChain te permite definir esta plantilla y luego rellenarla automáticamente con los valores que quieras:\n",
    "- `language = \"francés\"`\n",
    "- `text = \"Hola, ¿cómo estás?\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "74d2b4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "13abd576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para casos sencillos\n",
    "template_string = \"Traduce el siguiente texto al {language}: {text}\"\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "27e3d06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para casos mas complejos\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un trabajador del campo, oriundo de la alpujarra de Granada.\"),\n",
    "    (\"human\", \"Traduce esto al {language}: {text}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "8528252a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['language', 'text'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Eres un trabajador del campo, oriundo de la alpujarra de Granada.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language', 'text'], input_types={}, partial_variables={}, template='Traduce esto al {language}: {text}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3a30e830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Eres un trabajador del campo, oriundo de la alpujarra de Granada.'), additional_kwargs={})"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "67a5b66d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language', 'text'], input_types={}, partial_variables={}, template='Traduce esto al {language}: {text}'), additional_kwargs={})"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "9c124e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['language', 'text']"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[1].prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "1ce1c912",
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"Granaíno\"\n",
    "text = \"¿De dónde vienes?\"\n",
    "\n",
    "translation_prompt = prompt_template.format_messages(\n",
    "    language=language,\n",
    "    text=text\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "4f363829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Eres un trabajador del campo, oriundo de la alpujarra de Granada.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Traduce esto al Granaíno: ¿De dónde vienes?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "9bf2683b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'langchain_core.messages.system.SystemMessage'>\n",
      "<class 'langchain_core.messages.human.HumanMessage'>\n"
     ]
    }
   ],
   "source": [
    "print(type(translation_prompt))\n",
    "print(type(translation_prompt[0]))\n",
    "print(type(translation_prompt[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "721168c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¿Ande vienes, majo?'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_response = llm.invoke(translation_prompt)\n",
    "\n",
    "customer_response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "10fdba7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¿De do vienes?'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un académico de la RAE Española.\"),\n",
    "    (\"human\", \"Traduce esto al {language}: {text}\")\n",
    "])\n",
    "\n",
    "language = \"Castellano antiguo\"\n",
    "text = \"¿De dónde vienes?\"\n",
    "\n",
    "translation_prompt = prompt_template.format_messages(\n",
    "    language=language,\n",
    "    text=text\n",
    "    )\n",
    "\n",
    "customer_response = llm.invoke(translation_prompt)\n",
    "\n",
    "customer_response.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7887270a",
   "metadata": {},
   "source": [
    "Al implementar la llamada desde LangChain, evitamos depender de la API propia de un modelo como los de OpenAI, a utilizar la API de LangChain que es similar para todos los modelos.\n",
    "\n",
    "Además LangChain incopora otras funciones como los OutputParsers, que mejoran la gestión de respuestas por parte de los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "526e9ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': False, 'delivery_days': 5, 'price_value': 'pretty affordable!'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    \"gift\": False,\n",
    "    \"delivery_days\": 5,\n",
    "    \"price_value\": \"pretty affordable!\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "53c9ce45",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review = \"\"\"\\\n",
    "This leaf blower is pretty amazing.  It has four settings:\\\n",
    "candle blower, gentle breeze, windy city, and tornado. \\\n",
    "It arrived in two days, just in time for my wife's \\\n",
    "anniversary present. \\\n",
    "I think my wife liked it so much she was speechless. \\\n",
    "So far I've been the only one using it, and I've been \\\n",
    "using it every other morning to clear the leaves on our lawn. \\\n",
    "It's slightly more expensive than the other leaf blowers \\\n",
    "out there, but I think it's worth it for the extra features.\n",
    "\"\"\"\n",
    "\n",
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product \\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "gift\n",
    "delivery_days\n",
    "price_value\n",
    "\n",
    "text: {text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6fcaf224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['text'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='For the following text, extract the following information:\\n\\ngift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\\n\\ndelivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\\n\\nprice_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\ngift\\ndelivery_days\\nprice_value\\n\\ntext: {text}\\n'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "497a6778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c7b68e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"gift\": true,\n",
      "    \"delivery_days\": 2,\n",
      "    \"price_value\": \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "messages = prompt_template.format_messages(text=customer_review)\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\", \n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "57b33cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "11ca7f06",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# You will get an error by running this line of code \u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# because'gift' is not a dictionary\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# 'gift' is a string\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m(\u001b[33m'\u001b[39m\u001b[33mgift\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "# You will get an error by running this line of code \n",
    "# because'gift' is not a dictionary\n",
    "# 'gift' is a string\n",
    "response.content.get('gift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dcbe8c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "94e7a407",
   "metadata": {},
   "outputs": [],
   "source": [
    "gift_schema = ResponseSchema(\n",
    "    name=\"gift\",\n",
    "    description=\"Was the item purchased\\\n",
    "    as a gift for someone else? \\\n",
    "    Answer True if yes,\\\n",
    "    False if not or unknown.\"\n",
    "    )\n",
    "\n",
    "delivery_days_schema = ResponseSchema(\n",
    "    name=\"delivery_days\",\n",
    "    description=\"How many days\\\n",
    "    did it take for the product\\\n",
    "    to arrive? If this \\\n",
    "    information is not found,\\\n",
    "    output -1.\"\n",
    "    )\n",
    "\n",
    "price_value_schema = ResponseSchema(\n",
    "    name=\"price_value\",\n",
    "    description=\"Extract any\\\n",
    "    sentences about the value or \\\n",
    "    price, and output them as a \\\n",
    "    comma separated Python list.\"\n",
    "    )\n",
    "\n",
    "response_schemas = [\n",
    "    gift_schema, \n",
    "    delivery_days_schema,\n",
    "    price_value_schema\n",
    "    ]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "29e2a373",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0a9a7945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // Was the item purchased    as a gift for someone else?     Answer True if yes,    False if not or unknown.\n",
      "\t\"delivery_days\": string  // How many days    did it take for the product    to arrive? If this     information is not found,    output -1.\n",
      "\t\"price_value\": string  // Extract any    sentences about the value or     price, and output them as a     comma separated Python list.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "da5d6f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_template_2 = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product\\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
    "\n",
    "messages = prompt.format_messages(\n",
    "    text=customer_review, \n",
    "    format_instructions=format_instructions\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "24370c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the following text, extract the following information:\n",
      "\n",
      "gift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
      "\n",
      "delivery_days: How many days did it take for the productto arrive? If this information is not found, output -1.\n",
      "\n",
      "price_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\n",
      "\n",
      "text: This leaf blower is pretty amazing.  It has four settings:candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversary present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\n",
      "\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // Was the item purchased    as a gift for someone else?     Answer True if yes,    False if not or unknown.\n",
      "\t\"delivery_days\": string  // How many days    did it take for the product    to arrive? If this     information is not found,    output -1.\n",
      "\t\"price_value\": string  // Extract any    sentences about the value or     price, and output them as a     comma separated Python list.\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1900a02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8b786b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"gift\": true,\n",
      "\t\"delivery_days\": 2,\n",
      "\t\"price_value\": [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "845839ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = output_parser.parse(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b5b87d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': True,\n",
       " 'delivery_days': 2,\n",
       " 'price_value': [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5269c248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "575c5cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict.get('delivery_days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e772a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07a436c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fee3b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26807d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9ffdf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f154ff8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fea4a58",
   "metadata": {},
   "source": [
    "LangChain:\n",
    "- Model\n",
    "- Prompts\n",
    "- Output Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c6c0b3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exploring-langchain",
   "language": "python",
   "name": "exploring-langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

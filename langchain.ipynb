{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ae230d7-2ab1-469e-a6a0-238293c1eeb1",
   "metadata": {},
   "source": [
    "# 0 - Librerías y variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d99484c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Librerías\n",
    "# ------------------------------------------------------------------------------\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d680460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY\n",
      "PROXYCURL_API_KEY\n",
      "TAVILY_API_KEY\n",
      "LANGCHAIN_TRACING_V2\n",
      "LANGCHAIN_ENDPOINT\n",
      "LANGCHAIN_API_KEY\n",
      "LANGCHAIN_PROJECT\n"
     ]
    }
   ],
   "source": [
    "# Variables\n",
    "# ------------------------------------------------------------------------------\n",
    "env_vars = dotenv_values()\n",
    "for key in env_vars.keys():\n",
    "    print(key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b042c85b-397e-45fd-92ff-3177d182c1cb",
   "metadata": {},
   "source": [
    "# 1 - ChatLLM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0a5d7ff",
   "metadata": {},
   "source": [
    "En este apartado se llama a la API de OpenAI directamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4a9fac69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f09214c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-BWkfhRmicb3QmFwbJdVQAdrleJCzv', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='1+1 es igual a 2.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1747145601, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=10, prompt_tokens=17, total_tokens=27, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inicializar el cliente\n",
    "client = OpenAI()\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"¿Cuánto es 1+1?\"}]\n",
    "    \n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=messages,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f6a6be29",
   "metadata": {},
   "source": [
    "Si quiero modificar el código por ejemplo para utilizar un modelo LLaMA 3.1 en Ollama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9887075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La respuesta a la pregunta es 2.\n"
     ]
    }
   ],
   "source": [
    "# Define el endpoint local de Ollama\n",
    "OLLAMA_URL = \"http://localhost:11434/api/chat\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"¿Cuánto es 1+1?\"}]\n",
    "\n",
    "response = requests.post(OLLAMA_URL, json={\n",
    "    \"model\": \"llama3.1\",\n",
    "    \"messages\": messages,\n",
    "    \"temperature\": 0.0,\n",
    "    \"stream\": False\n",
    "})\n",
    "\n",
    "data = response.json()\n",
    "print(data[\"message\"][\"content\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e004c686",
   "metadata": {},
   "source": [
    "Cada vez que haya un cambio de LLM, tendría que haber un cambio de código."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88c74f4a",
   "metadata": {},
   "source": [
    "# 2 - LangChain: Chat API"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c809ced",
   "metadata": {},
   "source": [
    "En este apartado se va a replicar lo se ha hecho en el punto anterior, pero desde la API de LangChain. Adicionalemnte se va ahondar en los siguientes conceptos de LangChain y sus ventajas:\n",
    "- Modelos\n",
    "- Prompts Templates\n",
    "- Output Parsers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa5c9eb1",
   "metadata": {},
   "source": [
    "## 2.1. - Modelos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3eeca3d6",
   "metadata": {},
   "source": [
    "En LangChain, los models (modelos) son los componentes que generan texto o responden a mensajes. Son la parte que realmente interactúa con modelos de lenguaje (LLMs) como los de OpenAI, Anthropic, Cohere, etc. LangChain organiza los modelos según lo que hacen. Los más comunes son:\n",
    "\n",
    "| Tipo de modelo     | Qué hace                                                                    | Clase típica                                |\n",
    "| ------------------ | --------------------------------------------------------------------------- | ------------------------------------------- |\n",
    "| **LLM**            | Genera texto a partir de un *prompt plano*                                  | OpenAI, HuggingFaceHub                      |\n",
    "| **ChatModel**      | Maneja conversaciones con roles (usuario, asistente, sistema)               | ChatOpenAI, ChatAnthropic                   |\n",
    "| **EmbeddingModel** | Convierte texto en vectores (útiles para búsquedas o comparación semántica) | OpenAIEmbeddings, HuggingFaceEmbeddings     |\n",
    "\n",
    "\n",
    "LangChain permite llamar a diferentes modelos, con diferentes APIs, de forma agnóstica. Cambiando ligeramente el código, puedo aprovechar una estructura ya creada para apuntar a otro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6287b970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.25\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a22bf983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "061259f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear los mensajes en el formato de LangChain\n",
    "messages = [\n",
    "    HumanMessage(content=\"¿Cuánto es 1+1?\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "db336180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La respuesta a la pregunta de \"¿Cuánto es 1+1?\" es 2.\n"
     ]
    }
   ],
   "source": [
    "# Llamar a un modelo A\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1\",\n",
    "    temperature=0.0\n",
    "    )\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b140df4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1+1 es igual a 2.\n"
     ]
    }
   ],
   "source": [
    "# Llamar a un modelo B\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\", \n",
    "    temperature=0.0\n",
    "    )\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2c85cbe",
   "metadata": {},
   "source": [
    "La estructura es exactamente la misma, se podría incluso encapsular el codigo en una función y pasar el modelo como parámetro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04321608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andrew Ng es un científico de datos, empresario e investigador en inteligencia artificial. Es conocido por ser uno de los pioneros en el campo del aprendizaje profundo y por su trabajo en el desarrollo de algoritmos de aprendizaje automático. Ha sido profesor en la Universidad de Stanford, cofundador de Google Brain y Baidu AI Group, y fundador de la plataforma de educación en línea Coursera. También es cofundador y presidente de Landing AI, una empresa que ayuda a las empresas a implementar soluciones de inteligencia artificial.\n"
     ]
    }
   ],
   "source": [
    "def call_llm_model(\n",
    "    model_name: str,\n",
    "    temperature: float,\n",
    "    message: str\n",
    ") -> str:\n",
    "\n",
    "    llm = ChatOpenAI(\n",
    "        model_name=model_name,\n",
    "        temperature=temperature\n",
    "    )\n",
    "\n",
    "    response = llm.invoke([HumanMessage(content=message)])\n",
    "\n",
    "    return response.content\n",
    "\n",
    "\n",
    "respuesta = call_llm_model(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0.2,\n",
    "    message=\"¿Quién es Andrew Ng?\"\n",
    ")\n",
    "\n",
    "print(respuesta)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d28abe4",
   "metadata": {},
   "source": [
    "En las últimas versiones de LangChain, se ha creado una abstracción para inicilizar modelos de diferentes proveedores desde la misma función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cbb162aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "# model = init_chat_model(\"claude-3-5-sonnet-latest\", model_provider=\"anthropic\")\n",
    "# model = init_chat_model(\"mistral-large-latest\", model_provider=\"mistralai\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a9b0129",
   "metadata": {},
   "source": [
    "## 2.2. - Prompt Templates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7a67aee",
   "metadata": {},
   "source": [
    "Las Prompt Templates (plantillas de prompt) son una herramienta de LangChain que te ayuda a crear automáticamente los mensajes que le envías al modelo de lenguaje, de una forma organizada, flexible y reutilizable.\n",
    "\n",
    "Cuando trabajas con modelos como GPT, no sueles enviar directamente el texto del usuario al modelo. Normalmente quieres hacer algo más, como:\n",
    "\n",
    "- Agregar instrucciones específicas (por ejemplo: \"Traduce al francés...\")\n",
    "- Dar un contexto adicional al modelo\n",
    "- Formatear el texto de forma especial\n",
    "- Usar el mismo formato muchas veces con diferentes datos\n",
    "\n",
    "Ahí es donde entran las prompt templates.\n",
    "\n",
    "Piensa en ellas como plantillas con huecos (como los de un formulario). Tú defines una estructura fija y dejas espacios para completar con datos reales más tarde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd77b09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"Traduce el siguiente texto al {language}: {text}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f88974e3",
   "metadata": {},
   "source": [
    "LangChain te permite definir esta plantilla y luego rellenarla automáticamente con los valores que quieras:\n",
    "- `language = \"francés\"`\n",
    "- `text = \"Hola, ¿cómo estás?\"`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68687fc0",
   "metadata": {},
   "source": [
    "los principales templates y su propósito son:\n",
    "\n",
    "| Prompt Template                    | ¿Para qué sirve?                                                                                                                                   |\n",
    "| ---------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `ChatPromptTemplate`               | Plantilla para **modelos de chat**. Permite combinar mensajes (`System`, `Human`, etc.) fácilmente. Ideal para `ChatOpenAI`, `ChatAnthropic`, etc. |\n",
    "| `PromptTemplate`                   | Plantilla simple de texto plano. Usado con modelos de lenguaje **no conversacionales** como `OpenAI(model=\"text-davinci-003\")`.                    |\n",
    "| `SystemMessagePromptTemplate`      | Subplantilla usada dentro de `ChatPromptTemplate` para definir el **mensaje del sistema**.                                                         |\n",
    "| `HumanMessagePromptTemplate`       | Subplantilla usada dentro de `ChatPromptTemplate` para el **mensaje del usuario**                                                                  |\n",
    "| `MessagesPlaceholder`              | Placeholder especial dentro de `ChatPromptTemplate` para insertar una lista dinámica de mensajes (ej: historial de conversación).                  |\n",
    "| `AIMessagePromptTemplate`          | Subplantilla para simular un mensaje **anterior del asistente** en el historial.                                                                   |\n",
    "| `FewShotPromptTemplate`            | Plantilla para tareas de **few-shot learning**. Permite definir ejemplos que se combinan con el input.                                             |\n",
    "| `ChatMessagePromptTemplate`        | Versión más flexible que permite definir mensajes de un rol personalizado (ej: `\"role\": \"function\"`).                                              |\n",
    "| `FewShotChatMessagePromptTemplate` | Lo mismo que `FewShotPromptTemplate`, pero adaptado para `ChatPromptTemplate`. Útil si quieres ejemplos en formato chat.                           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "74d2b4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15e8084e",
   "metadata": {},
   "source": [
    "\n",
    "| Método            | ¿Qué construye?                             | ¿Cuándo usarlo?                                 |\n",
    "| ----------------- | ------------------------------------------- | ----------------------------------------------- |\n",
    "| `from_template()` | Una sola plantilla de mensaje               | Para casos simples o rápidos                    |\n",
    "| `from_messages()` | Una conversación entera con múltiples roles | Cuando necesitas dar contexto o varios mensajes |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76a4a5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plantilla de traducción\n",
    "template = \"Traduce el siguiente texto al {language}: {text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "13abd576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para casos sencillos\n",
    "template_string = \"Traduce el siguiente texto al {language}: {text}\"\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "27e3d06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para casos mas complejos: sistema, usuarios, asistente...etc\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un trabajador del campo, oriundo de la alpujarra de Granada.\"),\n",
    "    (\"human\", \"Traduce esto al {language}: {text}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8528252a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['language', 'text'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Eres un trabajador del campo, oriundo de la alpujarra de Granada.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language', 'text'], input_types={}, partial_variables={}, template='Traduce esto al {language}: {text}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3a30e830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Eres un trabajador del campo, oriundo de la alpujarra de Granada.'), additional_kwargs={})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "67a5b66d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language', 'text'], input_types={}, partial_variables={}, template='Traduce esto al {language}: {text}'), additional_kwargs={})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9c124e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['language', 'text']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[1].prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1ce1c912",
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"Granaíno\"\n",
    "text = \"¿De dónde vienes?\"\n",
    "\n",
    "translation_prompt = prompt_template.format_messages(\n",
    "    language=language,\n",
    "    text=text\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4f363829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Eres un trabajador del campo, oriundo de la alpujarra de Granada.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Traduce esto al Granaíno: ¿De dónde vienes?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9bf2683b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'langchain_core.messages.system.SystemMessage'>\n",
      "<class 'langchain_core.messages.human.HumanMessage'>\n"
     ]
    }
   ],
   "source": [
    "print(type(translation_prompt))\n",
    "print(type(translation_prompt[0]))\n",
    "print(type(translation_prompt[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "721168c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¿Ande vienes, chavá?'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator_response = llm.invoke(translation_prompt)\n",
    "\n",
    "translator_response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "10fdba7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¿De do vienes?'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un noble de la realeza española del siglo XV.\"),\n",
    "    (\"human\", \"Traduce esto al {language}: {text}\")\n",
    "])\n",
    "\n",
    "language = \"Castellano antiguo\"\n",
    "text = \"¿De dónde vienes?\"\n",
    "\n",
    "translation_prompt = prompt_template.format_messages(\n",
    "    language=language,\n",
    "    text=text\n",
    "    )\n",
    "\n",
    "customer_response = llm.invoke(translation_prompt)\n",
    "\n",
    "customer_response.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07abd8fe",
   "metadata": {},
   "source": [
    "## 2.3. - Output Parsers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58faea88",
   "metadata": {},
   "source": [
    "En LangChain, los output parsers son herramientas clave para procesar y estructurar las respuestas que se obtienen de los modelos de lenguaje antes de ser utilizadas en otros pasos del flujo de trabajo. Estos parsers te permiten transformar la salida en bruto de los modelos en un formato más adecuado para tu aplicación, ya sea en forma de texto, datos estructurados o incluso en la ejecución de funciones específicas.\n",
    "\n",
    "A continuación, una tabla con los output parsers más comunes en LangChain y su uso principal:\n",
    "\n",
    "| **Output Parser**        | **¿Para qué sirve?**                                                                                                                                                 |\n",
    "| ------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `JsonOutputParser`       | Convierte la salida del modelo en un objeto JSON (estructura de diccionario o lista). Ideal cuando esperas respuestas estructuradas como JSON o diccionarios.        |\n",
    "| `RegexOutputParser`      | Extrae información utilizando expresiones regulares. Se usa cuando la respuesta del modelo sigue un patrón específico que se puede identificar con regex.            |\n",
    "| `StructuredOutputParser` | Permite parsear la salida en estructuras de datos más complejas. Ideal para cuando necesitas que el modelo devuelva datos tabulares o jerárquicos.                   |\n",
    "| `VariableParser`         | Analiza las respuestas del modelo en busca de valores de variables específicas. Utilizado cuando deseas extraer datos de las respuestas para usarlos en otros pasos. |\n",
    "| `TextOutputParser`       | Convierte la salida en un texto plano procesable. Ideal cuando la salida del modelo es simplemente texto sin estructura.                                             |\n",
    "| `SQLOutputParser`        | Convierte la salida del modelo en consultas SQL estructuradas. Es útil si el modelo está generando consultas a bases de datos.                                       |\n",
    "| `PythonOutputParser`     | Convierte la salida del modelo en código Python ejecutable. Utilizado cuando necesitas generar código a partir de la respuesta del modelo.                           |\n",
    "| `ActionOutputParser`     | Permite que la salida sea convertida en una acción específica o una ejecución de función. Ideal para flujos de trabajo más dinámicos donde se deben ejecutar tareas. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "526e9ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': False, 'delivery_days': 5, 'price_value': 'muy asequible!'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    \"gift\": False,\n",
    "    \"delivery_days\": 5,\n",
    "    \"price_value\": \"muy asequible!\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "53c9ce45",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review = \"\"\"\\\n",
    "Este soplador de hojas es bastante increíble. Tiene cuatro configuraciones:\\\n",
    "soplador de vela, brisa suave, ciudad ventosa y tornado. \\\n",
    "Llegó en dos días, justo a tiempo para el regalo de aniversario de mi esposa. \\\n",
    "Creo que a mi esposa le gustó tanto que se quedó sin palabras. \\\n",
    "Hasta ahora he sido el único que lo ha usado, y lo he estado usando cada dos mañanas para limpiar las hojas de nuestro césped. \\\n",
    "Es ligeramente más caro que los otros sopladores de hojas que hay por ahí, pero creo que vale la pena por las características adicionales.\n",
    "\"\"\"\n",
    "\n",
    "review_template = \"\"\"\\\n",
    "Para el siguiente texto, extrae la siguiente información:\n",
    "\n",
    "gift: ¿Fue el artículo comprado como un regalo para otra persona? \\\n",
    "Responde True si es sí, False si no o si no se sabe.\n",
    "\n",
    "delivery_days: ¿Cuántos días tardó en llegar el producto? \\\n",
    "Si esta información no se encuentra, devuelve -1.\n",
    "\n",
    "price_value: Extrae cualquier frase sobre el valor o precio,\\\n",
    "y devuélvelas como una lista de Python separada por comas.\n",
    "\n",
    "Formatea la salida como JSON con las siguientes claves:\n",
    "gift\n",
    "delivery_days\n",
    "price_value\n",
    "\n",
    "texto: {text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6fcaf224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['text'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='Para el siguiente texto, extrae la siguiente información:\\n\\ngift: ¿Fue el artículo comprado como un regalo para otra persona? Responde True si es sí, False si no o si no se sabe.\\n\\ndelivery_days: ¿Cuántos días tardó en llegar el producto? Si esta información no se encuentra, devuelve -1.\\n\\nprice_value: Extrae cualquier frase sobre el valor o precio,y devuélvelas como una lista de Python separada por comas.\\n\\nFormatea la salida como JSON con las siguientes claves:\\ngift\\ndelivery_days\\nprice_value\\n\\ntexto: {text}\\n'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "497a6778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c7b68e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"gift\": true,\n",
      "    \"delivery_days\": 2,\n",
      "    \"price_value\": [\"ligeramente más caro\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "messages = prompt_template.format_messages(text=customer_review)\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "57b33cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "11ca7f06",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Si intentamos acceder a response como si fuera un diccionario...\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m(\u001b[33m'\u001b[39m\u001b[33mgift\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "# Si intentamos acceder a response como si fuera un diccionario...\n",
    "response.content.get('gift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dcbe8c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "94e7a407",
   "metadata": {},
   "outputs": [],
   "source": [
    "gift_schema = ResponseSchema(\n",
    "    name=\"gift\",\n",
    "    description=\"Was the item purchased\\\n",
    "    as a gift for someone else? \\\n",
    "    Answer True if yes,\\\n",
    "    False if not or unknown.\"\n",
    "    )\n",
    "\n",
    "delivery_days_schema = ResponseSchema(\n",
    "    name=\"delivery_days\",\n",
    "    description=\"How many days\\\n",
    "    did it take for the product\\\n",
    "    to arrive? If this \\\n",
    "    information is not found,\\\n",
    "    output -1.\"\n",
    "    )\n",
    "\n",
    "price_value_schema = ResponseSchema(\n",
    "    name=\"price_value\",\n",
    "    description=\"Extract any\\\n",
    "    sentences about the value or \\\n",
    "    price, and output them as a \\\n",
    "    comma separated Python list.\"\n",
    "    )\n",
    "\n",
    "response_schemas = [\n",
    "    gift_schema, \n",
    "    delivery_days_schema,\n",
    "    price_value_schema\n",
    "    ]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "29e2a373",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0a9a7945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // Was the item purchased    as a gift for someone else?     Answer True if yes,    False if not or unknown.\n",
      "\t\"delivery_days\": string  // How many days    did it take for the product    to arrive? If this     information is not found,    output -1.\n",
      "\t\"price_value\": string  // Extract any    sentences about the value or     price, and output them as a     comma separated Python list.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "da5d6f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_template_2 = \"\"\"\\\n",
    "Para el siguiente texto, extrae la siguiente información:\n",
    "\n",
    "gift: ¿Fue el artículo comprado como un regalo para otra persona? \\\n",
    "Responde True si es sí, False si no o si no se sabe.\n",
    "\n",
    "delivery_days: ¿Cuántos días tardó en llegar el producto? \\\n",
    "Si esta información no se encuentra, devuelve -1.\n",
    "\n",
    "price_value: Extrae cualquier frase sobre el valor o el precio, \\\n",
    "y devuélvelas como una lista de Python separada por comas.\n",
    "\n",
    "texto: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
    "\n",
    "messages = prompt.format_messages(\n",
    "    text=customer_review, \n",
    "    format_instructions=format_instructions\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "24370c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para el siguiente texto, extrae la siguiente información:\n",
      "\n",
      "gift: ¿Fue el artículo comprado como un regalo para otra persona? Responde True si es sí, False si no o si no se sabe.\n",
      "\n",
      "delivery_days: ¿Cuántos días tardó en llegar el producto? Si esta información no se encuentra, devuelve -1.\n",
      "\n",
      "price_value: Extrae cualquier frase sobre el valor o el precio, y devuélvelas como una lista de Python separada por comas.\n",
      "\n",
      "texto: Este soplador de hojas es bastante increíble. Tiene cuatro configuraciones:soplador de vela, brisa suave, ciudad ventosa y tornado. Llegó en dos días, justo a tiempo para el regalo de aniversario de mi esposa. Creo que a mi esposa le gustó tanto que se quedó sin palabras. Hasta ahora he sido el único que lo ha usado, y lo he estado usando cada dos mañanas para limpiar las hojas de nuestro césped. Es ligeramente más caro que los otros sopladores de hojas que hay por ahí, pero creo que vale la pena por las características adicionales.\n",
      "\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // Was the item purchased    as a gift for someone else?     Answer True if yes,    False if not or unknown.\n",
      "\t\"delivery_days\": string  // How many days    did it take for the product    to arrive? If this     information is not found,    output -1.\n",
      "\t\"price_value\": string  // Extract any    sentences about the value or     price, and output them as a     comma separated Python list.\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1900a02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"gift\": True,\n",
      "\t\"delivery_days\": 2,\n",
      "\t\"price_value\": [\"Es ligeramente más caro que los otros sopladores de hojas que hay por ahí, pero creo que vale la pena por las características adicionales\"]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "845839ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'OutputParserException' from 'langchain.output_parsers' (/home/gmachin/.local/share/virtualenvs/exploring-langchain-UOzDUSui/lib/python3.11/site-packages/langchain/output_parsers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[79]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moutput_parsers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OutputParserException\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m      4\u001b[39m     output_dict = output_parser.parse(response.content)\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'OutputParserException' from 'langchain.output_parsers' (/home/gmachin/.local/share/virtualenvs/exploring-langchain-UOzDUSui/lib/python3.11/site-packages/langchain/output_parsers/__init__.py)"
     ]
    }
   ],
   "source": [
    "output_dict = output_parser.parse(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b5b87d55",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[77]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43moutput_dict\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'output_dict' is not defined"
     ]
    }
   ],
   "source": [
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5269c248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "575c5cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict.get('delivery_days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e772a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07a436c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fee3b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26807d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9ffdf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f154ff8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fea4a58",
   "metadata": {},
   "source": [
    "LangChain:\n",
    "- Model\n",
    "- Prompts\n",
    "- Output Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c6c0b3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exploring-langchain",
   "language": "python",
   "name": "exploring-langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
